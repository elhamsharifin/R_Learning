---
title: "Probability (learned from edx (Probability), note: the note part are exactly the same of edx course)"
author: "Elham Sharifin"
date: "4/10/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Key points: Discrete Probability
•	The probability of an event is the proportion of times the event occurs when we repeat the experiment independently under the same conditions.
Pr(A)=probability of event A
•	An event is defined as an outcome that can occur when something happens by chance.
•	We can determine probabilities related to discrete variables (picking a red bead, choosing 48 Democrats and 52 Republicans from 100 likely voters) and continuous variables (height over 6 feet).


## Key points: Monte Carlo Simulations
Monte Carlo simulations model the probability of different outcomes by repeating a random process a large enough number of times that the results are similar to what would be observed if the process were repeated forever.
The sample() function draws random outcomes from a set of options.
The replicate() function repeats lines of code a set number of times. It is used with sample() and similar functions to run Monte Carlo simulations.

### example:
following is a simple example of how we can calculate the probability of an event by Monto Carlo simulation. In this event, the calculation of exact probability was simple; however, sometimes it is not the case. 
```{r}
beads <- rep(c("red","blue"),times=c(2,3)) #creates a run with 2 red and 3 blue
beads
sample(beads,1) #it shows 1 random sample of beads
sample(beads,5) #it shows 5 random sample of beads (2 reds, 3 blue)
sample(beads,5)
#sample(beads,6) #error, it will not replicate
sample(beads,6,replace=TRUE) #it will replicate
```
```{r}
B <- 1000 #how many time to draw one bead
events <- replicate(B,sample(beads,1)) #draw 1 beads, B times
#events  it creats 1000 random events of beads
tab <- table(events)
tab
prop.table(tab)
```

### sample function
we dont need to use replecate. because sample function has this advantage that can drew a lot of samples!
By default this selection is without replacement!
```{r}
sample(beads,5) #we will have 2 red and 3 blues
sample(beads,5)
sample(beads,5)
#sample(beads,6) #error: because it will do it without replacement
```
```{r}
events <- sample(beads,B,replace=TRUE) #it gives us the distibution
prop.table(table(events)) #it gives us the proportion
```
## Setting the Random Seed
The set.seed() function: because all the generated numbers are randomely selected, they are different from every person run to another one. However, if you want to make sure the results are exactly the same, we can use this function.
A popular way for picking the seed is: the year - month - day.
For example, we picked 1986 on December 20, 2018:  2018 − 12 − 20 = 1986.
```{r}
set.seed(1986)
?set.seed
```

if you run R 3.6, you can revert to the original seed setting behavior by adding the argument sample.kind="Rounding". For example:

```{r}
set.seed(1)
set.seed(1,sample.kind="Rounding") #will make R 3.6 generate a seed as in R 3.5
```
## Using the mean Function for Probability
example:
```{r}
beads <- rep(c("blue","red"),times=c(3,2))
beads
```
the probability of drawing a blue bead at random:
```{r}
mean(beads=="blue")
```
exaplanation: in facet it will evaluate if it is true or false tha we have blue: (true,true,false,false,false). so that mean only calculate the numbers is true: (1,1,1,0,0), the means of 1+1+1+0+0/5=0.6 and in fact, it gives the proportion of TRUE. The probabilities are directly related to the proportion of events that satisfy a requirement.

## Key points: Probability Distributions
The probability distribution for a variable describes the probability of observing each possible outcome.
For discrete categorical variables, the probability distribution is defined by the proportions for each group. such as beads!

## Key points: Independence
Conditional probabilities compute the probability that an event occurs given information about dependent events. 
For example, the probability of drawing a second king given that the first draw is a king is (if we have 4 series cards):

Pr(Card 2 is a king∣Card 1 is a king)=3/51 # we already have taken one card, so we have 51 cards!

```{r}
x <- sample(beads,5)
set.seed(1,sample.kind="Rounding")
x[2:5] #now the probability of red is one
```


If two events A and B are independent, 
Pr(A∣B)=Pr(A) # because their probabilities do not depend on each other

To determine the probability of multiple events occurring, we use the multiplication rule.

### Equations
The multiplication rule for independent events is:
Pr(A and B and C)=Pr(A)×Pr(B)×Pr(C)
This is equivalent to drawing a bead with replacement!

The multiplication rule for dependent events considers the conditional probability of both events occurring:
Pr(A and B)=Pr(A)×Pr(B∣A)

We can expand the multiplication rule for dependent events to more than 2 events:

Pr(A and B and C)=Pr(A)×Pr(B∣A)×Pr(C∣A and B)

### exercise:
Suppose, we have 3 cyan, 5 magneta, and 7 yellow, we take 1 card:
```{r}
set.seed(1,sample.kind="Rounding")
cyan <- 3
magenta <- 5
yellow <- 7

 #peobability of choosing cyan in the first round:
p_cyan_1 <-  cyan / (cyan + magenta + yellow) #without replacement
p_cyan_1

#probability of not choosing cyan in the second round if the first one is selected (sampling without replacement)
p_not_cyan_without <- (magenta + yellow)/(cyan + magenta + yellow-1) #without replacement
p_not_cyan_without 

#probability of not choosing cyan in the second round if the first one is selected (sampling without replacement)
p_not_cyan_with <- (magenta + yellow)/(cyan + magenta + yellow) #without replacement
p_not_cyan_with

#the probability that the first draw is cyan and the second draw is not cyan
p_cyan_1*p_not_cyan_without
p_cyan_1*p_not_cyan_with
```
## Key points: Combinations and Permutations
paste() joins two strings and inserts a space in between.
expand.grid() gives the combinations of 2 vectors or lists.
permutations(n,r) from the gtools package lists the different ways that r items can be selected from a set of n options when order matters.
combinations(n,r) from the gtools package lists the different ways that r items can be selected from a set of n options when order does not matter.

### Code: Introducing paste() and expand.grid()
```{r}
# joining strings with paste
number <- "Three"
suit <- "Hearts"
paste(number, suit)

# joining vectors element-wise with paste
paste(letters[1:5], as.character(1:5))

# generating combinations of 2 vectors with expand.grid
expand.grid(pants = c("blue", "black"), shirt = c("white", "grey", "plaid"))
```
### Code: Generating a deck of cards
```{r}
suits <- c("Diamonds", "Clubs", "Hearts", "Spades")
numbers <- c("Ace", "Deuce", "Three", "Four", "Five", "Six", "Seven", "Eight", "Nine", "Ten", "Jack", "Queen", "King")
deck <- expand.grid(number = numbers, suit = suits)
deck <- paste(deck$number, deck$suit)

# probability of drawing a king
kings <- paste("King", suits)
mean(deck %in% kings)
```
### Code: Permutations and combinations
Correction: The code shown does not generate all 7 digit phone numbers because phone numbers can have repeated digits. It generates all possible 7 digit numbers without repeats.
```{r}
library(gtools)
permutations(5,2) # ways to choose 2 numbers in order from 1:5 without replacement (when we take a number, it cannot be appeared again)

all_phone_numbers <- permutations(10, 7, v = 0:9) #v=0:9 sjows the vector. for example it cannot be from 1:10
n <- nrow(all_phone_numbers)
index <- sample(n, 5) #it show 5 random number
all_phone_numbers[index,]  #it shows the phone number associated with the index

permutations(3,2)    # order matters
combinations(3,2)    # order does not matter
```
### Code: Probability of drawing a second king given that one king is drawn
```{r}
suits <- c("Diamonds", "Clubs", "Hearts", "Spades")
numbers <- c("Ace", "Deuce", "Three", "Four", "Five", "Six", "Seven", "Eight", "Nine", "Ten", "Jack", "Queen", "King")
deck <- expand.grid(number = numbers, suit = suits)
deck <- paste(deck$number, deck$suit)

# probability of drawing a king
kings <- paste("King", suits)
mean(deck %in% kings)

# Probability of drawing a second king given that one king is drawn
hands <- permutations(52,2, v = deck) #select from vectors that are card names (deck)
first_card <- hands[,1] #it shows the column 1
second_card <- hands[,2] #column 2 of hands
sum(first_card %in% kings)

sum(first_card %in% kings & second_card %in% kings) / sum(first_card %in% kings)
# it is equivalant with Pr(B|A)=Pr(A and B)/Pr(A) #conditional probability for dependent variables!!!
```
### Code: Probability of a natural 21 in blackjack natural 21: (natural 21: Ace+king,queen,jack,ten)
```{r}
suits <- c("Diamonds", "Clubs", "Hearts", "Spades")
numbers <- c("Ace", "Deuce", "Three", "Four", "Five", "Six", "Seven", "Eight", "Nine", "Ten", "Jack", "Queen", "King")
deck <- expand.grid(number = numbers, suit = suits)
deck <- paste(deck$number, deck$suit)

aces <- paste("Ace",suits) #the vector include all Aces
facecard <- c("King","Queen","Jack","Ten") 
facecard <- expand.grid(number=facecard,suit=suits)
facecard <- paste(facecard$number,facecard$suit) #the vector includes all facecard

hands <- combinations(52,2,v=deck) #all combination of picking 2 cards from 4 hands (13*4=52)

#probability of a natural 21 given that the ace is listed first in  'combinations'
mean(hands[,1] %in% aces & hands[,2] %in% facecard)

# for more caustion: probability of a natural 21 checking for both ace first and ace second (it means: first aces and second facecard or first facecard and second aces)
mean((hands[,1] %in% aces & hands[,2] %in% facecard)|(hands[,2] %in% aces & hands[,1] %in% facecard))
```
### Code: Monte Carlo simulation of natural 21 in blackjack
```{r}
# instead of finding the exact combination as previouse, we can use Monto Carlo and find the approximate probability
set.seed(1986)
suits <- c("Diamonds", "Clubs", "Hearts", "Spades")
numbers <- c("Ace", "Deuce", "Three", "Four", "Five", "Six", "Seven", "Eight", "Nine", "Ten", "Jack", "Queen", "King")
deck <- expand.grid(number = numbers, suit = suits)
deck <- paste(deck$number, deck$suit)

aces <- paste("Ace",suits) 

facecard <- c("King","Queen","Jack","Ten") 
facecard <- expand.grid(number=facecard,suit=suits)
facecard <- paste(facecard$number,facecard$suit) 

hands <- sample(deck,2)  #drawing two card without replacement
hands

# code for B=10,000 hands of blackjack
B <- 10000
results <- replicate(B,{
  hands <- sample(deck,2)
  (hands[1] %in% aces & hands[2] %in% facecard)|(hands[2] %in% aces & hands[1] %in% facecard)
})
mean(results)
```

## Key points: The Birthday Problem
duplicated() takes a vector and returns a vector of the same length with TRUE for any elements that have appeared previously in that vector.
We can compute the probability of shared birthdays in a group of people by modeling birthdays as random draws from the numbers 1 through 365. We can then use this sampling model of birthdays to run a Monte Carlo simulation to estimate the probability of shared birthdays.

### Code: The birthday problem
what is the chance that at least two people in a classromm of 50 peopel has the same birthday?

```{r}
# how many duplicate birthday in a group of 50
n <- 50
bdays <- sample(1:365,n,replace = TRUE)
any(duplicated(bdays))  #it is only for one sample

# Monte Carlo simulation with B=10000 replicates
# for finding the probability, we have to run it for many times and have a lot of sampels
B <- 10000
results <- replicate(B,{   # returns vector of B logical values
  bdays <- sample(1:365,n,replace=TRUE)
  any(duplicated(bdays))
})
mean(results)  #calculate the proportion or probability of duplicated birthdays
```

## Key points: sapply
Some functions automatically apply element-wise to vectors, such as sqrt() and *. However, other functions do not operate element-wise by default. This includes functions we define ourselves.

The function sapply(x, f) allows any other function f to be applied element-wise to the vector x.

The probability of an event happening is 1 minus the probability of that event not happening:
Pr(event)=1−Pr(no event)

We can compute the probability of shared birthdays mathematically:
 Pr(shared birthdays)=1−Pr(no shared birthdays)=1−(1×364/365×364/365× ...× (365-n+1)/365)

```{r}
# function of calculation of probability of same birth days across n people
compute_bday <- function(n,B=10000){
  same_day <- replicate(B,{
    bday <- sample(1:365,n,replace = TRUE)
    any(duplicated(bday))
  })
  mean(same_day)
}

n <- seq(1,60) 
```
### Code: Element-wise operation over vectors and sapply:
```{r}
x <- 1:10
sqrt(x) #sqrt applys on every elements of the vector x

y <- 1:10
x*y  #it multiply each elements from x on y

compute_bday(n)    # does not iterate over the vector n without sapply

x <- 1:10
sapply(x,sqrt) #this is equivalent with sqrt(x)

prob <- sapply(n,compute_bday) #element-wise application of compute_prob to n 9insteaf of reiting a loop, in R we prefer to use sapply)

plot(n,prob)  #n the size of the group
```
#### Code: Computing birthday problem probabilities with sapply
if we calculate the probability of peoples do not have the same birthday:
Pr(person 1 has unique bday) = 1
Pr(person 2 has a uique birthday|person 1 has a unique bday) = 364/365
Pr(person 3 has a unique bday | person 1 , 2 have unique bdays)=363/365
...
(365/365)*(364/365)*(363/365)*...*((365-(n-1))/365)
now we can write a function for that.
```{r}
# exact formulation for bday problem
exact_prob <- function(n){
  prob_unique <- seq(365,365-n+1)/365
  1 - prod(prob_unique)
}

# applying function element-wise to vector of n values
eprob <- sapply(n, exact_prob)

# plotting Monte Carlo results and exact probabilities on same graph
plot(n, prob)    # plot Monte Carlo results
lines(n, eprob)   # add line for exact prob
# as seen, monto-carlo simulation can predict the probability very good.
```

## Key points: How Many Monte Carlo Experiments are Enough?

The larger the number of Monte Carlo replicates B, the more accurate the estimate.

Determining the appropriate size for B can require advanced statistics.

One practical approach is to try many sizes for B and look for sizes that provide stable estimates.

The following code runs Monte Carlo simulations to estimate the probability of shared birthdays using several B values and plots the results. When B is large enough that the estimated probability stays stable, then we have selected a useful value of B.

```{r}
# Code: Estimating a practical value of B
B <- 10^seq(1,5,length=100)

compute_bday <- function(B,n=22){
  same_day <- replicate(B,{
    bday <- sample(1:365,n,replace = TRUE)
    any(duplicated(bday))
  })
  mean(same_day)
}

prob <- sapply(B, compute_bday) # apply compute_bday to many values of B
plot(log10(B), prob, type = "l")    # plot a line graph of estimates 
```
### Exercise (independent)
suppose we have a box cotaining 3 cyan, 5 magenta, and 7 yellow balls. suppose we draw 5 balls with replacement, and all have been yellow. what is the probability that the sixth draw become yellow too?
```{r}
# Sampling with replacement
yellow <- 7
magenta <- 5
cyan <-3

prob_yellow <- yellow/(yellow+magenta+cyan)

prob_7_yellow <- prob_yellow #The probability of drawing a yellow ball is not dependent of previous draws when balls are replaced after each draw. If two events A and B are independent,Pr(A∣B)=Pr(A)

prob_7_yellow
```
### Exercise (Rolling a die)
suppose we have a die with six sides:
what is the probability of not-seeing a six in one roll? what is the probability of not-seeing a six for all of the six rolls?
```{r}
prob_no6 <- 5/6
prob_no6_6roll <- prob_no6*prob_no6*prob_no6*prob_no6*prob_no6*prob_no6
prob_no6_6roll
```
### Exercise (winning between two teams)
Suppose there are two teams Esteghlas and Piroozi and Esteghlala is more stronger and has 1 60% of wininng in front of piroozi. what is the probability that piroozi win at least one game? (Remember that the piroozi must win one of the first four games, or the series will be over!)
```{r}
# at least win 1 game = 1-(pirrozi win 0 game)
#pirrozi win 0 game = esteghala winn all of 4 games
win_esteghlal_4 <- 0.6*0.6*0.6*0.6
win_pirrozi_1 <- 1- win_esteghlal_4
win_pirrozi_1
```
###Exercise: Monte Carlo simulation for Esteghlal winning a game:
Create a Monte Carlo simulation to confirm your answer to the previous problem by estimating how frequently the Celtics win at least 1 of 4 games (assume: B <- 10000).
```{r}
simulated_games <- sample (c("win","lose"),4,replace = TRUE, prob = c(0.4,0.6)) #these are simulated games for piroozi that the probabilities are not equal
simulated_games

B <- 10000
perspolis_win <- replicate(B,{
  simulated_games <- sample (c("win","lose"),4,replace = TRUE, prob = c(0.4,0.6))
  any(simulated_games==c("win"))
})
mean(perspolis_win)
```
## Key points: The Addition Rule
The addition rule states that the probability of event A or event B
happening is the probability of event A plus the probability of event B minus the probability of both events A and B happening together.
Pr(A or B)=Pr(A)+Pr(B)−Pr(A and B)
Note that (A or B) is equivalent to (A|B)

### example:
Example: The addition rule for a natural 21 in blackjack.
We apply the addition rule where A = drawing an ace then a facecard and  B = drawing a facecard then an ace. Note that in this case, both events A and B cannot happen at the same time, so Pr(A and B)=0.
Pr(ace_then_facecard) = (4/52)*(16/51)
Pr(facecard_then_ace) = (16/51)*(4/52)
Pr(ace_then_facecard | facecard_then_ace)= (4/52) * (16/51) + (16/51) *(4/52)

## Key points: The Monty Hall Problem
Monte Carlo simulations can be used to simulate random outcomes, which makes them useful when exploring ambiguous or less intuitive problems like the Monty Hall problem.

In the Monty Hall problem, contestants choose one of three doors that may contain a prize. Then, one of the doors that was not chosen by the contestant and does not contain a prize is revealed. The contestant can then choose whether to stick with the original choice or switch to the remaining unopened door.

Although it may seem intuitively like the contestant has a 1 in 2 chance of winning regardless of whether they stick or switch, Monte Carlo simulations demonstrate that the actual probability of winning is 1 in 3 with the stick strategy and 2 in 3 with the switch strategy.

### Code: Monte Carlo simulation of stick strategy
```{r}
doors <- as.character(1:3)
doors
prize <- sample(c("car","goat","goat")) # puts prizes in random order
prize
prize_door <- doors[prize=="car"] # note which door has prize
prize_door
my_pick <- sample(doors,1) # note which door is chosen
my_pick
show <- sample(doors[!doors %in% c(my_pick,prize_door)],1) # open door with no prize that isn't chosen
show
stick <- my_pick
stick == prize_door


#Monto_carlo simulation
B <- 10000
stick <- replicate(B, {
	doors <- as.character(1:3)
	prize <- sample(c("car","goat","goat"))    # puts prizes in random order
	prize_door <- doors[prize == "car"]    # note which door has prize
	my_pick  <- sample(doors, 1)    # note which door is chosen
	show <- sample(doors[!doors %in% c(my_pick, prize_door)],1)    # open door with no prize that isn't chosen
	stick <- my_pick    # stick with original door
	stick == prize_door    # test whether the original door has the prize
})
mean(stick)    # probability of choosing prize door when sticking
```
### Code: Monte Carlo simulation of switch strategy
```{r}
doors <- as.character(1:3)
doors
prize <- sample(c("car","goat","goat")) # puts prizes in random order
prize
prize_door <- doors[prize=="car"] # note which door has prize
prize_door
my_pick <- sample(doors,1) # note which door is chosen
my_pick
show <- sample(doors[!doors %in% c(my_pick,prize_door)],1) # open door with no prize that isn't chosen
show
switch <- doors[!doors%in%c(my_pick,show)]  ## switch to the door that wasn't chosen first or opened
switch
switch == prize_door

# Monto_carlo simulation
B <- 10000
switch <- replicate(B, {
	doors <- as.character(1:3)
	prize <- sample(c("car","goat","goat"))    # puts prizes in random order
	prize_door <- doors[prize == "car"]    # note which door has prize
	my_pick  <- sample(doors, 1)    # note which door is chosen first
	show <- sample(doors[!doors %in% c(my_pick, prize_door)], 1)    # open door with no prize that isn't chosen
	switch <- doors[!doors%in%c(my_pick, show)]    # switch to the door that wasn't chosen first or opened
	switch == prize_door    # test whether the switched door has the prize
})
mean(switch)    # probability of choosing prize door when switching
```
### Exercise: The Cavs and the Warriors
Two teams of Cavs and the Warriors are playing a seven game championship series. The first to win four games wins the series. The teams are equally good, so they each have a 50-50 chance of winning each game. 
If the Cavs lose the first game, what is the probability that they win the series.

```{r}
## Monto Carlo simulation
simulated_games <- sample (c("win","lose"),6,replace = TRUE, prob = c(0.5,0.5)) 
simulated_games

B <- 10000
set.seed(1) #if we want that our answer become identical with edx
Warriors_win <- replicate(B,{
  simulated_games <- sample (c("win","lose"),6,replace = TRUE, prob = c(0.5,0.5))
  })
win <- colSums(Warriors_win=="win")>=4
mean(win)

#if we define win and lose as (0,1)
#sum(simulated_games)>=4  inside the Monoto caerlo before })
```
Another method:
```{r}
# Assign a variable 'n' as the number of remaining games.
n <- 6
n
# Assign a variable `outcomes` as a vector of possible game outcomes, where 0 indicates a loss and 1 indicates a win for the Cavs.
outcomes <- c(0:1)
outcomes
# Assign a variable `l` to a list of all possible outcomes in all remaining games. Use the `rep` function on `list(outcomes)` to create list of length `n`.
l <- rep(list(outcomes),n)
l
# Create a data frame named 'possibilities' that contains all combinations of possible outcomes for the remaining games.
possibilities <- expand.grid(l)
possibilities

# Create a vector named 'results' that indicates whether each row in the data frame 'possibilities' contains enough wins for the Cavs to win the series.

results <- rowSums(possibilities==1)>=4
results

# Calculate the proportion of 'results' in which the Cavs win the series. Print the outcome to the console.
mean(results)
```
### Exercise: 
Suppose two teams of A, and B are playing a sevn-series game. Team A is better than B and has a p>0.5chance of winning each game.compute the probability of wininng for the p <- seq(0.5,0.95,0.025)

```{r}
p <- seq(0.5,0.95,0.025)  #probability that team A wins
## Given a value 'p', the probability of winning the series for the underdog team B can be computed with the following function based on a Monte Carlo simulation:
prob_win <- function(p){
  results <- replicate(B,{
    b_win <- sample(c(0,1),7,replace = TRUE, prob=c(1-p,p)) #1-p:0 losing #p:1 wining
    sum(b_win) >= 4
  })
  mean(results)
}
Pr <- sapply(p,prob_win) ## Apply the 'prob_win' function across the vector of probabilities that team A will win to determine the probability that team B will win. Call this object 'Pr'.
plot(p,Pr)
```
part 2 of wining A team or B team:
if team A wins at the probability of 0.75. compute the probability for different series of game (1 game, 2 games , ..., 25 games)
```{r}
B <- 10000
compute_win <- function(n,p=0.75){
  results <- replicate(B,{
    b_win <- sample(c(1,0),n,replace=TRUE,prob=c(1-p,p))
    sum(b_win)>=(n+1)/2
  })
  mean(results)
}

n <- seq(1,25,2)
Pr <- sapply(n,compute_win)
plot(n,Pr)
```
## Assessment:Olympic Running
in the final of 200m, theer are 8 people who fight for the three medals in which order is matter. in Olympic 2012, three of eight runner were from Jamaica and other fiver runners from other countries. all of three medal were won by Jamaica . 

```{r}
library(gtools)
library(tidyverse)

# different ways the three medals can be distributed accross runners:
medals <- permutations(8,3)
nrow(medals)

# different ways that three medals can be distributed among three Jamaican runners:
Jamaica_medals <- permutations(3,3)
nrow(Jamaica_medals )

# probability that all three medals are won by Jamaica n runners:
Pr_Jamaica  <- nrow(Jamaica_medals)/nrow(medals)
Pr_Jamaica 

# Monto carlo 
runners <- c("Jamaica", "Jamaica", "Jamaica", "USA", "Ecuador", "Netherlands", "France", "South Africa")
B <- 10000
set.seed(1)
jam_win <- replicate(B,{
  simulated_games <- sample(runners,3) #be careful: replace=false order is matter
  all(simulated_games=="Jamaica")
   })
mean(jam_win)
```
## Assessment:Restaurant management
A restaurane manager is going to advertise that his lunch menue has differnt foods for each days of year (365 diffrent foods).
each meal at the restaurans includes 1 entree, 2 sides, and 1 drink.
He currently offers a choice of 1 entree from a list of 6 options, a choice of 2 different sides from a list of 6 options, and a choice of 1 drink from a list of 2 options.

```{r}
### possible meal combination at his restaurant:
entree6_1 <- combinations(6,1,v=c("E1","E2","E3","E4","E5","E6"))
side6_2 <- combinations(6,2,v=c("s1","s2","s3","s4","s5","s6"))
drink2_1 <- combinations(2,1,v=c("d1","d2"))
nrow(entree6_1)*nrow(side6_2)*nrow(drink2_1)

### possible meal options if the manager add one aditional drink:
drink3_1 <- combinations(3,1,v=c("d1","d2","d3"))
nrow(entree6_1)*nrow(side6_2)*nrow(drink3_1)

### the manager decided to not add any thing else to his menue but get his golas with choosing more options. 
# all options of choosing from 6 entrees, 3 drinks, and 3 sides. 
side6_3 <- combinations(6,3,v=c("s1","s2","s3","s4","s5","s6"))
nrow(entree6_1)*nrow(side6_3)*nrow(drink3_1)

### manager is concerns that customers do not want three sides. so he want to increase the number of entree options. he wanna know how many entree he has to add to entrees to meet his goals. Write a function that takes a number of entree choices and returns the number of meal combinations possible given that number of entree options, 3 drink choices, and a selection of 2 sides from 6 options.

entree_options <- function(n){
  nrow(combinations(n,1))*nrow(combinations(6,2))*nrow(combinations(3,1))
}

# apply the function to entree option counts ranging from 1 to 12.
entree_option_12 <- sapply(1:12,entree_options)

# minimum number of meals to get his goals (more than 365 meal options)
data.frame(entrees=1:12,entree_option_12) %>%
  filter(entree_option_12>=365) %>%
  min(.$entrees)

### the manager cannot afford increasing the number of entrees and he thinks it is better to increase the number of sides. how many sides he has to offer to meet the at least 365 meals. 
#Write a function that takes a number of side choices and returns the number of meal combinations possible given 6 entree choices, 3 drink choices, and a selection of 2 sides from the specified number of side choices. 

side_options <- function(s){
  nrow(combinations(6,1))*nrow(combinations(s,2))*nrow(combinations(3,1))
}

# apply the function to side option counts ranging from 2 to 12.
side_option_12 <- sapply(2:12,side_options)

# minimum number of meals to get his goals (more than 365 meal options)
data.frame(sides=2:12,side_option_12) %>%
  filter(side_option_12>=365) %>%
  min(.$sides)
```
## Assessment: cancer and alcohol/tobacco use
The dataset contains data from a study in France that compare people with esophageal cancer (ncases) to poeple without cancer (ncontrols) that are carefully matched on a variety of demographic and medical conditions. The study compares alcohol (alcgp) and tabacco (tobgp) intakes in grams per day among cases and controls grouped by age range (agegp). each group is shown in one row.
```{r}
library(tidyverse)
head(esoph)

### number of groups in the study
nrow(esoph)

### how many cases and controls are there?
all_cases <- sum(esoph$ncases)
all_cases

all_controls <- sum(esoph$ncontrols)
all_controls

### the probaility of a subject in the highest alcohol consumption group is a cancer case?
min(esoph$alcgp) #which group is max 

 esoph %>%
   filter(alcgp=="120+") %>%
   summarize(ncases = sum(ncases),ncontrols = sum(ncontrols)) %>%
   mutate(p_cases=ncases/(ncases+ncontrols),p_controls=ncontrols/(ncases+ncontrols))
 
 ### the probaility of a subject in the lowest alcohol consumption group is a cancer case?
 min(esoph$alcgp) #which group is min
 
  esoph %>%
   filter(alcgp=="0-39g/day") %>%
   summarize(ncases = sum(ncases),ncontrols = sum(ncontrols)) %>%
   mutate(p_cases=ncases/(ncases+ncontrols),p_controls=ncontrols/(ncases+ncontrols))
  
  ### given a person is a case (means that he has cancer), the probability that they smoke 10 gram or more?
  head(esoph)
  levels(esoph$tobgp)
  
  esoph %>%
    filter(tobgp=="10-19"|tobgp=="20-29"|tobgp=="30+") %>%
    summarise(pr_cases=sum(ncases)/all_cases)
# we can use tobgp != "0-9g/day" instead of defining all group >10
  
  ### given the person is control, the probability that they smoke 10g or more? 
    esoph %>%
    filter(tobgp=="10-19"|tobgp=="20-29"|tobgp=="30+") %>%
    summarise(pr_controls=sum(ncontrols)/all_controls)

```

Part 2:
```{r}
### for cases, the probaility of being in the highest alcohol group:
 levels(esoph$alcgp)

ncases_high_alco <- esoph %>%
  filter(alcgp=="120+") %>%
  pull(ncases) %>%
  sum()

Pr_ncases_high_alco <- ncases_high_alco/all_cases
Pr_ncases_high_alco
   
   
### for cases, the probaility of being in the highest tabacco group:
 levels(esoph$tobgp)

ncases_high_tob <- esoph %>%
  filter(tobgp=="30+") %>%
  pull(ncases) %>%
  sum()

Pr_ncases_high_tob <- ncases_high_tob/all_cases
Pr_ncases_high_tob


### for cases, the probability of being in highest tabacco and alcohol group?
levels(esoph$tobgp)
levels(esoph$alcgp)

ncases_high_tob_alco <- esoph %>%
  filter(tobgp=="30+" & alcgp=="120+") %>%
  pull(ncases) %>%
  sum()

Pr_ncases_high_tob_alco <- ncases_high_tob_alco/all_cases
Pr_ncases_high_tob_alco

### for cases, the probability of being in highest tabacco or alcohol group?
Pr_ncases_either_tab_alco <- Pr_ncases_high_alco + Pr_ncases_high_tob - Pr_ncases_high_tob_alco
Pr_ncases_either_tab_alco

### for controls, probability of being in the highest alcohol group:
ncontrols_high_alco <- esoph %>%
  filter(alcgp=="120+") %>%
  pull(ncontrols) %>%
  sum()

Pr_ncontrols_high_alco <- ncontrols_high_alco/all_controls
Pr_ncontrols_high_alco

### how many times are more likely that case than controls be in the highest alcohol group:
Pr_ncases_high_alco/Pr_ncontrols_high_alco

### for controls, the probability to being in highest tabacco group:
ncontrols_high_tob <- esoph %>%
  filter(tobgp=="30+" ) %>%
  pull(ncontrols) %>%
  sum()

Pr_ncontrols_high_tob <- ncontrols_high_tob/all_controls
Pr_ncontrols_high_tob

### for controls, the probability to being in highest tabacco and alcohol group:
ncontrols_high_tob_alco <- esoph %>%
  filter(tobgp=="30+" & alcgp=="120+" ) %>%
  pull(ncontrols) %>%
  sum()

Pr_ncontrols_high_tob_alco <- ncontrols_high_tob_alco/all_controls
Pr_ncontrols_high_tob_alco

### for controls, the probability to being in highest tabacco or alcohol group:
Pr_ncontrols_either_tob_alco <- Pr_ncontrols_high_tob + Pr_ncontrols_high_alco - Pr_ncontrols_high_tob_alco

Pr_ncontrols_either_tob_alco

### How many times more likely are cases than controls to be in the highest alcohol group or the highest tobacco group?
Pr_ncases_either_tab_alco/Pr_ncontrols_either_tob_alco
```
## Key points: Continuous Probability
The cumulative distribution function (CDF) is a distribution function for continuous data x that reports the proportion of the data below 
a for all values of a:
F(a)=Pr(x≤a)
The CDF is the probability distribution function for continuous variables. For example, to determine the probability that a male student is taller than 70.5 inches given a vector of male heights 
x, we can use the CDF:
Pr(x>70.5)=1−Pr(x≤70.5)=1−F(70.5)
The probability that an observation is in between two values 
a,b is F(b)−F(a).
```{r}
# Cumulative distribution function
library(dslabs)
library(tidyverse)
data(heights)

# x defines as male heights from dslabs dataset:
x <- heights %>% 
  filter(sex=="Male") %>%
  .$height

# we can define a function of cumulative distribution (CDF) of x:
F <- function(a) mean(x <= a)
1 - F(70)  #it gives us the probability of male height more than 70
```
## Key points:Theoretical Distribution
pnorm(a, avg, s) gives the value of the cumulative distribution function F(a) for the normal distribution defined by average avg and standard deviation s.

We say that a random quantity is normally distributed with average avg and standard deviation s if the approximation pnorm(a, avg, s) holds for all values of a.

If we are willing to use the normal approximation for height, we can estimate the distribution simply from the mean and standard deviation of our values.

If we treat the height data as discrete rather than categorical, we see that the data are not very useful because integer values are more common than expected due to rounding. This is called discretization.

With rounded data, the normal approximation is particularly useful when computing probabilities of intervals of length 1 that include exactly one integer.

```{r}
# Cumulative distribution function

### probability of male heights more than 70
1-pnorm(70.5,mean(x),sd(x))

### plot distribution of exact heights in data
plot(prop.table(table(x)), xlab="a = Heights (inches)", ylab="Pr(X = a)")

### probabilities in actual data over length 1 ranges containing an integer: Be careful this is actual data
mean(x <= 68.5) - mean(x <= 67.5)
mean(x <= 69.5) - mean(x <= 68.5)
mean(x <= 70.5) - mean(x <= 69.5)

### probabilities in normal approximation match well for these intervals: Be carful: this is approximation
pnorm(68.5,mean(x),sd(x))-pnorm(67.5,mean(x),sd(x))
pnorm(69.5,mean(x),sd(x))-pnorm(68.5,mean(x),sd(x))
pnorm(70.5,mean(x),sd(x))-pnorm(69.5,mean(x),sd(x))

### However, approximation does not match with actual data well for other inetervals:
mean(x <= 70.9) - mean(x <= 70.1)
pnorm(70.9,mean(x),sd(x))-pnorm(70.1,mean(x),sd(x))
```

## Key points: Probability Density
The probability of a single value is not defined for a continuous distribution.

The quantity with the most similar interpretation to the probability of a single value is the probability density function f(x).

The probability density f(x) is defined such that the integral of 
f(x) over a range gives the CDF of that range.
F(a)=Pr(X≤a)=∫(−∞,a)f(x)dx

In R, the probability density function for the normal distribution is given by dnorm(). We will see uses of dnorm() in the future.

Note that dnorm() gives the density function and pnorm() gives the distribution function, which is the integral of the density function.

## Plotting the Probability Density for normal distribution:
using the function of dnorm(), we can plot the probaility density for normal distribution. dnorm() gives the probaility density f(x) for a certain z-score. by calculationg the f(x) for a range of possible values of z score, we can plot the density. 

1) we can generate a series of z-score that is typical range of normal distribution. 
2) since 99.7% of observation would be in the range of −3≤z≤3, if we assume the range of z slightly more than this range, it will cover the normal distribution. 
3) we will calculate f(z) using dnorm()
4) we will plot z v. f(z)

Note: dnorm() gives us the normal distribution on zero by default. 
density function for mean of mu and standard deviation of sigma would be:
dnorm(z, mu, sigma)

```{r}
### plotting a density function 
library(tidyverse)
x <- seq(-4,4,length=100)
data.frame(x,f=dnorm(x)) %>%
  ggplot(aes(x,f)) +
  geom_line()
```

## Key points: Monto Carlo Simulation
rnorm(n, avg, s) generates n random numbers from the normal distribution with average avg and standard deviation s. (By defaults, average and standard deviations are 0 and 1 repectively) 

By generating random numbers from the normal distribution, we can simulate height data with similar properties to our dataset. Here we generate simulated height data using the normal distribution.

```{r}
library(dslabs)
library(tidyverse)
data(heights)

## male height data
height_male <- heights %>% filter(sex=="Male") %>% .$height

## normal distribution of male heights. both dataset should have n #observations
n <- length(height_male)
avg <- mean(height_male)
s <- sd(height_male)
simulated_heights <- rnorm(n,avg,s)

##plotting distribution of simulated_heights
data.frame(simulated_heights = simulated_heights) %>%
  ggplot(aes(simulated_heights)) +
  geom_histogram(color="black",binwidth=1)
# the distribution looks normal because it generated to be normal

## Monto Carlo Simulation of tallest person over 7 feet:
# (the only input data are average and standard deviation of data)
B <- 10000 # we will run the model for 10000 times
tallest <- replicate(B,{
  simulated_data <- rnorm(800,avg,s) #for each run, it generates 800 normally distributed random heights
  max(simulated_data)
})
mean(tallest >= 7*12) #proportion of times that the tallest person exceeds 7 feet.
```

## Key points: other continuous distributions
You may encounter other continuous distributions (Student t, chi-squared, exponential, gamma, beta, etc.).

R provides functions for density (d), quantile (q), probability distribution (p) and random number generation (r) for many of these distributions.

Each distribution has a matching abbreviation (for example, norm() or t()) that is paired with the related function abbreviations (d, p, q, r) to create appropriate functions.

For example, use rt() to generate random numbers for a Monte Carlo simulation using the Student t distribution.

```{r}
### plotting normal distribution with dnorm() function
x <- seq(-4,4,length.out=100)
data.frame(x,f=dnorm(x)) %>%
  ggplot(aes(x,f)) +
  geom_line()
```
### Exercises:
Distribution of female heights:
If the female heights from a dataset approximated with a normal distribution with average of 64 inch and stand deviation of 3 inch. what is the propability that the female height be shorter than 5 foot. 
```{r}
### female heights
avg_female <- 65
sd_female <- 3
pnorm(5*12,avg_female,sd_female)

### the prbability that a random height be more than 7 feet?
1-pnorm(7*12,avg_female,sd_female)

## the prbability that a random height be between 62 and 68 inches?
pnorm(68,avg_female,sd_female)-pnorm(62,avg_female,sd_female)

### the prbability that a random height be between 62 and 68 inches based on centimeter?
avg_female_cm <- 65*2.54
sd_female_cm <- 3*2.54
pnorm(68*2.54,avg_female_cm,sd_female_cm)-pnorm(62*2.54,avg_female_cm,sd_female_cm)
# as noticed the answer to this question does not change with converting the units to cm. In fact, if you look closely, you notice that 61 and 67 are both 1 SD away from the average.

###the probability that a random chosen height be between one sd from average.
avg_female <- 65
sd_female <- 3
taller <- avg_female + sd_female
shorter <- avg_female - sd_female
pnorm(taller,avg_female, sd_female) - pnorm(shorter,avg_female, sd_female)

```
```{r}
### male_heights:
# Imagine that the male heights are normally distributed with an average of 69inch and standard deviation of 3 feet.
male_avg <- 69
male_sd <- 3

### how tall is man in 99th percentile:
qnorm(0.99,male_avg,male_sd)
```
```{r}
### distribution of IQ scores:
# the IQ results are normally distributed with average of 100 and standard deviation of 15. Suppose you wannna know the distribution of the person with the highest IQ in your school district, in which 10,000 people are born each year.
# generate 10,000 IQ scores 1,000 times using a Monte Carlo simulation. Make a histogram of the highest IQ scores
B <- 1000
IQ_mean <- 100
IQ_sd <- 15

high_IQ <- replicate(B,{
  simulated_IQ <- rnorm(10000,IQ_mean,IQ_sd)
  max(simulated_IQ)
})

hist(high_IQ)
```
### assessment: ACT data
ACT is a test score in the US. duron 2016-2018, the distribution of teset score was normal with mean of 20.9 and standard deviation of 5.7. 
Set the seed to 16, generate a normal distribution of 10000 tests with mean of 20.9 and sd of 5.7 as ACT_score. ( the real scores are between 1 and 36)
Note: for R (version 3.6), you should use set.seed(x, sample.kind = "Rounding") instead of set.seed(x):
```{r}
### data of ACT test
library(tidyverse)
set.seed(16,sample.kind = "Rounding")
ACT_score <- rnorm(10000,20.9,5.7)
mean(ACT_score)
sd(ACT_score)

### how many perfect score (36 or greater) are there out of 10,000 #simulated test?
sum(ACT_score>=36)

### The probability of a score greater than 30?
mean(ACT_score > 30)

### the probability that a score is less or equall of 10:
mean(ACT_score <= 10)

### suppose x equal to the sequence of integers 1 to 36. Use dnorm to determine the value of the probability density function over x given a mean of 20.9 and standard deviation of 5.7; save the result as f_x. Plot x against f_x.
x <- seq(1,36)

data.frame(x,f_x=dnorm(x,20.9,5.7)) %>%
  ggplot(aes(x,f_x)) +
  geom_line()

```
### Assessment part 2: Converting ACT scores to z-scores:
Converting to Z-scores: values with the distribution with mean of 0 and standard deviation of 1. you must subtract data from mean and devide by standard deviation. Use the mean and sd of ACT_scores, not the roiginal oned which used to generate ACT_scores.

Note:
A Z-score of 2 corresponds roughly to the 97.5th percentile.

```{r}
### probability of Z-score greater than 2. (2 standard deviation above the mean)
library(tidyverse)
set.seed(16,sample.kind = "Rounding")
ACT_score <- rnorm(10000,20.9,5.7)
mean(ACT_score)
sd(ACT_score)

z_score <- (ACT_score - mean(ACT_score))/sd(ACT_score)
mean(z_score_ACT >2)

### which ACT score corresponds to 2 standard deviation above the mean?it means z=2  z_score <- (ACT_score - mean(ACT_score))/sd(ACT_score)
2*sd(ACT_score)+ mean(ACT_score)

### what is 97.5th percentile of normally distributed data with the mean and standard deviation observed in act_scores:
qnorm(0.975,mean(ACT_score),sd(ACT_score)) 
```
### exercise: (function of creating CDF for ACT score)
```{r}
### write a function taking value and producing the probability of an ACT score less than or equal to that value (the CDF). Apply this function to the range 1 to 36. 

library(tidyverse)
set.seed(16,sample.kind = "Rounding")
ACT_score <- rnorm(10000,20.9,5.7)
mean(ACT_score)
sd(ACT_score)

### the minimum intiger score that the probability of the score or lower is at-least 0.95
cdf <- sapply(1:36, function(a){
  mean(ACT_score <= a)
})
min(which(cdf >= 0.95)) # we didnot use qnorm because it is not for sample, and it is theoritical

### what is expected score for 95th percentile, the value for which the probability of receiving that score or lower is 0.95, given a mean score of 20.9 and standard deviation of 5.7.
qnorm(0.95,20.9,5.7)

### calculate 1st through 99th percentiles of the ACT_scores for p #<- seq(0.01, 0.99, 0.01). Save these as sample_quantiles.
#In what percentile is a score of 26?

p <- seq(0.01,0.99,0.01)
sample_quantile <- quantile(ACT_score,p)
sample_quantile[max(which(sample_quantile<26))]
names(sample_quantile[max(which(sample_quantile<26))])

### QQ-plot
#Make a corresponding set of theoretical quantiles using qnorm() over the interval p <- seq(0.01, 0.99, 0.01) with mean 20.9 and standard deviation 5.7. Save these as theoretical_quantiles. Make a QQ-plot graphing sample_quantiles on the y-axis versus theoretical_quantiles on the x-axis.
theoretical_quantile <- qnorm(p,20.9,5.7)
plot(theoretical_quantile,sample_quantile)
```
## Key points: Random Variables:
Random variables are numeric outcomes resulting from random processes.

Statistical inference offers a framework for quantifying uncertainty due to randomness.

```{r}
# defines random variable x that is 1 if it is blue, otherwise 0
beads <- rep(c("red","blue"),times=c(2,3))
beads
x <- ifelse(sample(beads,1)=="blue",1,0)
x
# x is a random variable and it will vary everytime!
x <- ifelse(sample(beads,1)=="blue",1,0)
x
x <- ifelse(sample(beads,1)=="blue",1,0)
x
x <- ifelse(sample(beads,1)=="blue",1,0)
x  


```
## Key points: Sampling Models
A sampling model models the random behavior of a process as the sampling of draws from an urn.

The probability distribution of a random variable is the probability of the observed value falling in any given interval.

We can define a CDF F(a)=Pr(S≤a) to answer questions related to the probability of S being in any interval.

The average of many draws of a random variable is called its expected value.

The standard deviation of many draws of a random variable is called its standard error.

###Example: we can generate a sampling model for the random variable S, showing the casino's total winnings. 
```{r}
### Monte Carlo simulation: Chance of casino losing money on roulette
# suppose 1000 people are playing and only they can bet on red or black. The casino wants to predeict how much money they can win or lose: thay wanna know the chance of loosing money.
# S: casiona total winning
#earn for sampling model: 18 red pocket, 18 black, and 2 greens 

### sampling model 1: define urn, then sample
color <- rep(c("red","black","green"),c(18,18,2))
color

n <- 1000
X <- sample(ifelse(color=="red",-1,1),n,replace=TRUE)
X[1:10] #first 10 oucomes of these 1000 draws

```

```{r}
### sampling model 2: define urn inside sample function by noting probabilities and not colors
n <- 1000
X <- sample(c(-1,1),n,replace=TRUE,prob=c(9/19,10/19)) #1000 dependent draw
S <- sum(X)  #total winning: sum of draws
S
#18/(38) of winning casino and 20/(38) of loosing. every green color is corresponding with loosing. but poeple only can choose from 38.
```

```{r}
###We use sampling models to run Monto Carlo Simulation and based on the result to calculate the probability of casiono losing money.

B <- 10000 #repetition over Monto Carlo
n <- 1000  #number of people
S <- replicate(B,{
  X <- sample(c(-1,1),n,replace=TRUE,prob=c(9/19,10/19))
  sum(X)
})

mean(S<0)  #probability of casino losing 
```

```{r}
### we can plot a histogram of observed values of S and also normal density curve based on mean and standard deviation of S.
library(tidyverse)
s <- seq(min(S),max(S),length=100) #sequence of 100 in the range of S
normal_density <- data.frame(s=s,f=dnorm(s,mean(S),sd(S))) #producing normal density for S

data.frame(S=S) %>%
  ggplot(aes(S,..density..)) +  #it can be count or density
  geom_histogram(color="black",binwidth=10) +
  ylab("Probability") +
  geom_line(data=normal_density,mapping=aes(s,f),color="blue")
```  
## Key points: Distributions versus Probability Distributions
A random variable X has a probability distribution function 
F(a) that defines Pr(X≤a) over all values of a.

Any list of numbers has a distribution. The probability distribution function of a random variable is defined mathematically and does not depend on a list of numbers.

The results of a Monte Carlo simulation with a large enough number of observations will approximate the probability distribution of X.

If a random variable is defined as draws from an urn:
- The probability distribution function of the random variable is defined as the distribution of the list of values in the urn.

- The expected value of the random variable is the average of values in the urn.

- The standard error of one draw of the random variable is the standard deviation of the values of the urn.

## Key points:Notation for Random Variables
- Capital letters denote random variables (X) and lowercase letters denote observed values (x).

- In the notation Pr(X=x), we are asking how frequently the random variable X is equal to the value x. For example, if x=6, this statement becomes Pr(X=6).

##Key points: Central Limit Theorem
- The Central Limit Theorem (CLT) says that the distribution of the sum of a random variable is approximated by a normal distribution.

- The expected value of a random variable, E[X]=μ, is the average of the values in the urn.This represents the expectation of one draw.

- The standard error of one draw of a random variable is the standard deviation of the values in the urn.

- The expected value of the sum of draws is the number of draws times the expected value of the random variable. 

- The standard error of the sum of independent draws (very important assumption) of a random variable is the square root of the number of draws times the standard deviation of the urn. 

Equations: 
These equations apply to the case where there are only two outcomes, a and b with proportions p and 1−p respectively. The general principles above also apply to random variables with more than two outcomes.
- Expected value of a random variable: E(X) = ap+b(1−p)
because: (n×a×p + n×b×(1-p))/n
- Expected value of the sum of n draws of a random variable:
n×(ap+b(1−p))
- Standard deviation of an urn with two values: |b-a|sqrt(p(1-p))
- Standard error of the sum of n draws of a random variable:
sqrt(n)|b-a|sqrt(p(1-p))

## Exercise: American Roulette
An American roulette wheel includes 18 black, 18 red, and 2 green pockets that each red and black pocket is associated with a number from 1 to 36. Two remaining green pockets, represent "0" and "00". Players bet on pockets which they think a ball will be landed after spunning the wheel. Players can bet on a specific number(0, 00, 1-36) or color (red, black, or green). 


```{r}
### what is the chance of landing balls in green pockets:
green <- 2
red <- 18
black <- 18
P_green <- green/(green+red+black)
P_green
```
 In the game, the payout to winning on green is 17 dollars. It means that if you bet 1 dollar and it lands on green, you will win 17 dollars. if you land on others, you will lose 1 dollar.
 
```{r}
### Create a model to predict your winnings from betting on green one time:
set.seed(1,sample.kind="Rounding")
green <- 2
red <- 18
black <- 18

p_green <- green/(green+red+black)
p_not_green <- 1- p_green

X <- sample(c(17,-1), 1, prob=c(p_green,p_not_green)) #this is only 1 sample and do not need replace=TRUE
X

### compute the expected value of X:
E_X <- 17 * p_green + (-1)*p_not_green #E[X]=(ap)+(b(1-p))
E_X

### compute the standard error of random variable of X that shows a single outcome after one spin of the roulette wheel. 
abs(17-(-1))*sqrt(p_green*p_not_green)  # |b-a|sqrt(p(1-p))

```
```{r}
### Predict sum of winnings
### create a random variable S which sums your winnings after betting on green 1,000 times.
set.seed(1,sample.kind="Rounding")
green <- 2
red <- 18
black <- 18

p_green <- green/(green+red+black)
p_not_green <- 1- p_green

n <- 1000

X <- sample(c(17,-1),1000,replace=TRUE,prob=c(p_green,p_not_green))
S <- sum(X)
S

### What is the expected value of S?
E_S <- n *((p_green*17)+(p_not_green)*(-1))
E_S

### What is the standard error of S?
se_S <- sqrt(n)*abs(-1+-17)*sqrt(p_green*p_not_green)
se_S
# sqrt(n)|b-a|sqrt(p(1-p))
```
## Key points: Averages and Proportions:

### Random variable times a constant
- The expected value of a random variable multiplied by a constant is that constant times its original expected value: E[aX]=aE[X]=aμ
for example: change of units of random variables from ft to inch
In the same urn: E[(X1+X2+...+Xn)/n]=E[X1+X2+...+Xn]/n=nμ/n=μ

- The standard error of a random variable multiplied by a constant is that constant times its original standard error: SE[aX]=a.SE[X]=a.σ

### Average of multiple draws of a random variable
- The expected value of the average of multiple draws from an urn is the expected value of the urn (μ).

- The standard deviation of the average of multiple draws (independent draws) from an urn (same urn) is the standard deviation of the urn, σ, divided by the square root of the number of draws (σ/√n)
SE[(X1+X2+...+Xn)/n]=SE[(X1+X2+...+Xn)]/n=sqrt(SE[X1]^2+SE[X2]^2+...+SE[Xn]^2)/n=[sqrt(σ1^2+σ2^2+...+σn^2)]/n=[sqrt(σ^2+σ^2+...+σ^2)]/n=[sqrt(n.σ^2)]/n=σ/sqrt(n)

### The sum of multiple draws of a random variable
- The expected value of the sum of n draws of a random variable is n
times its original expected value: E[nX]=nμ

- The standard error of the sum of n draws of random variable is 
√n times its original standard error: SE[nX]=√nσ

### The sum of multiple different random variables
- The expected value of the sum of different random variables is the sum of the individual expected values for each random variable:
E[X1+X2+...+Xn]=E[X1]+E[X2]+...+E[Xn]=μ1+μ2+...+μn
if all of random variables drawn from a urn:
E[X1+X2+...+Xn]=nμ

- The standard error of the sum of different (Independenct) random variables is the square root of the sum of squares of the individual standard errors:
SE[X1+X2+...+Xn]=sqrt(SE[X1]^2+SE[X2]^2+...+SE[Xn]^2)=sqrt(σ1^2+σ2^2+...+σn^2)
varience[X1]=SE[X1]^2


### Transformation of random variables
- If X is a normally distributed random variable and a and b are non-random constants, then aX+b is also a normally distributed random variable.

## Key points: Law of Large Numbers
- The law of large numbers states that as n increases, the standard error of the average of a random variable decreases. In other words, when n is large, the average of the draws converges to the average of the urn.

- The law of large numbers is also known as the law of averages.

- The law of averages only applies when n is very large and events are independent. It is often misused to make predictions about an event being "due" because it has happened less frequently than expected in a small sample size.

## Key points:How Large is Large in CLT?
The sample size required for the Central Limit Theorem and Law of Large Numbers to apply differs based on the probability of success.
- If the probability of success is high, then relatively few observations are needed.
- As the probability of success decreases, more observations are needed.

If the probability of success is extremely low, such as winning a lottery, then the Central Limit Theorem may not apply even with extremely large sample sizes. The normal distribution is not a good approximation in these cases, and other distributions such as the Poisson distribution (not discussed in these courses) may be more appropriate.

### Exercise: American Roulette probability of winning money (CLT)
```{r}
### What is the probability that you end up winning money if you bet on green 100 times?
set.seed(1,sample.kind="Rounding")
p_green <- 2/38  #assume
p_not_green <- 1-p_green
n <- 100

# avg: the expected outcome of 100 spins if you win $17 when the ball lands on green and you lose $1 when the ball doesn't land on green
avg <- n*((17*p_green)+(-1*p_not_green)) #n×(ap+b(1−p))
se <- sqrt(n)*abs(17- -1)*sqrt(p_green*p_not_green)   #sqrt(n)|b-a|sqrt(p(1-p)) standard error of sum of n outcomes

### the probability of ending on winning money on betting 100 times on green
pnorm(0,avg,se) #loosing (pr <= 0$)
1-pnorm(0,avg,se)  #1-loosing=winning
```
### Exercise: American Roulette Monte Carlo simulation:
```{r}
### Generate monto Carlo simulation that genrate 10000 outcomes of S (summ of 100 bets). compare the expected value and standar deviation of list with expected value and standard devaition calculated for S?

set.seed(1,sample.kind="Rounding")
p_green <- 2/38  #assume
p_not_green <- 1-p_green
n <- 100
B <- 10000
S <- replicate(B,{
  X <- sample(c(17,-1),n,replace=TRUE,prob=c(p_green,p_not_green))
  sum(X)
  })
avg_monto <- mean(S)
avg_monto
sd_monto <- sd(S)
sd_monto

### probability of winning money from the Monte Carlo simulation
mean(S>0)
```
Result: The proability of winning from Monto-Carlo and CLT are close but not exactly the same:
because: The CLT does not work as well when the probability of success is small. If we make the number of roulette plays bigger, the probabilities will match better.

### Exercise: American Roulette average winnings per bet
```{r}
### define a random variable Y containing the average winning per bet after betting on green 10,000 times.
set.seed(1,sample.kind="Rounding")
p_green <- 2/38  #assume
p_not_green <- 1-p_green

X <- sample(c(17,-1),n,replace=TRUE,prob=c(p_green,p_not_green)) # we dont need to use replicate and we can use just 10000 in sample
Y <- mean(X)
Y

### What is the expected value of the average outcome per bet after betting on green 10,000 times?
E_X <- p_green*17+p_not_green*-1  #refer to Average of multiple draws of a random variable
E_X

### What is the standard error of Y, the average result of 10,000 spins?
se <- abs(17- -1)*sqrt(p_green*p_not_green)/sqrt(10000) #σ/√n = |b-a|√(p(1-p))/√n

### What is the probability that your winnings are positive after betting on green 10,000 times?
1-pnorm(0,E_X,se)
```
### Exercise: American Roulette Monte Carlo again
```{r}
### generate a Monte Carlo simulation producing 10,000 outcomes of S, the average outcome from 10,000 bets on green. Calculate the avg and standard error and compare with CLT results?

set.seed(1,sample.kind="Rounding")
p_green <- 2/38  #assume
p_not_green <- 1-p_green

B <- 10000
n <- 10000
S <- replicate(B,{
  X <- sample(c(17,-1),n,replace=TRUE,prob=c(p_green,p_not_green))
  mean(X)
})
avg <- mean(S)
avg
se <- sd(S)
se

### What is the probability of winning more than $0 estimated using  Monte Carlo simulation? 
mean(S>0)
```

Result: the result of Monto-Carlo simulation and CLT are much more closer because The CLT works better when the sample size is larger.


