---
title: "6_Linear_Regression"
author: "Elham Sharifin" The notes are exactly came from the Edx course,
date: "5/13/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Key point: Linear Regression
Bill James was the originator of the sabermetrics, the approach of using data to predict what outcomes best predicted if a team would win.

## Key points: Baseball Basics
- The goal of a baseball game is to score more runs (points) than the other team.
- Each team has 9 batters who have an opportunity to hit a ball with a bat in a predetermined order. 
- Each time a batter has an opportunity to bat, we call it a plate appearance (PA).
- The PA ends with a binary outcome: the batter either makes an out (failure) and returns to the bench or the batter doesn’t (success) and can run around the bases, and potentially score a run (reach all 4 bases).
- We are simplifying a bit, but there are five ways a batter can succeed (not make an out):
- Bases on balls (BB): the pitcher fails to throw the ball through a predefined area considered to be hittable (the strike zone), so the batter is permitted to go to first base.
-- Single: the batter hits the ball and gets to first base.
-- Double (2B): the batter hits the ball and gets to second base.
-- Triple (3B): the batter hits the ball and gets to third base.
-- Home Run (HR): the batter hits the ball and goes all the way home and scores a run.

Historically, the batting average has been considered the most important offensive statistic. To define this average, we define a hit (H) and an at bat (AB). Singles, doubles, triples and home runs are hits. The fifth way to be successful, a walk (BB: base in the ball), is not a hit. An AB is the number of times you either get a hit or make an out; BBs are excluded. The batting average is simply H/AB and is considered the main measure of a success rate.

## Key points:Bases on Balls or Stolen Bases?
The visualization of choice when exploring the relationship between two variables like home runs and runs is a scatterplot.

### the relationship between HRs and wins:
```{r}
library(Lahman) #library related to basebal satistics
library(tidyverse)
library(dslabs)
ds_theme_set()

## plot of Runs per game vs. Home Runs per game
Teams %>%
  filter(yearID %in% 1961:2001) %>%
  mutate(HR_per_game=HR/G,R_per_game=R/G) %>% 
  ggplot(aes(HR_per_game,R_per_game)) +
  geom_point(alpha=0.5)

```

### relationship between stolen bases and wins:
```{r}
## plot stolen base per game vs home run per game
Teams %>%
  filter(yearID %in% 1961:2001) %>%
  mutate(SB_per_game=SB/G,R_per_game=R/G) %>% 
  ggplot(aes(SB_per_game,R_per_game)) +
  geom_point(alpha=0.5)
```
### relationship between bases on balls and runs:
```{r}
## plot Base on Balls vs home run per game
Teams %>%
  filter(yearID %in% 1961:2001) %>%
  mutate(BB_per_game=BB/G,R_per_game=R/G) %>% 
  ggplot(aes(BB_per_game,R_per_game)) +
  geom_point(alpha=0.5)
```

### plot runs per game versus at bats (AB) per game
```{r}
Teams %>%
  filter(yearID %in% 1961:2001) %>%
  mutate(AB_per_game=AB/G,R_per_game=R/G) %>% 
  ggplot(aes(AB_per_game,R_per_game)) +
  geom_point(alpha=0.5)

```
### plot win rate (number of wins per game) versus number of fielding errors (E) per game.
```{r}
Teams %>%
  filter(yearID %in% 1961:2001) %>%
  mutate(Error_per_game=E/G,W_per_game=W/G) %>% 
  ggplot(aes(Error_per_game,W_per_game)) +
  geom_point(alpha=0.5)
```

### relationships between triples (X3B) per game versus doubles (X2B) per game.
```{r}
Teams %>%
  filter(yearID %in% 1961:2001) %>%
  mutate(tripple_per_game=X3B/G,doubles_per_game=X2B/G) %>% 
  ggplot(aes(tripple_per_game,doubles_per_game)) +
  geom_point(alpha=0.5)
```

## Key points: Correlation
- Galton tried to predict sons' heights based on fathers' heights.
- The mean and standard errors are insufficient for describing an important characteristic of the data: the trend that the taller the father, the taller the son.
- The correlation coefficient is an informative summary of how two variables move together that can be used to predict one variable using the other.

```{r}
library(HistData)
data("GaltonFamilies")
Galton_heights <- GaltonFamilies %>%
  filter(childNum==1 & gender=="male") %>%
  select(father,childHeight) %>%
  rename(son=childHeight)

# means and standard deviations of heights
Galton_heights %>%
  summarise(mean(father),sd(father),mean(son),sd(son))
```
```{r}
# scatterplot of father and son heights
Galton_heights %>%
  ggplot(aes(father,son)) +
  geom_point(alpha=0.5)
```

## Key points: Correlation Coefficient
- The correlation coefficient is defined for a list of pairs $(x_1,y_1),...,(x_n,y_n)$ as the product of the standardized values: 
$(\frac{x_i-\mu_x}{\sigma_x})(\frac{y_i-\mu_y}{\sigma_y})$

- The correlation coefficient essentially conveys how two variables move together.

- The correlation coefficient is always between -1 and 1.
```{r}
library(HistData)
data("GaltonFamilies")
Galton_heights <- GaltonFamilies %>%
  filter(childNum==1 & gender=="male") %>%
  select(father,childHeight) %>%
  rename(son=childHeight)

## correlation between the heights of son and his father
Galton_heights %>%
  summarize(cor(father,son))
```

## Key points: Sample Correlation is a Random Variable
- The correlation that we compute and use as a summary is a random variable.

- When interpreting correlations, it is important to remember that correlations derived from samples are estimates containing uncertainty. (keep in mide that correlatio has about high standard error).

- Because the sample correlation is an average of independent draws, the central limit theorem applies. So, for a sample of large enough, the distribution of the sampel is Normal. 
$\sim N(\rho,\sqrt{\frac{1-r^2}{N-2}})$
- r=mean(R) 
- $\rho=$population correlation
- $sd=\sqrt{\frac{1-r^2}{N-2}$

```{r}
## sampling from the Galton heights:
set.seed(0, sample.kind="Rounding")
R <- sample(Galton_heights,25,replace=TRUE) %>%
  summarize(cor(father,son))
R
```
```{r}
## Run a Monto Carlo simulation to see the distribution of sample correlation
B <- 1000
N <- 25
R <- replicate(B,{
  sample_n(Galton_heights,N,replace=TRUE) %>%
    summarize(r=cor(father,son)) %>% .$r
})

## plot the distribution of sample correlation
data.frame(R) %>%
  ggplot(aes(R)) +
  geom_histogram(binwidth = 0.05, color="black")

## expected value and standard error of corrolation
mean(R)
sd(R)
```
```{r}
# QQ-plot to evaluate whether N is large enough
data.frame(R) %>%
    ggplot(aes(sample = R)) +
    stat_qq() +
    geom_abline(intercept = mean(R), slope = sqrt((1-mean(R)^2)/(N-2)))
#result: N=25 is not a large enough for a good approximation
```

Note: If we increase the size of sample to for example N=50, 
- the mean of our sample stay approximately the same: because the expected value of the sample correlation is the population correlation, it should stay approximately the same even if the sample size is increased. 
- As the sample size N increases, the standard deviation of the sample correlation should decrease. 

### Exercise: correlation between varaible in Basebal statistics
```{r}
## correlation between rus per game and at bat per game
library(Lahman)
Teams %>%
  filter(yearID%in%1961:2001) %>%
  mutate(AB_per_game=AB/G,R_per_game=R/G) %>% 
  summarize(r=cor(AB_per_game,R_per_game))

## correlation voefficient between win rate (number of wins per game) and number of errors per game
Teams %>%
  filter(yearID%in%1961:2001) %>%
  mutate(win_per_game=W/G,error_per_game=E/G) %>% 
  summarize(r=cor(win_per_game,error_per_game))

## correlation coef between doubles (X2B) per game and triples (X3B) per game
Teams %>%
  filter(yearID%in%1961:2001) %>%
  mutate(double_per_game=X2B/G,triple_per_game=X3B/G) %>% 
  summarize(r=cor(double_per_game,triple_per_game))
```

## Key points: Anscombe's Quartet/Stratification
- Correlation is not always a good summary of the relationship between two variables.

- The general idea of conditional expectation is that we stratify a population into groups and compute summaries in each group.

- A practical way to improve the estimates of the conditional expectations is to define strata of with similar values of x.

- the formula for regression line is:
$(\frac{y_i-\mu_y}{\sigma_y})=\rho(\frac{x_i-\mu_x}{\sigma_x})$

- If there is perfect correlation, the regression line predicts an increase that is the same number of SDs for both variables. 
- If there is 0 correlation, then we don’t use x at all for the prediction and simply predict the average $\mu_y$. 
- For values between 0 and 1, the prediction is somewhere in between
- If the correlation is negative, we predict a reduction instead of an increase.

- we can write the formula as: $y=b+mx$ (b=intercept, m=slope)
 slop: $m=\rho\frac{\sigma_y}{\sigma_x}$ 
 intercept: $b=\mu_y-m\mu_x$

_ if we write the variables in the standard format (mean=0,sd=1):
The regression line will have the intercept of zero and slope of $\rho$, regression coef.


```{r}
## number of fathers that have height 72 or 72.5 inches
sum(Galton_heights$father==72)
sum(Galton_heights$father==72.5)

## predicted height of a son with a father of 72 inch height 
conditional_avg <- Galton_heights %>%
  filter(round(father)==72) %>%
  summarize(avg=mean(son)) %>% .$avg
conditional_avg
```
```{r}
## stratify fathers' heights to generate a boxplot of son heights
Galton_heights %>%
  mutate(father_strata=factor(round(father))) %>%
  ggplot(aes(father_strata,son)) +
  geom_boxplot() +
  geom_point()

# as we see some of the limitation of conditional averages ares:
# Each stratum we condition on may not have many data points. 
# Because there are limited data points for each stratum, our average values have large standard errors. 
#Conditional means are less stable than a regression line. 
```
```{r}
## center of each boxplot
Galton_heights %>%
  mutate(father=round(father)) %>%
  group_by(father) %>%
  summarize(son_conditional_avg=mean(son)) %>%
  ggplot(aes(father,son_conditional_avg)) +
  geom_point()
# The mean of each group appears to follow a linear relationship. The slope of line is 0.5 which happens to be the correlation between father and son height.
```
```{r}
r <- Galton_heights %>%
  summarise(r=cor(father,son)) %>% .$r

Galton_heights %>%
  mutate(father=round(father)) %>%
  group_by(father) %>%
  summarize(son=mean(son)) %>%
  mutate(z_father=scale(father),z_son=scale(son)) %>%
  ggplot(aes(z_father,z_son)) +
  geom_point() +
  geom_abline(intercept = 0, slope=r)
  
```
```{r}
## computing values to plot regression line on original data
mu_x <- mean(Galton_heights$father)
mu_y <- mean(Galton_heights$son)
s_x <- sd(Galton_heights$father)
s_y <- sd(Galton_heights$son)
r <- cor(Galton_heights$father, Galton_heights$son)
m <- r * s_y/s_x
b <- mu_y - m*mu_x

## adding regression line to plot
Galton_heights %>%
    ggplot(aes(father, son)) +
    geom_point(alpha = 0.5) +
    geom_abline(intercept = b, slope = m)
```
```{r}
## adding regression line to plot of standarded data (avg=0,sd=1)
Galton_heights %>%
  ggplot(aes(scale(father),scale(son))) +
           geom_point(alpha=0.5) +
  geom_abline(intercept = 0, slope=r)
           
```

## Key points: Bivariate Normal Distribution
- When a pair of random variables are approximated by the bivariate normal distribution, scatterplots look like ovals. They can be thin (high correlation) or circle-shaped (no correlation). (However, The v-shaped distribution of points means that the x and y variables do not follow a bivariate normal distribution).

- If X and Y are normally distributed random variables, and for any group of X, X=x, Y is approximately normal in that group, then the pair is approximately bivariate normal.

- When two variables follow a bivariate normal distribution, computing the regression line is equivalent to computing conditional expectations.
$f_{Y|X=x}$ is the conditional distibution
and $E(Y|X=x)$ is conditional expected value

- We can obtain a much more stable estimate of the conditional expectation by finding the regression line and using it to make predictions.
```{r}
Galton_heights %>%
  mutate(z_father=round((father-mean(father))/sd(father))) %>%
  filter(z_father %in% -2:2) %>%
  ggplot() +
  stat_qq(aes(sample=son)) +
  facet_wrap(~z_father)
         
```

## Key points: Variance explained
- Conditioning on a random variable X can help to reduce variance of response variable Y.

- The standard deviation of the conditional distribution is $SD(Y|X=x)=\sigma_y\sqrt{1-\rho^2}$, which is smaller than the standard deviation without conditioning $\sigma_y$.

- Because variance is the standard deviation squared, the variance of the conditional distribution is $\sigma^2_y(1-\rho^2)$.

- In the statement "X explains such and such percent of the variability," the percent value refers to the variance. The variance decreases by $\rho^2$ percent.

- When two variables follow a bivariate normal distribution, the variation explained can be calculated as $\rho^2*100$.

- The “variance explained” statement only makes sense when the data is approximated by a bivariate normal distribution.

## Key point: There are Two Regression Lines
There are two different regression lines depending on whether we are taking the expectation of Y given X or taking the expectation of X given Y.
```{r}
# calculate a regression line for prediction of the son's height from the father's height
mu_x <- mean(Galton_heights$father)
mu_y <- mean(Galton_heights$son)
s_x <- sd(Galton_heights$father)
s_y <- sd(Galton_heights$son)
r <- cor(Galton_heights$father, Galton_heights$son)
m_1 <-  r * s_y / s_x
b_1 <- mu_y - m_1*mu_x

m_1
b_1
#E(Y|X=x)=35+0.5*x

# compute a regression line to predict the father's height from the son's height
m_2 <-  r * s_x / s_y
b_2 <- mu_x - m_2*mu_y

m_2
b_2
#E(X|Y=y)=34+0.5y
```

### Exercise:
Assume that the correlation between father's height and his son’s height is 0.5, the standard deviation of fathers’ heights and son's heights are is 2, and 3 inches respectively.
Given a one inch increase in a father’s height, what is the predicted change in the son’s height?
```{r}
## The slope of the regression line is calculated by multiplying the correlation coefficient by the ratio of the standard deviation of son heights and standard deviation of father heights: 
r=0.5
sigma_father=2
sigma_son=3
r*sigma_son/sigma_father
```

### Exercise 2: Mother's height and daughter's height
```{r}
set.seed(1989, sample.kind="Rounding")
library(HistData)
data("GaltonFamilies")

female_heights <- GaltonFamilies%>%     
    filter(gender == "female") %>%     
    group_by(family) %>%     
    sample_n(1) %>%     
    ungroup() %>%     
    select(mother, childHeight) %>%     
    rename(daughter = childHeight)

## mean and standard deviation of mothers' heights
mean(female_heights$mother)
sd(female_heights$mother)

## mean and standard deviation of dauther' heights
mean(female_heights$daughter)
sd(female_heights$daughter)

## correlation coef of mather's height and dauther's height
cor(female_heights$daughter,female_heights$mother)

```

```{r}
##calculate the slope and intercept of the regression line predicting daughters' heights given mothers' heights.
mu_x <- mean(female_heights$mother)
mu_y <- mean(female_heights$daughter)
s_x <- sd(female_heights$mother)
s_y <- sd(female_heights$daughter)
r <- cor(female_heights$daughter,female_heights$mother)

m_1 <-  r * s_y / s_x
b_1 <- mu_y - m_1*mu_x

m_1
b_1

##Given an increase in mother's height by 1 inch, how many inches is the daughter's height expected to change?
r*(s_y/s_x)

## the percent of the variability in daughter heights which can be  explained by the mother's height.(The percent of variability can be calculated using r^2*100.)
r^2*100

## A mother has a height of 60 inches. What is the conditional expected value of her daughter's height given the mother's height?
b_1+(m_1*60)

```

## Key points: Confounding: Are BBs More Predictive?
-Association is not causation!

-Although it may appear that BB cause runs, it is actually the HR that cause most of these runs. We say that BB are confounded with HR.

-Regression can help us account for confounding.
```{r}
library(Lahman)
library(tidyverse)

## the scatterplot of walk by batters (BB) per game and Run per game:
Teams %>%
  filter(yearID %in% 1961:2001) %>%
  mutate(BB_per_game=BB/G,R_per_game=R/G) %>% 
  ggplot(aes(BB_per_game,R_per_game)) +
  geom_point(alpha=0.5) 
```
```{r}
## calculating the slope of regression line on the BB, and R
bb_slope <- Teams %>%
  filter(yearID %in% 1961:2001) %>%
  mutate(BB_per_game=BB/G,R_per_game=R/G) %>% 
  lm(R_per_game~BB_per_game,data=.) %>%
  .$coef %>%
  .[2]
bb_slope 
```
```{r}
## calculating the slope of regression line for prediction of Single runs per game vs. runs per game:
Singles_slope <- Teams %>%
  filter(yearID %in% 1961:2001) %>%
  mutate(Singles_per_game=(H-HR-X2B-X3B)/G, R_per_game=R/G) %>%
  lm(R_per_game~Singles_per_game,data=.) %>%
  .$coef %>%
  .[2]
Singles_slope
```
```{r}
## calculating correlation between HR,BB, and Single runs
Teams %>%
  filter(yearID %in% 1961:2001) %>%
  mutate(Singles=(H-HR-X2B-X3B)/G, BB=BB/G, HR=HR/G) %>%
  summarize(cor(BB,HR),cor(Singles,HR),cor(BB,Singles))
```
## Key points: Stratification and Multivariate Regression
- A first approach to check confounding is to keep HRs fixed at a certain value and then examine the relationship between BB and runs.
- The slopes of BB after stratifying on HR are reduced, but they are not 0, which indicates that BB are helpful for producing runs, just not as much as previously thought. In fact, the slope of runs per game vs. bases on balls within each stratum was reduced because we removed confounding by home runs.

```{r}
## stratifing  HR per game to nearest 10, filter out strata with few points
dat <- Teams %>%
  filter(yearID%in%1961:2001) %>%
  mutate(HR_strata=round(HR/G,1),
         BB_per_game=BB/G,
         R_per_game=R/G) %>%
  filter(HR_strata>=0.4&HR_strata<=1.2)

## creating a scatter plot for each strata (BB_per_game vs. R_per_game)
dat %>%
  ggplot(aes(BB_per_game,R_per_game)) +
  geom_point(alpha=0.5) +
  geom_smooth(method="lm") +
  facet_wrap(~HR_strata)

# remember: the slope of regression line of BB vs. R per game was 0.75 with ignoring Home run
```

```{r}
## finging out the slope of regression line of the above regression lines:
dat %>%
  group_by(HR_strata) %>%
  summarize(slope=cor(BB_per_game,R_per_game)*sd(R_per_game)/sd(BB_per_game))

# these values are closer to slope obtained from Singles (0.449)
```

```{r}
## stratifying by BB
dat <- Teams %>%
  filter(yearID%in%1961:2001) %>%
  mutate(BB_strata=round(BB/G,1),
         HR_per_game=HR/G,
         R_per_game=R/G) %>%
  filter(BB_strata>=2.8&BB_strata<=3.9)

## creating a scatter plot for each strata (HB_per_game vs. R_per_game)
dat %>%
  ggplot(aes(HR_per_game,R_per_game)) +
  geom_point(alpha=0.5) +
  geom_smooth(method="lm") +
  facet_wrap(~BB_strata)
```
```{r}
## slope of regression line after stratification by BB
dat %>%
  group_by(BB_strata) %>%
  summarize(slope=cor(HR_per_game,R_per_game)*sd(R_per_game)/sd(HR_per_game))

# they do not chnage that much from the original slope that was 1.84
```

## Key points: Linear Models
- Regression allows us to find relationships between two variables while adjusting for others.

- “Linear” here does not refer to lines, but rather to the fact that the conditional expectation is a linear combination of known quantities.

- In Galton's model, we assume Y (son's height) is a linear combination of a constant and X (father's height) plus random noise. We further assume that $\epsilon_i$ are independent from each other, have expected value 0 and the standard deviation $\sigma$ which does not depend on i.

- Note that if we further assume that $\epsilon$ is normally distributed, then the model is exactly the same one we derived earlier by assuming bivariate normal data.

- We can subtract the mean from X to make $\beta_0$ more interpretable. 

_ the assumptopnf for $\epsilon_i$ (error) are:
--- The $\epsilon_i$ are independent from each other
--- The $\epsilon_i$ have expected value of 0 and sd of $\sigma$
--- the varianc eof $\epsilon_i$ is a constant

```{r}
##  for the Galto heights, the coesfficient of regression line is:
lm(son ~ father, data = Galton_heights)
# it means with every inch increase in father's height, the son's height increase by 0.5 inch. 
```
```{r}
## running a linear model using centered fathers’ height variable
Galton_heights2 <- Galton_heights %>%
    mutate(father_centered=father - mean(father))

## finding out the coefficient of regression line:
 lm(son ~ father_centered, data = Galton_heights2)
```

## Key points: Least Squares Estimates (LSE)
- For regression, we aim to find the coefficient values that minimize the distance of the fitted model to the data.

- Residual sum of squares (RSS) measures the distance between the true value and the predicted value given by the regression line.
$RSS=\sum_{i=1}^{N}{\{Y_i-(\beta_0-\beta_1x_i)}\}^2$

- The values that minimize the RSS are called the least squares estimates (LSE). 

- The least squares estimates for parameters $\beta_0,\beta_1,..,\beta_n$ $minimize, not maximize, the residual sum of squares.

- We can use partial derivatives to get the values for $\beta_0$ and $\beta_1$ in Galton's data.

```{r}
library(HistData)
data("GaltonFamilies")
set.seed(1983)
Galton_heights <- GaltonFamilies %>%
  filter(gender == "male") %>%
  group_by(family) %>%
  sample_n(1) %>%
  ungroup() %>%
  select(father, childHeight) %>%
  rename(son = childHeight)

# calculation of RSS for any pair of beta0 and beta1 in Galton's data
rss <- function(beta0,beta1,data){
  resid <- Galton_heights2$son - (beta0+beta1*Galton_heights2$father)
  return(sum(resid^2))
}
#this is a 3D with x, y in the horizontal plan and RSS in the vertical axes.

## Plotting the rss vs. beta1 assuming w have a fix beta0 (2D).
beta1 <- seq(0,1,len=nrow(Galton_heights))
results <- data.frame(beta1=beta1,rss=sapply(beta1,rss,beta0=25))

results %>% ggplot(aes(beta1,rss)) +
  geom_line() +
  geom_line(aes(beta1,rss),col=2)
# as seen there is a minimum of 0.64 for rss (when beta0 is fixed at 25)
```

## Key points: The lm Function
- When calling the lm() function, the variable that we want to predict is put to the left of the ~ symbol, and the variables that we use to predict is put to the right of the ~ symbol. The intercept is added automatically. To fit the equation 

- LSEs are random variables.
```{r}
#$Y_i=\beta_0+\beta_1x_i+\epsilon_i$
fit <- lm(son ~ father, data=Galton_heights)
fit

## function summary gives more info about fit
summary(fit)
```

## Key points: LSE are Random Variables
- Because they are derived from the samples ($Y_1,Y_2, ..,Y_N$), LSE are random variables.

- $\beta_0$ and $\beta_1$ appear to be normally distributed because the central limit theorem plays a role. for Large enough N, the LSEs will be approximately normal with expected values $\beta_0$ and $\beta_1$.

- The t-statistic ate not based on CLT, and depends on the assumption that $\epsilon$ follows a normal distribution. In fact, $\hat{\beta_0}/\hat{SE}(\hat{\beta_0)}}$
$\hat{\beta_1}/\hat{SE}(\hat{\beta_1)}}$
have t distribution with N-p degrees of freedom. 
(p:Number of parameters in the model)

```{r}
## Monto carlo simulation of lse:
B <- 1000
N <- 50
lse <- replicate(B,{
  sample_n(Galton_heights,N,replace=TRUE) %>%
    lm(son ~ father, data=.) %>% .$coef
})

lse <- data.frame(beta_0=lse[1,],beta_1=lse[2,])

##  distribution of beta_0, beta_1
library(gridExtra)
p1 <- lse %>%
  ggplot(aes(beta_0)) +
  geom_histogram(binwidth=5,color="black")

p2 <- lse %>%
  ggplot(aes(beta_1)) +
  geom_histogram(binwidth=0.1,color="black")

grid.arrange(p1,p2,ncol=2)
```

```{r}
## standard error of coefficient beta_0 and beta_1
lse %>%
  summarize(se_0=sd(beta_0),se_1=sd(beta_1))
```

```{r}
## Estimated standard error of coefficient beta_0 and beta_1 and other info
sample_n(Galton_heights, N, replace=TRUE) %>%
  lm(son ~ father, data=.) %>% summary
```
## Advanced Note on LSE
Although interpretation is not straight-forward, it is also useful to know that the LSE can be strongly correlated, which can be seen using this code:
```{r}
lse %>% summarize(cor(beta_0, beta_1))
```
However, the correlation depends on how the predictors are defined or transformed.
- Here we standardize the father heights, which changes $x_i$ to $x_i-\bar{x}$.

```{r}
B <- 1000
N <- 50
lse <- replicate(B, {
      sample_n(Galton_heights, N, replace = TRUE) %>%
      mutate(father = father - mean(father)) %>%
      lm(son ~ father, data = .) %>% .$coef 
})
#Observe what happens to the correlation in this case:
cor(lse[1,], lse[2,]) 
```
## Key points: Predicted Variables are Random Variables
- The predicted value is often denoted as $\hat{Y}$, which is a random variable. Mathematical theory tells us what the standard error of the predicted value is.

- ggplot2 layer: geom_smooth(meth="lm") draw confidence intervals around the predicted variable $\hat{Y}$. 

- The predict() function in R can give us predictions directly.

```{r}
Galton_heights %>%
  ggplot(aes(son,father)) +
  geom_point() +
  geom_smooth(method="lm")

# as seen, the prediction line and confidenc eintervals are shown  
```

```{r}
# ploting the best fit line
Galton_heights %>%
  mutate(Y_hat=predict(lm(son~father,data=.))) %>%
  ggplot(aes(father,Y_hat)) +
  geom_line()
```
```{r}
##Predict Y directly (standard error and other parameters can be obtained from predict function)
fit <- Galton_heights %>%
  lm(son~father,data=.)
Y_hat <- predict(fit,se.fit=TRUE)
names(Y_hat)
```

### Exercise: find the value of least squares estimate (LSE) for 
$\beta_1$ if $\beta_0=36$
```{r}
## the function of rss
rss <- function(beta0,beta1,data){
  resid <- Galton_heights$son - (beta0+beta1*Galton_heights2$father)
  return(sum(resid^2))
}
  
beta1 = seq(0, 1, len=nrow(Galton_heights))
results <- data.frame(beta1 = beta1,
                      rss = sapply(beta1, rss, beta0 = 36))

results %>% ggplot(aes(beta1, rss)) +
  geom_line() + 
  geom_line(aes(beta1, rss), col=2)
```
### Exercise:
predict the number of runs per game based on both the number of bases on balls per game and the number of home runs per game?
In fact $Runs = \beta_0+\beta_{BB}.BB+\beta_{HR}.HR+\epsilon$
```{r}
library(Lahman)
fit <- Teams %>%  
  filter(yearID%in%1961:2001) %>%
  mutate(R=R/G,BB=BB/G,HR=HR/G) %>%
  lm(R ~ BB+HR, data=.) 
summary(fit)

```
### Example 3:
Run a Monte Carlo simulation of sample zise 100 from the Galton heights data and calculate the regression slope coefficients for each sample.
```{r}
N <- 100
B <- 1000
lse <- replicate(B,{
  sample_n(Galton_heights,N,replace=TRUE) %>%
  lm(son~father,data=.) %>% .$coef
})

lse <- data.frame(beta_0=lse[1,],beta_1=lse[2,])

p0 <- lse %>%
  ggplot(aes(beta_0)) +
  geom_histogram(binwidth=1.4)
p0

p1 <- lse %>%
  ggplot(aes(beta_1)) +
  geom_histogram(binwidth=0.03)

grid.arrange(p0,p1,ncol=2)
```

### exercise 4: 
we can plot father height and the regression line and its intervals by the following methods:

```{r}
#method2
Galton_heights %>% ggplot(aes(father, son)) +
    geom_point() +
    geom_smooth(method = "lm") #for linear model, it should have method="lm"
```
```{r}
#method3
model <- lm(son ~ father, data = Galton_heights)
predictions <- predict(model, interval = c("confidence"), level = 0.95)
data <- as_tibble(predictions) %>% bind_cols(father = Galton_heights$father)

ggplot(data, aes(x = father, y = fit)) +
    geom_line(color = "blue", size = 1) + 
    geom_ribbon(aes(ymin=lwr, ymax=upr), alpha=0.2) + 
    geom_point(data = Galton_heights, aes(x = father, y = son))
```

### Exercise 5: (Least Squares Estimates)
we are considering Female height data from galton_heights

```{r}
set.seed(1989, sample.kind="Rounding")
library(HistData)
data("GaltonFamilies")
options(digits = 3)    # report 3 significant digits

female_heights <- GaltonFamilies %>%     
    filter(gender == "female") %>%     
    group_by(family) %>%     
    sample_n(1) %>%     
    ungroup() %>%     
    select(mother, childHeight) %>%     
    rename(daughter = childHeight)
```
```{r}
## Fitting a linear regression model predicting the mothers' heights from daughters' heights. 
fit <- lm(mother~daughter,data=female_heights)
fit$coef[1]
fit$coef[2]
```
```{r}
## predicted height of the first mother in the dataset?
predict(fit)[1]

## the actual height of the first mother
female_heights$mother[1]
```
### Exercise 6: (Least Squares Estimates) Baseball example
we are going to compare the stability of singles and BBs. create two table including one for 2002 and other one for the average of 1999-2001 seasons. Lets define per plate appearance statistics, keeping only players with more than 100 plate appearances.

```{r}
## filtering the Batting data to contain year of 2002
library(Lahman)
bat_02 <- Batting %>%
  filter(yearID == 2002) %>%
    mutate(pa = AB + BB, 
           singles = (H - X2B - X3B - HR)/pa, 
           bb = BB/pa) %>%
    filter(pa >= 100) %>%
    select(playerID, singles, bb)

## filtering the Batting data to contain years of 1999:2001
bath_99_01 <- Batting %>%
  filter(yearID %in%1999:2001) %>%
   group_by(playerID) %>%
    mutate(pa = AB + BB, 
           singles = (H - X2B - X3B - HR)/pa, 
           bb = BB/pa) %>%
    filter(pa >= 100) %>%
  summarize(mean_singles=mean(singles),mean_bb=mean(bb))

## the number of playes who has the mean of singles more than 0.2 per plate appearance
bath_99_01 %>% filter(mean_singles>0.2) %>% nrow()

## the number of playes who has the mean of BB more than 0.2 per plate appearance
bath_99_01 %>% filter(mean_bb>0.2) %>% nrow()
```

```{r}
## joining the year of 2002 with the table of years 1999:2001
## calculate the correlation between the singles and bb?
bat_join <- inner_join(bat_02,bath_99_01) 
cor(bat_join$singles,bat_join$mean_singles)
cor(bat_join$bb,bat_join$mean_bb)
```
```{r}
## plotting mean_singles vs. singles and also mean_bb vs. bb
bat_join %>%
  ggplot(aes(bat_join$mean_singles,bat_join$singles)) +
  geom_point()
  
bat_join %>%
  ggplot(aes(bat_join$mean_bb,bat_join$bb)) +
  geom_point()
# because they are oval, both distributions are bivarial Normal
```

```{r}
## Fit a linear model for predicting 2002 singles based on given 1999-2001 mean_singles.
fit_singles <- lm(singles~mean_singles,data=bat_join)
fit_singles$coef[1]
fit_singles$coef[2]

fit_bb <- lm(bb~mean_bb,data=bat_join)
fit_bb$coef[1]
fit_bb$coef[2]
```
# Section 2
## Key points: Advanced dplyr: Tribbles
- Tibbles can be regarded as a modern version of data frames and are the default data structure in the tidyverse.

- Some functions that do not work properly with data frames do work with tibbles.

- tidyverse functions such as group_by and summarize always return tibbles.

- Note, select, filter, mutate, and arrange do not neccessarrly return tibbles and they preserve the class of input.
```{r}
## stratifying the Teams data by HR
library(Lahman)
dat <- Teams %>% filter(yearID%in%1961:2001) %>%
  mutate(HR=round(HR/G,1),
         BB=BB/G,
         R=R/G) %>%
  select(HR,BB,R) %>%
  filter(HR>=0.4 & HR<=1.2)

## Computing the slope of regression lines to predict runs by BB in different HR strata
dat %>%
  group_by(HR) %>%
  summarize(slope=cor(BB,R)*sd(R)/sd(BB))
```
```{r}
## Use lm fucntion to find estimated slopes - lm function ignores the grouped tibbles
dat %>%
  group_by(HR) %>%
  lm(R ~ BB, data=.) %>%
  .$coef
# reason: lm is not a part of tidyverse and does not know how to handle the outcome of group_by which is a group tibble
```
```{r}
## inspecting a grouped tibble
dat %>%
  group_by(HR) %>%
  head()
# as seen on the console, the type if Tibble of 6 in 3
```
```{r}
##
dat %>%
  group_by(HR) %>%
  class()
```

## Key points: Tibbles: Differences from Data Frames
- Tibbles are more readable than data frames.

- If you subset a data frame, you may not get a data frame. If you subset a tibble, you always get a tibble.

- Tibbles can hold more complex objects such as lists or functions.

- Tibbles can be grouped.

```{r}
## diff_1:  calling data by dataframe and tibble on the Console
Teams
as.tibble(Teams)
```
```{r}
##dif_2: subset of tibbles are also tibbles

#subsetting a data frame usually return a vector
class(Teams[,20]) 

#subsetting tibble always return tibble
class(as.tibble(Teams[,20])) 

# pulling a vector out of a tibble
class(as.tibble(Teams)$HR) 

```
```{r}
## accessing to a non-existing column in a data frame vs. tibble
Teams$hr
as.tibble(Teams)$hr
```
```{r}
## we can create more complex objects with tibbles
tibble(id=c(1,2,3),fucn=c(mean,median,sd))
```
```{r}
dat %>%
  group_by(HR) %>%
  lm(R~BB,data=.)
# as lm function is not a part of tidyverse, does not know how to handle with group tibbles. The objetc is basically converted to a dataframe and then the function runs ignoring groups. that's why it only get one pair of estimates.
```

## Key points: do functions
- The do() function serves as a bridge between R functions, such as lm(), and the tidyverse. The do function undestand group and always return a dataframe.

- We have to specify a column when using the do() function, otherwise we will get an error.

- If the data frame being returned has more than one row, the rows will be concatenated appropriately.
```{r}
## stratifying the Teams data by HR
library(Lahman)
dat <- Teams %>% filter(yearID%in%1961:2001) %>%
  mutate(HR=round(HR/G,1),
         BB=BB/G,
         R=R/G) %>%
  select(HR,BB,R) %>%
  filter(HR>=0.4 & HR<=1.2)

## we can use do to fit a regression line to each HR stratum
dat %>%
  group_by(HR) %>%
  do(fit=lm(R~BB,data=.))
```
```{r}
## using do without a column name returns an error
dat%>%
  group_by(HR) %>%
  do(lm( R ~ BB, data=.))
```
```{r}
## defining a function to extract slope from lm
get_slopes <- function(data){
  fit <- lm(R ~ BB, data=data)
  data.frame(slope=fit$coefficient[2],
             se=summary(fit)$coefficient[2,2])
}

## returning the desired data frame
dat %>%
  group_by(HR) %>%
  do(get_slopes(.))
```
```{r}
## returning not the desired output: a column containing data frames
dat %>%
  group_by(HR) %>%
  do(slope=get_slopes(.))
```
```{r}
## data frames with multiple rows will be concatenated appropriately
get_lse <- function(data){
  fit <- lm(R ~ BB, data=data)
  data.frame(term=names(fit$coefficients),
             slope=fit$coefficients,
             se=summary(fit)$coefficient[,2])
}

## getting the slope and intercept and also se of different HR
dat %>%
  group_by(HR) %>%
  do(get_lse(.))
```

## Key points: broom
- The broom package has three main functions, all of which extract information from the object returned by lm and return it in a tidyverse friendly data frame.

- The tidy() function returns estimates and related information as a data frame.

- The functions glance() and augment() relate to model specific and observation specific outcomes respectively.

```{r}
## using tidy to return lm estimates and related information as a data frame
library(broom)
fit <- lm(R ~ BB, data=dat)
tidy(fit)
```
```{r}
## add the confidence intervals with tidy
tidy(fit,conf.int = TRUE)
```
```{r}
## pipeline with lm, do, tidy
dat %>%
  group_by(HR) %>%
  do(tidy(lm(R ~ BB, data=.),conf.int=TRUE)) %>%
  filter(term=="BB") %>%
  select(HR,estimate,conf.low,conf.high)
```
```{r}
## make visualization with ggplot
dat %>%
  group_by(HR) %>%
  do(tidy(lm(R ~ BB, data=.),conf.int=TRUE)) %>%
  filter(term=="BB") %>%
  select(HR,estimate,conf.low,conf.high) %>%
  ggplot(aes(HR,y=estimate,ymin=conf.low,ymax=conf.high)) +
  geom_errorbar() +
  geom_point()
```
As a result, as seen the confidence intervals overlap, which provides a nice visual confirmation that our assumption that the slopes do not change with home run strata, is realtively safe.

```{r}
## glance function
fit <- lm(R ~ BB, data=dat)
glance(fit)
```

### Exercise:
Add the following parameters including (the coefficient, standard error, and p-value) for the BB term in the model.
```{r}
get_slope <- function(data) {
  fit <- lm(R ~ BB, data = data)
  sum.fit <- summary(fit)

  data.frame(slope = sum.fit$coefficients[2, "Estimate"], 
             se = sum.fit$coefficients[2, "Std. Error"],
             pvalue = sum.fit$coefficients[2, "Pr(>|t|)"])
}

dat %>% 
  group_by(HR) %>% 
  do(get_slope(.))
```

### Exercise: 
Investigate if the relationship between home runs and runs per game chnages by baseball league.
```{r}
dat <- Teams %>% filter(yearID %in% 1961:2001) %>%
  mutate(HR = HR/G,
         R = R/G) %>%
  select(lgID, HR, BB, R) 

dat %>% 
  group_by(lgID) %>% 
  do(tidy(lm(R ~ HR, data = .), conf.int = T)) %>% 
  filter(term == "HR") %>%   #gives us the slope
  select(lgID,estimate,conf.low,conf.high) %>%
  ggplot(aes(lgID,y=estimate,ymin=conf.low,ymax=conf.high)) +
   geom_errorbar() +
  geom_point()

```

### Exercise: Tibbles, do, and broom for height data
We want to incestigate the parent-child relationships. The Galton dataset is a sample of one male and one female child from each family in the GaltonFamilies dataset.

```{r}
library(tidyverse)
library(HistData)
set.seed(1, sample.kind = "Rounding") 

galton <- GaltonFamilies %>%
    group_by(family, gender) %>%
    sample_n(1) %>%
    ungroup() %>% 
    gather(parent, parentHeight, father:mother) %>%
    mutate(child = ifelse(gender == "female", "daughter", "son")) %>%
    unite(pair, c("parent", "child"))

head(galton)
```
```{r}
## group by pair and summarizing the observation base don the group
galton %>%
  group_by(pair) %>%
  summarize(n())
```
```{r}
## calculating the correlation coef. for fathers and daughters, fathers and sons, mothers and daughters and mothers and sons
galton %>%
  group_by(pair) %>%
  summarize(corr=cor(childHeight,parentHeight))
```
```{r}
## For each parent-child pair type, fit regression lines. calculate the least squares estimates, standard errors, confidence intervals and p-values for the parentHeight coefficient for each pair
library(broom)

## the different statistics
galton %>%
  group_by(pair) %>%
  do(tidy(lm(childHeight~parentHeight, data=.),conf.int=TRUE))
```
```{r}
## for every inch increase in mother's height, how many son's height will increase?
galton %>%
  group_by(pair) %>%
  do(tidy(lm(childHeight~parentHeight, data=.),conf.int=TRUE)) %>% filter(pair=="mother_son"& term=="parentHeight") %>% 
  .$estimate
```
```{r}
## Which sets of parent-child heights are significantly correlated at a p-value cut off of .05?
galton %>%
  group_by(pair) %>%
  do(tidy(lm(childHeight~parentHeight, data=.),conf.int=TRUE)) %>% select(pair,p.value) %>% filter(p.value<0.05)

# all the parent-child heights are correlated with a p-value of less than 0.05.
```
```{r}
## confidenct intervals
galton %>%
  group_by(pair) %>%
  do(tidy(lm(childHeight~parentHeight, data=.),conf.int=TRUE)) %>% filter(term=="parentHeight") %>%
  select(pair,estimate,conf.low,conf.high) %>%
  ggplot(aes(pair,y=estimate,ymin=conf.low,ymax=conf.high)) +
  geom_errorbar() +
  geom_point()
# since the confidence intervals overlap, the data are consistent with inheritance of height being independent of the child's or the parent's gender.
```

