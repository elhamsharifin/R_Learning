---
title: "Data_wrangling"
author: "Elham Sharifin, this course is taken from Edx data wrangling course. all the key points exactly the same of the course. I solved the questions myself!"
date: "4/22/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Key points: Introduction to wrnagling:
- The first step in data analysis is importing, tidying and cleaning the data. This is the process of data wrangling.

- Data analysis, for example checking correlations or creating visualizations, are done AFTER wrangling in which the data will be processed into a tidy format.

- In this course, we cover several common steps of the data wrangling process: tidying data, string processing, html parsing, working with dates and times, and text mining.

## Key points: Importing dat
- Many datasets are stored in spreadsheets. A spreadsheet is essentially a file version of a data frame with rows and columns.

- Spreadsheets have rows separated by returns and columns separated by a delimiter. The most common delimiters are comma, semicolon, white space and tab.

- Many spreadsheets are raw text files and can be read with any basic text editor. However, some formats are proprietary and cannot be read with a text editor, such as Microsoft Excel files (.xls).

- Most import functions assume that the first row of a spreadsheet file is a header with column names. To know if the file has a header, it helps to look at the file with a text editor before trying to import it.

## Key points: Paths and working directories:
- The working directory is where R looks for files and saves files by default.

- See your working directory with getwd(). Change your working directory with setwd().

- We suggest you create a directory for each project and keep your raw data inside that directory.

- Use the file.path() function to generate a full path from a relative path and a file name. Use file.path() instead of paste() because file.path() is aware of your operating system and will use the correct slashes to navigate your machine.

- The file.copy() function copies a file to a new path.
```{r}
#shows your working directory
getwd() 
```
```{r}
#changing the working directory
#setwd()
```
```{r}
# it shows the path of raw data inside the dslabs package and list files 
path <- system.file("extdata",package = "dslabs")
path 
list.files(path)
```
```{r}
# it will copy file from dslabs package to your working directory
filename <- "murders.csv"
fullpath <- file.path(path,filename)
file.copy(fullpath,getwd())
```
```{r}
#it shows if file exist in your working directory or not?
file.exists(filename)
```
```{r}
# copying the data murder.csv to an existing folder named data
getwd()
filename <- "murders.csv"
path <- system.file("extdata",package = "dslabs")
```
```{r}

# there are differnt ways for copying murders into data folder
## way1:
setwd("data")
file.copy(file.path(path, filename), getwd()) 
## way2:
file.copy(file.path(path, "murders.csv"), file.path(getwd(), "data"))
## way3:
file.location <- file.path(system.file("extdata", package = "dslabs"), "murders.csv")
file.destination <- file.path(getwd(), "data")
file.copy(file.location, file.destination) 

#way4: it does not work. it copies it into the parent directory
file.copy(file.path(path, "murders.csv"), getwd()) 
```

## Key points:The readr and readxl Packages
- readr is the tidyverse library that includes functions for reading data stored in text file spreadsheets into R. Functions in the package include:
read_csv(): comma seperated values, csv format
read_tsv(): tab delimited seperated values, tsv format
read_delim():general text file format,must define delimiter,txt format
read.csv2(): semicolon seperated values, csv format
read.table():white space seperated values, txt format
These differ by the delimiter they use to split columns.


- The readxl package provides functions to read Microsoft Excel formatted files.
read.excel: auto detect format. xlx, and xlsx formats
read.xls: original format (xls)
read.xlsx: new format, xlsx

- The excel_sheets() function gives the names of the sheets in the Excel file. These names are passed to the sheet argument for the readxl functions read_excel(), read_xls() and read_xlsx().

- The read_lines() function shows the first few lines of a file in R.

```{r}
library(dslabs)
library(tidyverse)    # includes readr
library(readxl)
```
```{r}
#it will investigate the first three lines
read_lines("murders.csv",n_max=3)
```
```{r}
#read files in csv format
dat <- read_csv(filename) #fiename from previous part
```
```{r}
# read file using fullpath
dat <- read_csv(fullpath) 
```
```{r}
head(dat)  #if we run in R console: we can see that file is a tibble
```
```{r}
#example: 
path <- system.file("extdata",package = "dslabs")
files <- list.files(path)
files

filename <- "murders.csv"
filename1 <- "life-expectancy-and-fertility-two-countries-example.csv"
filename2 <- "fertility-two-countries-example.csv"
dat <- read.csv(file.path(path,filename))
dat1 <- read.csv(file.path(path,filename1))
dat2 <- read.csv(file.path(path,filename2))

## excel files: suppose we have an excel file names times and the second sheet named 2016. we can read the second sheet with the following methos:
#times_2016 <- read_excel("times.xlsx", sheet = 2) 
#times_2016 <- read_excel("times.xlsx", sheet = "2016") 
```

## Key point: Importing Data Using R-base Functions
R-base import functions (read.csv(), read.table(), read.delim()) are different with read_... functions. R-base functions generate data frames rather than tibbles and character variables are converted to factors. This can be avoided by setting the argument stringsAsFactors=FALSE.
```{r}
filename <- "murders.csv"
dat2 <- read.csv(filename)
class(dat2)  #difference 1: we have data.frame not tibble
class(dat2$region) #character variables are converted to factors
```
```{r}
#converting to factors can be prevented by:
dat3 <- read.csv(filename,stringsAsFactors=FALSE)
class(dat3$region)
```

## Key points: Downloading Files from the Internet
- The read_csv() function and other import functions can read a URL directly.

- If there are no variable names in the first row fo a dataset, we have to add col_names=FALSE to skip putting the first row as header

- If you want to have a local copy of the file, you can use download.file().

- tempdir() creates a directory with a name that is very unlikely not to be unique.

- tempfile() creates a character string that is likely to be a unique filename.
```{r}
# reading data from internet
url <- "https://raw.githubusercontent.com/rafalab/dslabs/master/inst/extdata/murders.csv"
dat <- read_csv(url)
download.file(url,"murders.csv") #downloading file on your computer
tempfile() 
tmp_filename <- tempfile() #give a temporary name
download.file(url,tmp_filename)  #download it
file.remove(tmp_filename) #remove the temporary file
```
### Exercise: importing data
```{r}
library(tidyverse)
library(readr)

# import the file in the following url: http://mlr.cs.umass.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data
url <- "http://mlr.cs.umass.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data"
dat <- read_csv(url,col_names=FALSE) #very important, bcd there is no header for this data
ncol(dat)
nrow(dat)
```

## Key points: Tidy Data:
In tidy data, each row represents an observation and each column represents a different variable. very important!!!

In wide data, 1)each row includes several observations and 2)one of the variables is stored in the header.

```{r}
# Fertility data (example of tidy data)
library(tidyverse)
library(dslabs)
data(gapminder)
```
```{r}
# creating and investigating a tidy data frame
tidy_data <- gapminder %>%
  filter(country %in% c("South Korea","Germany")) %>%
  select(country,year,fertility)
head(tidy_data)
```
```{r}
#plotting fertility vs. year
tidy_data %>%
  filter(!is.na(fertility),!is.na(year)) %>%
  ggplot(aes(year,fertility, color=country)) +
  geom_point()
```
```{r}
# import and investigate the original Gapminder data in wide format
path <- system.file("extdata",package = "dslabs")
list.files(path)
filename <- file.path(path,"fertility-two-countries-example.csv")
filename
file.copy(file.path(path, "fertility-two-countries-example.csv"), file.path(getwd(), "data"))
wide_data <- read_csv(filename) #same info but different format
select(wide_data, country, `1960`:`1967`)
```

## Key points: Reshaping data
- The tidyr package includes several functions that are useful for tidying data.

- The gather() function converts wide data into tidy data. gather takes 4 arguments: (1) the data, (2) the name of the key column, (3) the name of the value column, and optionally (4) the names of any columns not to gather

- The spread() function converts tidy data to wide data.

```{r}
## the original wide data
library(tidyverse) 
path <- system.file("extdata", package="dslabs")
filename <- file.path(path,  "fertility-two-countries-example.csv")
wide_data <- read_csv(filename)
```
```{r}
## tidy data from dslab package
tidy_data <- gapminder %>%
  filter(country %in% c("South Korea","Germany")) %>%
  select(country,year,fertility)
head(tidy_data)
```
```{r}
## gather wide data to make new tidy data (way 1)
new_tidy_data <- wide_data %>%
  gather(year,fertility,'1960':'2015')
# This code will gather the years from 1960 to 2015 into a single column and create a single column called “fertility” that contains the fetrility rate for each country and each year.
head(new_tidy_data)
```
```{r}
## gather all columns except country (way 2)
new_tidy_data <- wide_data %>%
  gather(year,fertility,-country)
head(new_tidy_data)
```
```{r}
class(tidy_data$year)
class(new_tidy_data$year)
# the gather function assumes that column name are charactor and we need to convert the column to numbers!
```
```{r}
## convert gathered column names to numeric:
new_tidy_data <- wide_data %>%
  gather(year,fertility,-country,convert=TRUE)
head(new_tidy_data)

class(new_tidy_data$year)
```
```{r}
## now that the data are tidy, plotting
new_tidy_data %>%
  ggplot(aes(year,fertility,color=country)) +
  geom_point()
```
```{r}
## spread tidy data to generate wide data
new_wide_data <- new_tidy_data %>%
  spread(year,fertility)
select(new_wide_data, country, '1960':'1967')

```

## Key points: Separate and Unite
- The separate() function splits one column into two or more columns at a specified character that separates the variables. It has 3 argumnets: 1- the name of column to be seperated 2- the name to be used for the new column 3- charactor that seperates the variables

- When there is an extra separation in some of the entries, use fill="right" to pad missing values with NAs, or use extra="merge" to keep extra elements together.

- The unite() function combines two columns and adds a separating character.

1_Importing data:
```{r}
## Import data:
path <- system.file("extdata",package = "dslabs")
list.files(path)
filename <- file.path(path,"life-expectancy-and-fertility-two-countries-example.csv")
raw_data <- read_csv(filename)
select(raw_data,1:5)
```
2_gathering:
```{r}
## gather all columns but country
dat <- raw_data %>% gather(key,value,-country)
head(dat)
dat$key[1:5]
```
3_seperating:
```{r}
## seperate on underscores
dat %>% separate(key,c("year","variable_name"),"-")
dat %>% separate(key,c("year","variable_name"))
#error: it will seperate life_expectancy to two different parts and show only the first one
```
4_1: method 1 for tyding data:
```{r}
## way1_1: split on first underscore but keep life_expectancy merged 
dat %>% 
  separate(key,c("year","variable_name"), sep="_",extra="merge")
```
```{r}
## way1_full code: seperate and then spread
dat %>% 
  separate(key,c("year","variable_name"), sep="_",extra="merge") %>%
  spread("variable_name",value)
```
4_2: method 2 for tyding data (less efficient):
```{r}
## way2_1: split the variable to two different ones, NA(empty)
dat %>% separate(key,c("year","first_variable_name","second_variable_name"),fill = "right")
```
```{r}
## way2_2: seperate and then unite
dat %>% separate(key,c("year","first_variable_name","second_variable_name"),fill = "right") %>%
  unite(variable_name,"first_variable_name","second_variable_name",sep="_")
```
```{r}
# way2_full code for tidying data (way2: using unite)
dat %>% 
  separate(key,c("year","first_variable_name","second_variable_name"),fill = "right") %>%
  unite(variable_name,"first_variable_name","second_variable_name",sep="_") %>%
  spread(variable_name,value) %>%
  rename(fertility=fertility_NA)
```

### Exercise1 (Reshaping Data)
For the data names times.csv, tidy the data? 
and then return them to the wide data!
```{r}
## Import and read data:
path <- getwd()
list.files(path)
filename <- file.path(path,"times.csv")
x <- read_csv(filename)

## tidying data
tidy_data1 <- x %>% gather(year,time,'2015':'2017')

## converting to wide data again
tidy_data1 %>% spread(year,time)
# it will create new columns for every year and spread the time values over those cells
```

### Exercise 2: 
Suppose we have the US_disease data in the work directory, tydy the data in which all disease are put in one column.
```{r}
## Import and read data:
path <- getwd()
list.files(path)
filename <- file.path(path,"US_disease.csv")
y <- read_csv(filename)

## tidying data:
tidy_data2 <- y %>% gather(key=disease,value,'HepatitisA':'Rubella')
```

### exercise 3: tidy data that population and total be in different columns? and change the name of column to population and total
```{r}
## Import and read data:
path <- getwd()
list.files(path)
filename <- file.path(path,"Population.csv")
z <- read_csv(filename)

## tidying data:
tidy_data3 <- z %>% spread(key=var,value=people)
```
### Exercise 4: for the data of times2.csv in the working directory, in the best way tidy the data:
```{r}
## Import and read data:
path <- getwd()
list.files(path)
filename <- file.path(path,"times2.csv")
x4 <- read_csv(filename)

## tidying data
x4 %>% gather(key="key",value="value",-age_group) %>%
    separate(key,c("year","variable_name"), sep="_") %>%
  spread(variable_name,value)
```

### Exercise 5: tidy the dataset named basketbal with different columns for varaiables:
```{r}
## Import and read data:
path <- getwd()
filename <- file.path(path,"basketball.csv")
Q <- read_csv(filename)

## tidying data
Q %>%
  separate(key,c("name","variable_name"), sep="_",extra="merge") %>%
  spread(variable_name,value)
```

### Exercise 6: Consider dataset co2 which comes with basic R. 
This dataset is not tidy. becuase as seen, month is a variable and it should be in a column. also there are multiple observations in each wow. each observation should be different from another one. 
```{r}
## co2 concentration dataset:
library(tidyverse)
library(dslabs)
co2

co2_wide <- data.frame(matrix(co2,ncol=12,byrow=TRUE)) %>%
  setNames(1:12) %>% mutate(year=as.character(1959:1997))
co2_wide 

co2_tidy <- gather(co2_wide,month,co2,-year)
co2_tidy

co2_tidy %>% ggplot(aes(as.numeric(month),co2,color=year)) + geom_line()
```

### Exercise 7: for the data of admission on the dslabs package: 
```{r}
## tidy data which has one row for each major:
library(dslabs)
data(admissions)
head(admissions)

dat <- admissions %>% select(-applicants)
dat

dat %>% spread("gender",admitted)
```
```{r}
## using admission dataset, generate tmp which has major, gender, key and value as columns. 

tmp <- gather(admissions, key, value, admitted:applicants)
tmp

## create a column from joining gender and key:
tmp2 <- unite(tmp,column_name,c(key,gender))
tmp2
```

## Key points: combining tables
- The join functions in the dplyr package combine two tables such that matching rows are together.
- left_join() only keeps rows that have information in the first table.
- right_join() only keeps rows that have information in the second table.
- inner_join() only keeps rows that have information in both tables.
- full_join() keeps all rows from both tables.
- semi_join() keeps the part of first table for which we have information in the second.
- anti_join() keeps the elements of the first table for which there is no information in the second.

```{r}
## joining tables
library(tidyverse)
library(ggrepel)
library(dslabs)
ds_theme_set()

# importing murders data
data(murders)
head(murders)

#importing US election results
data(polls_us_election_2016)
head(results_us_election_2016)

#check whether two columns of state in the tables are identical to be joined or not?
identical(murders$state,results_us_election_2016$state)
```
```{r}
## joining two tables by left_joint:
tab <- left_join(murders,results_us_election_2016,by="state")
head(tab)
```
```{r}
## plotting US population vs. electral votes
tab %>% ggplot(aes(population,electoral_votes, label=abb)) +
  geom_point() +
  geom_text_repel() +
  scale_x_continuous(trans="log2") +
  scale_y_continuous(trans="log2") +
  geom_smooth(method="lm",se=FALSE)
```
```{r}
## joing two tables in which state contained in two tables are different
tab1 <- slice(murders,1:6) %>% select(state,population)
tab1

tab2 <- slice(results_us_election_2016,c(1,14,22,27,44:45)) %>%
  select(state,electoral_votes)
tab2
```
```{r}
#different ways of using left_joint:
left_join(tab1,tab2) #it will start from left (tab1)
tab1 %>% left_join(tab2) #it will start from left (tab1)
tab1 %>% right_join(tab2) #it will start from right (tab2)
inner_join(tab1,tab2) 
full_join(tab1,tab2) #it works as union function
semi_join(tab1,tab2) #it does not add the column from second table
anti_join(tab1,tab2) #it will keep the informaton of first table that they do not exist in the second one
```

## Key points: Binding
- Unlike the join functions, the binding functions do not try to match by a variable, but rather just combine datasets.

- bind_cols() binds two objects by making them columns in a tibble. The R-base function cbind() binds columns but makes a data frame or matrix instead.

- The bind_rows() function is similar but binds rows instead of columns. The R-base function rbind() binds rows but makes a data frame or matrix instead.

```{r}
## creating a dataset of numbers
bind_cols(a=1:3,b=4:6)
cbind(a=1:3,b=4:6) #it will create matrix and ...
```
```{r}
tab <- left_join(murders,results_us_election_2016,by="state")

tab1 <- tab[, 1:3]
tab2 <- tab[, 4:6]
tab3 <- tab[, 7:9]
new_tab <- bind_cols(tab1,tab2,tab3)
head(new_tab)
```
```{r}
tab1 <- tab[1:2,]
tab2 <- tab[3:4,]
bind_rows(tab1,tab2)
```

## Key points:set operators
- By default, the set operators in R-base work on vectors. If tidyverse/dplyr are loaded, they also work on data frames.

- You can take intersections of vectors using intersect(). This returns the elements common to both sets.

- You can take the union of vectors using union(). This returns the elements that are in either set.

- The set difference between a first and second argument can be obtained with setdiff(). Note that this function is not symmetric.

- The function set_equal() tells us if two sets are the same, regardless of the order of elements.

```{r}
## Intersect for vectors:
intersect(1:10,6:18)
intersect(c("a","b","c"),c("b","c","d"))
```
```{r}
## Intersect for data frames:
tab <- left_join(murders,results_us_election_2016,by="state")
tab1 <- tab[1:5,]
tab2 <- tab[3:7,]
intersect(tab1,tab2)
```
```{r}
## Union for vectors and data frames:
union(1:10,6:18)
union(c("a","b","c"),c("b","c","d"))

tab <- left_join(murders,results_us_election_2016,by="state")
tab1 <- tab[1:5,]
tab2 <- tab[3:7,]
union(tab1,tab2)
```
```{r}
## setdiff function 
setdiff(1:10,6:18) #it is not symmetric
setdiff(6:18,1:10)
setdiff(c("a","b","c"),c("b","c","d"))

tab <- left_join(murders,results_us_election_2016,by="state")
tab1 <- tab[1:5,]
tab2 <- tab[3:7,]
setdiff(tab1,tab2)
```
```{r}
## setequal function
setequal(1:5,1:6)
setequal(1:5,5:1)

tab <- left_join(murders,results_us_election_2016,by="state")
tab1 <- tab[1:5,]
tab2 <- tab[3:7,]
setequal(tab1,tab2)
```

### Exercise 1: 
install Lahman library which is related to US profesional baseball. and load following datases: Batting, Master, Salaries, and AwardsPlayers tables.

Considering Batting dataset:
```{r}
#filter Batting dataset to define top as the top 10 home run (HR) hitters in 2016
library(Lahman)
#data(Batting)
top <- Batting %>% 
  filter(yearID == 2016) %>%
  arrange(desc(HR)) %>%    # arrange by descending HR count
  slice(1:10)    # take entries 1-10

top %>% as_tibble()
```
considering Master dataset:
```{r}
## considering Master dataset:
Master %>% as_tibble() #info about all players
```
Joining masters and Batting:
```{r}
# using differnt functions, show a final table including: player ID, first name, last name, and number of HR for the top 10 players
top_names <- top %>% left_join(Master) %>% select(playerID,nameFirst,nameLast,HR)
top_names
```
considering salaries dataframe and joing with top_names:
```{r}
#filter this data to year of 2016, and add a salary column to top_names and names the resulting data frame as top_salary.
library(Lahman)
#head(Salaries)
top_salaries <- Salaries %>% filter(yearID=="2016") %>%
  right_join(top_names) %>%
  select(nameFirst, nameLast, teamID, HR, salary)
top_salaries
```
considering AwardPlayers dataset:
```{r}
## considering AwardPlayers dataframe:
library(Lahman)
Award_players_2016 <- AwardsPlayers %>% filter(yearID=="2016")

## the number of top 10 players who won at least one award in 2016
length(intersect(Award_players_2016$playerID, top_names$playerID))
semi_join(top_names,Award_players_2016)

## the number of players who won an award but were not on top home hit runners in 2016
length(setdiff(Award_players_2016$playerID,top_names$playerID))
```

## Key points: Web Scrapping
- Web scraping is extracting data from a website.

- The rvest web harvesting package includes functions to extract nodes (<>) of an HTML document: html_nodes() extracts all nodes of different types, and html_node() extracts the first node.

- html_table() converts an HTML table to a data frame.
### importing webpage (murder) to R:
```{r}
# importing a webpage (murder) to R:
library(rvest)
url <- "https://en.wikipedia.org/wiki/Murder_in_the_United_States_by_state"
h <- read_html(url)
class(h)
h  # we dont see very much!

tab <- h %>% html_nodes("table")

tab <- tab[[2]]

tab <- tab %>% html_table
class(tab)

tab <- tab %>% setNames(c("state", "population", "total", "murders", "gun_murders", "gun_ownership", "total_rate", "murder_rate", "gun_murder_rate"))
head(tab)
```
### importing webpage (corona virus) to R:
```{r}
# importing a webpage (corona virus statistics) to R:
library(rvest)
url <- "https://www.worldometers.info/coronavirus/"
h_corona <- read_html(url)
class(h_corona)

tab_corona <- h_corona %>% html_nodes("table")
tab_corona <- tab_corona[[2]]

tab_corona <- tab_corona%>% html_table
class(tab_corona)

```

## CSS Selector:
Here, we want to extract the recipe name, total preparation time, and list of ingredients from this guacamole recipe (https://www.foodnetwork.com/recipes/alton-brown/guacamole-recipe-1940609). looking at the source page, it is very complex to determine the selector. SelectorGadget is piece of software that allows you to interactively determine what CSS selector you need to extract specific components from the webpage. If you plan on scraping data other than tables, we highly recommend you install it. 
By installing SelectBudget, you can undestand the selectors:
```{r}
h <- read_html("https://www.foodnetwork.com/recipes/alton-brown/guacamole-recipe-1940609")
recipe <- h %>% html_node(".o-AssetTitle__a-HeadlineText") %>% html_text()
prep_time <- h %>% html_node(".m-RecipeInfo__a-Description--Total") %>% html_text()
ingredients <- h %>% html_nodes(".o-Ingredients__a-Ingredient") %>% html_text()

guacamole <- list(recipe,prep_time,ingredients)
guacamole

# creating a function from this recipe
get_recipe <- function(url){
  h <- read_html(url)
  recipe <- h %>% html_node(".o-AssetTitle__a-HeadlineText") %>% html_text()
prep_time <- h %>% html_node(".m-RecipeInfo__a-Description--Total") %>% html_text()
ingredients <- h %>% html_nodes(".o-Ingredients__a-Ingredient") %>% html_text()
return(list(recipe,prep_time,ingredients))
}

# example of using the function get_recipe
get_recipe("https://www.foodnetwork.com/recipes/food-network-kitchen/pancakes-recipe-1913844")
```

There are several other powerful tools provided by rvest. For example, the functions html_form(), set_values(), and submit_form() permit you to query a webpage from R

### Exercise: Major League Baseballs Payrolls
use the following URL to access data related to Major League Baseball payrolls:
https://web.archive.org/web/20181024132313/http://www.stevetheump.com/Payrolls.htm
```{r}
library(rvest)
library(tidyverse)
h <- read_html("https://web.archive.org/web/20181024132313/http://www.stevetheump.com/Payrolls.htm")
nodes <- html_nodes(h,"table")
class(nodes) #html_nodes returns a list of objects of class xml_nodes

html_text(nodes[[8]]) #the content of each object can be seen by htm_text

html_table(nodes[[8]]) #convert the nodes (objects) to data.frame

## convert the first 4 tables to data.frame:
sapply(nodes[1:4],html_table)

## for the last three componenets of nodes:
html_table(nodes[[length(nodes)]])
html_table(nodes[[length(nodes)-1]])
html_table(nodes[[length(nodes)-2]])
```
```{r}
## for the nodes 9, 18: change table in which remove row1, and change the the column 1 of first table, and change the column names to Team, Payroll, and Average
###method 1 by Elham:
tab1 <- html_table(nodes[[9]]) %>% select(Team=X2,Payroll=X3,Average=X4) %>% slice(2:31)
head(tab1)
tab2 <- html_table(nodes[[18]]) %>% select(Team=X1,Payroll=X2,Average=X3) %>% slice(2:31)
head(tab2)
join_table <- full_join(tab1,tab2,by="Team")

### method 2 by Edx:
tab_1 <- html_table(nodes[[9]]) 
tab_2 <- html_table(nodes[[18]])

  #removing extra row and columns
tab_1 <- tab_1[-1,-1] #removing one row and one column from data frame
tab_2 <- tab_2[-1,]  #removing one raw from data frame.

  #changing the name of columns
column_name <- c("Team","Payroll","Average")
names(tab_1) <- column_name 
names(tab_2) <- column_name

full_join(tab1,tab2,by="Team")
```

### Exercise: 
following wikipedia page is related to polls of UK to leave the Europe Union in 2016. one of the tables show the results of all polls. 
https://en.wikipedia.org/w/index.php?title=Opinion_polling_for_the_United_Kingdom_European_Union_membership_referendum&oldid=896735054

```{r}
library(tidyverse)
library(rvest)
url <- "https://en.wikipedia.org/w/index.php?title=Opinion_polling_for_the_United_Kingdom_European_Union_membership_referendum&oldid=896735054"

h <- read_html(url)
tab <- html_nodes(h,"table")
html_table(tab[[5]],fill=TRUE)

## what is the table number of the table "Date Conducted"?
html_table(tab[[5]],fill=TRUE) %>% names()
```

# 3_String Processing
## Key points: String Processing:
The most common tasks in string processing include:
- extracting numbers from strings
- removing unwanted characters from text
- finding and replacing characters
- extracting specific parts of strings
- converting free form text to more uniform formats
- splitting strings into multiple values
- The stringr package in the tidyverse contains string processing functions that follow a similar naming format (str_functionname) and are compatible with the pipe.

```{r}
library(tidyverse)
library(rvest)
## extracting murder data from wikipedia:
## table 2
url <- "https://en.wikipedia.org/wiki/Murder_in_the_United_States_by_state"
murders_raw <- read_html(url) %>%
  html_nodes("table") %>%
  html_table()

murders_raw <- murders_raw[[2]] %>%
  setNames(c("state", "population", "total", "murders","gun_murders","gun_ownership","total_rate","murder_rate","gun_murder_rate"))
head(murders_raw)

#shorter methos!!!
murders_raw <- read_html(url) %>%
  html_nodes("table") %>%
  html_table() %>%
  .[[2]] %>%
  setNames(c("state", "population", "total", "murders","gun_murders","gun_ownership","total_rate","murder_rate","gun_murder_rate"))
head(murders_raw)


#specify the class of two columns which we expected to be numeric
class(murders_raw$total)
class(murders_raw$population)
# reason for that: for large numbers, ususlayy use , such as 2,50,000. and it results in assuming as charactor in web_scraping
  
```

## Key points: Defining Strings: Single and Double Quotes and How to Escape
- Define a string by surrounding text with either single quotes or double quotes.

- To include a single quote inside a string, use double quotes on the outside. To include a double quote inside a string, use single quotes on the outside.

- The cat() function displays a string as it is represented inside R.
To include a double quote inside of a string surrounded by double quotes, use the backslash (\) to escape the double quote. Escape a single quote to include it inside of a string defined by single quotes.

- We will see additional uses of the escape later.

```{r}
# strings
s <- "Hello" #defining string by double quotes
s1 <- 'Hello' #defining string by single quotes
#s <- `Hello` #backquoes gives error
class(s1)

s2 <- '5"' 
cat(s2)   #cat show what string looks like in the R

s3 <- "12'"
cat(s3)

# for including both sigle and double quotes in string, escape with \
#s4 <- '12'5"'    # error
#s4 <- "12'5""    # error
s4 <- '12\'5"'  
cat(s4)

s5 <- "12'5\""
cat(s5)

```

## Key points: stringr packages
- The main types of string processing tasks are detecting, locating, extracting and replacing elements of strings.

- The stringr package from the tidyverse includes a variety of string processing functions that begin with str_ and take the string as the first argument, which makes them compatible with the pipe.

```{r}
library(tidyverse)
library(rvest)
url <- "https://en.wikipedia.org/wiki/Murder_in_the_United_States_by_state"
murders_raw <- read_html(url) %>%
  html_nodes("table") %>%
  html_table() %>%
  .[[2]] %>%
  setNames(c("state", "population", "total", "murders","gun_murders","gun_ownership","total_rate","murder_rate","gun_murder_rate"))
murders_raw$population[1:3]
as.numeric(murders_raw$population[1:3]) #it does not work because of camma
```

## Key points: Case Study, Murder data
- Use the str_detect() function to determine whether a string contains a certain pattern.
- Use the str_replace_all() function to replace all instances of one pattern with another pattern. To remove a pattern, replace with the empty string ("").
- The parse_number() function removes punctuation from strings and converts them to numeric.
- mutate_at() performs the same transformation on the specified column numbers.

```{r}
# extracting Murder data from Wikipedia
library(tidyverse)
library(rvest)
url <- "https://en.wikipedia.org/wiki/Murder_in_the_United_States_by_state"
murders_raw <- read_html(url) %>%
  html_nodes("table") %>%
  html_table() %>%
  .[[2]] %>%
  setNames(c("state", "population", "total", "murders","gun_murders","gun_ownership","total_rate","murder_rate","gun_murder_rate"))

# detechting if there is any comma or not:
commas <- function(x) any(str_detect(x,","))
murders_raw %>% summarize_all(funs(commas))

#Replacing commas with space and converting to numeric values
test1 <- str_replace_all(murders_raw$population,",","")
class(test1)
test1 <- as.numeric(test1)
class(test1)

#using function parse_number for removing commas and converting to numeric
test2 <- parse_number(murders_raw$population)
identical(test1,test2) #they are not the same bcs of updated data and as.numeric does not work good because of inclusion of endnote links. later we will learn more

murders_new <- murders_raw %>% mutate_at(2:3,parse_number)
murders_new %>% head
```

### Exercise: for the "salary" dataset, convert string to numeric:

```{r}
library(tidyverse)
library(rvest)
path <- getwd()
#list.files(path)
filename <- file.path(path,"salary.csv")
salary <- read_csv(filename)

salary %>% mutate_at(2:3,parse_number) 
salary %>% mutate_at(2:3,funs(str_replace_all(., c("\\$|,"), ""))) %>%
  mutate_at(2:3,as.numeric)
salary %>% mutate_all(parse_number)  #incorrect, it will convert Month to numeric too
salary %>% mutate_at(2:3,as.numeric)  #incorrect, it gives NA because of comma
```

## Key points: Case Study 2: Reported Heights
- In the raw heights data, many students did not report their height as the number of inches as requested. There are many entries with real height information but in the wrong format, which we can extract with string processing. 

- When there are both text and numeric entries in a column, the column will be a character vector. Converting this column to numeric will result in NAs for some entries.

- To correct problematic entries, look for patterns that are shared across large numbers of entries, then define rules that identify those patterns and use these rules to write string processing tasks.

- Use suppressWarnings() to hide warning messages for a function.

```{r}
library(dslabs)
library(tidyverse)

#load raw data
data(reported_heights)
class(reported_heights$height) #because some students enter in other formats

#converting to numeric and counting NAs.
x <- as.numeric(reported_heights$height) #it create some NA
head(x)
sum(is.na(x)) #there are a lot of NA in the data

# shows entries resulting in NAs.
reported_heights %>% 
  mutate(new_height = as.numeric(height)) %>%
  filter(is.na(new_height)) %>%
  head(n=10)

```
```{r}
# # keep only entries that 1) result in NAs or 2) are outside the plausible range of heights
not_inches <- function(x,smallest=50,tallest=84){
  inches <- suppressWarnings(as.numeric(x))
  ind <- is.na(inches) | inches < smallest | inches > tallest
  ind
}
```
```{r}
# number of problematic entries
problems <- reported_heights %>%
  filter(not_inches(height)) %>% 
  .$height
length(problems)
```
```{r}
## the problematic entries are from 3 different groups:
# 10 examples of x'y or x'y" or x'y\"
pattern <- "^\\d\\s*'\\s*\\d{1,2}\\.*\\d*'*\"*$"
str_subset(problems, pattern) %>% head(n=10) %>% cat

# 10 examples of x.y or x,y
pattern <- "^[4-6]\\s*[\\.|,]\\s*([0-9]|10|11)$"
str_subset(problems, pattern) %>% head(n=10) %>% cat

# 10 examples of entries in cm rather than inches
ind <- which(between(suppressWarnings(as.numeric(problems))/2.54, 54, 81) )
ind <- ind[!is.na(ind)]
problems[ind] %>% head(n=10) %>% cat
```

### Pattern of string processing:
| or
\\d digit includinh(1:9)
^ = start of the string
[4-7] = one digit, either 4,5,6 or 7
' = feet symbol
\\d{1,2} = one or two digits
\" = inches symbol
$ = end of the string
\s=white space

^ = start of the string
[4-7] = one digit, either 4, 5, 6, or 7
\\s* = none or more white space
[,\\.\\s+] = feet symbol is either ,, . or at least one space
\\s* = none or more white space
\\d* = none or more digits
$ = end of the string

## Key points: Regular Expression
- A regular expression (regex) is a way to describe a specific pattern of characters of text. A set of rules has been designed to do this specifically and efficiently.

- stringr functions can take a regex as a pattern.

- str_detect() indicates whether a pattern is present in a string.

- The main difference between a regex and a regular string is that a regex can include special characters.

- The | symbol inside a regex means "or".

- Use '\\d' to represent digits. The backlash is used to distinguish it from the character 'd'. In R, you must use two backslashes for digits in regular expressions; in some other languages, you will only use one backslash for regex special characters.

- str_view() highlights the first occurrence of a pattern, and the str_view_all() function highlights all occurrences of the pattern.

```{r}
## loading stringr library through tidyverse
library(tidyverse)
library(rvest)

## loading data murders:
url <- "https://en.wikipedia.org/wiki/Murder_in_the_United_States_by_state"
murders_raw <- read_html(url) %>%
  html_nodes("table") %>%
  html_table() %>%
  .[[2]] %>%
  setNames(c("state", "population", "total", "murders","gun_murders","gun_ownership","total_rate","murder_rate","gun_murder_rate"))

## detecting whether comma exists in the column or not?
pattern <- ","
str_detect(murders_raw$total,pattern)

## show subset of string including "," in the column (murder data)
str_subset(murders_raw$total,pattern)

## show subset of string including "cm" in the column (height data)
str_subset(reported_heights$height,"cm")
```

```{r}
## using | (or) inside the regex
yes <- c("180 cm","70 inches")
no <- c("180",'70"')
s <- c(yes,no)
str_detect(s,"cm") | str_detect(s, "inches")
str_detect(s,"cm|inches")  #easier way

str_view(s,"cm")
```
```{r}
## using \\d inside the regex
yes <- c("5","6","5'10","5 feet","4'11")
no <- c("",".","Five","six")
s <- c(yes,no)
pattern <- "\\d"
str_detect(s,pattern) #which of the strings has the pattern

## str_view: highlight the first occurrence of a pattern
#install.packages("htmlwidgets")
str_view(s,pattern)

## str_view_all: highlight all instances  of a pattern
str_view_all(s,pattern)
```

## Key points: Character Classes, Anchors and Quantifiers
- Define strings to test your regular expressions, including some elements that match and some that do not. This allows you to check for the two types of errors: failing to match and matching incorrectly.

- Square brackets define character classes: groups of characters that count as matching the pattern. You can use ranges to define character classes, such as [0-9] for digits and [a-zA-Z] for all letters. lowercase[a-z], uppercase[A-Z].
Be careful: at least (important) one lowercase letter [a-z]

-Note: [1-20]=0,1,2 ,it does not mean 1 to 20. it means charactor 1:2 and charactor 0. 

- Anchors define patterns that must start or end at specific places. ^ and $ represent the beginning and end of the string respectively. 
only one charactor:^\\d$

- Curly braces are quantifiers that state how many times a certain character can be repeated in the pattern. \\d{1,2} matches exactly 1 or 2 consecutive digits.

```{r}
yes <- c("5","6","5'10","5 feet","4'11")
no <- c("",".","Five","six")
s <- c(yes,no)

## [56] means 5 or 6
str_view(s,"[56]")

##[0-9] or \\d are the same
str_view(s,"[0-9]")
str_view_all(s,"[0-9]")
str_view(s,"[4-7]")
```
```{r}
## range of charactors
yes <- as.character(4:7)
no <- as.character(1:3)
s <- c(yes,no)
str_detect(s,"[4-7]")
```
```{r}
## ^ and $ mean start and end of string
pattern <- "^\\d$"
yes <- c("1","5","9")
no <- c("12","123"," 1","a4","b") #1 does not match bcs there is space before that
s <- c(yes,no)
str_view(s,pattern)
```
```{r}
## curly braces define quantifiers: 1 or 2 digits 
pattern <- "^\\d{1,2}$"
yes <- c("1","5","9")
no <- c("12","123"," 1","a4","b") 
s <- c(yes,no)
str_view(s,pattern)
```
```{r}
## pattern for inches and feet
pattern <- "^[4-7]'\\d{1,2}\"$"
# ^       start of string
#[4-7]    one digit 4,5,6, or 7
#'        the symbol of foot
#\\d{1,2} one or two digits
#\"       the inches symbol
#$        end of string

yes <- c("5'7\"", "6'2\"",  "5'12\"")
no <- c("6,2\"", "6.2\"","I am 5'11\"", "3'2\"", "64")
s <- c(yes,no)
str_detect(yes,pattern)
str_detect(no,pattern)
```

## Key points: Search adn Replace with Regex
- str_replace() replaces the first instance of the detected pattern with a specified string.

- Spaces are characters and R does not ignore them. Spaces are specified by the special character \\s.

- Additional quantifiers include *, + and ?. * means 0 or more instances of the previous character. ? means 0 or 1 instances. + means 1 or more instances.

- Before removing characters from strings with functions like str_replace() and str_replace_all(), consider whether that replacement would have unintended effects.

```{r}
#for the the height problem:
not_inches <- function(x,smallest=50,tallest=84){
  inches <- suppressWarnings(as.numeric(x))
  ind <- is.na(inches) | inches < smallest | inches > tallest
  ind
}

problems <- reported_heights %>%
  filter(not_inches(height)) %>% 
  .$height

## number of entries matching our desired pattern for the heights problem
pattern <- "^[4-7]'\\d{1,2}\"$"
sum(str_detect(problems,pattern))

## why only a few entries are selected for this pattern
problems[c(2,10,11,12,15)] %>% str_view(pattern)

## inspecting some examples of entries with problems
str_subset(problems,"inches")
str_subset(problems,"''")

## replacing or removing feet/inches words before matching
pattern <- "^[4-7]'\\d{1,2}$" #pattern withpour inches at the end
problems %>%
  str_replace("feet|ft|foot", "'") %>%
  str_replace("inches|inch|in|''|\"", "") %>%
  str_detect(pattern) %>%
  sum

## space are charactor and are not ignored by R
identical("Hi","Hi ")

## whitespace are specified with \\s
pattern2 <- "^[4-7]'\\s\\d{1,2}\"$"
str_subset(problems,pattern2)
```

```{r}
## explanation of * in regex:it means 0 or more instances of previouse charactor
yes <- c("AB", "A1B", "A11B", "A111B", "A1111B")
no <- c("A2B", "A21B")
str_detect(yes,"A1*B")
str_detect(no,"A1*B")

## (? : for none or one) (+ : for one or more)
data.frame(string=c("AB","A1B","A11B","A111B","A1111B"),
           none_or_more=str_detect(yes,"A1*B"),
           none_or_once=str_detect(yes,"A1?B"),
           once_or_more=str_detect(yes,"A1+B"))
```
```{r}
## backing to heights dataset:
pattern <- "^[4-7]\\s*'\\s*\\d{1,2}$"
problems %>%
  str_replace("feet|ft|foot", "'") %>%
  str_replace("inches|inch|in|''|\"", "") %>%
  str_detect(pattern) %>%
  sum
```
###note: (str_replace_all())
we should not use function str_replace_all, becuase for the height dataset, some students enters 5 8, and if we remove space, it will be replaced by 58 that is incorect.

## Key Points: Group with regex
- Groups are defined using parentheses.

- Once we define groups, we can use the function str_match() to extract the values these groups define. str_extract() extracts only strings that match a pattern, not the values defined by groups.

- You can refer to the ith group with \\i. For example, refer to the value in the second group with \\2.

```{r}
## replacing x.y , x,y , x y to standard pattern of x'y
## define regix with and without group
pattern_without_group <- "^[4-7],\\d*$"
pattern_with_group <- "^([4-7]),(\\d*)$"

## example:
yes <- c("5,9", "5,11", "6,", "6,1")
no <- c("5'9", ",", "2,8", "6.1.1")
s <- c(yes,no)

## explaining the effect of group
str_detect(s,pattern_without_group)
str_detect(s,pattern_with_group) 
#Result;the parantheces do not change the matching processes

## explaining str_match and its difference with str_extract
str_match(s,pattern_with_group)
str_extract(s,pattern_with_group)

## improving the pattern to recognize more events
pattern_with_groups <-  "^([4-7]),(\\d*)$"
yes <- c("5,9", "5,11", "6,", "6,1")
no <- c("5'9", ",", "2,8", "6.1.1")
s <- c(yes, no)
str_replace(s,pattern_with_group,"\\1'\\2")
```

```{r}
#for height dataset:
pattern_with_group <- "^([4-7])\\s*[,\\.\\s+]\\s*(\\d*)$"
# explain: [,\\.\\s+] means comma,dot,or at least one space
#final pattern with considering group
str_subset(problems,pattern_with_group) %>% head

str_subset(problems,pattern_with_group) %>%
  str_replace(pattern_with_group,"\\1'\\2") %>% head()
```

## Key points: Testing and Improving
- Wrangling with regular expressions is often an iterative process of testing the approach, looking for problematic entries, and improving the patterns.

- Use the pipe to connect stringr functions.

- It may not be worth writing code to correct every unique problem in the data, but string processing techniques are flexible enough for most needs.

```{r}
## about the data of reported_heights
not_inches_or_cm <- function(x,smallest=50,tallest=84){
  inches <- suppressWarnings(as.numeric(x))
  ind <- !is.na(inches) &   #the entries that are not NA
    ((inches>=smallest & inches<=tallest) |
       (inches/2.54>=smallest & inches/2.54<=tallest))
  !ind  #everything except ind
}

## identify entries with problems
problems <- reported_heights %>%
  filter(not_inches_or_cm(height)) %>%
  .$height

length(problems)

converted <- problems %>%
    str_replace("feet|foot|ft", "'") %>%  #convert feet symbols to '
  str_replace("inches|in|''|\"", "") %>%  #remove inches symbols
  str_replace("^([4-7])\\s*[,\\.\\s+]\\s*(\\d*)$", "\\1'\\2") ##change format

## finding the proportion of entries that fit the pattern after reformatting
pattern <- "^[4-7]\\s*'\\s*\\d{1,2}$"
index <- str_detect(converted,pattern)
mean(index)

converted[!index]  #show the remained problems

```

```{r}
not_inches2 <- function(x, smallest = 50, tallest = 84) {
  inches <- suppressWarnings(as.numeric(x))
  ind <- is.na(inches) | inches < smallest | inches > tallest 
  ind
}
reported_heights$height[not_inches2(reported_heights$height)]
```

### Example:
```{r}
## differnt pattern for the following vector:
## Example1:
s <- c("70","5 ft","4'11","",".","Six feet")

pattern <- "\\d|ft" 
str_view_all(s,pattern)

pattern2 <- "\\d\\d|ft" 
str_view_all(s,pattern2)
```
```{r}
# Example2:
animals <- c("cat", "puppy", "Moose", "MONKEY")
pattern3 <- "[a-z]"
str_detect(animals, pattern3)
```
```{r}
# Example3:
animals <- c("cat", "puppy", "Moose", "MONKEY")
pattern4 <- "[A-Z]$" #it is looking for an uppercase at the end of string
str_detect(animals, pattern4)
```
```{r}
#Example4:
animals <- c("cat", "puppy", "Moose", "MONKEY")
pattern5 <- "[a-z]{4,5}"
str_detect(animals, pattern5)
#looking for either 4 or 5 lowercase letters in a row anywhere in the string
```
```{r}
#Example5:
animals <- c("moose", "monkey", "meerkat", "mountain lion")
pattern6 <- "mo*"
str_detect(animals, pattern6)

pattern7 <- "mo?"
str_detect(animals, pattern7)

pattern8 <- "mo+"
str_detect(animals, pattern8)

pattern9 <- "moo*"
str_detect(animals, pattern9) #it will look for one m foloowed by zero, or more m
```
```{r}
#Example 6:
library(tidyverse)
schools <- c("U. Kentucky","Univ New Hampshire","Univ. of Massachusetts"    ,"University Georgia","U California","California State University")

## clean the above string and convert every university symbol to University
schools %>% 
  str_replace("U\\.|Univ\\.|Univ\\s+|U\\s+","University") %>%
  str_replace("^University of","University") %>%
  str_replace("^University\\s?","University of ")

schools %>% 
  str_replace("U\\.|Univ\\.|Univ\\s+|U\\s+","University") %>%
  str_replace("^University of","University") %>%
  str_replace("^University\\s?","University of ")


schools %>% 
    str_replace("^Univ\\.?\\s|^U\\.?\\s", "University ") %>% 
    str_replace("^University of |^University ", "University of ")

```
```{r}
#Example 7: 
problems <- c("5.3", "5,5", "6 1", "5 .11", "5, 12")
pattern_with_groups1 <- "^([4-7])[,\\.](\\d*)$"
str_replace(problems, pattern_with_groups1, "\\1'\\2")

#Example 8:
problems <- c("5.3", "5,5", "6 1", "5 .11", "5, 12")
pattern_with_groups2 <- "^([4-7])[,\\.\\s](\\d*)$"
str_replace(problems, pattern_with_groups2, "\\1'\\2")
```
```{r}
#example 9:
yes <- c("5 feet 7inches", "5 7")
no <- c("5ft 9 inches", "5 ft 9 inches")
s <- c(yes, no)

converted <- s %>% 
  str_replace("\\s*(feet|foot|ft)\\s*", "'") %>% 
  str_replace("\\s*(inches|in|''|\")\\s*", "") %>% 
  str_replace("^([4-7])\\s*[,\\.\\s+]\\s*(\\d*)$", "\\1'\\2")

converted

pattern <- "^[4-7]\\s*'\\s*\\d{1,2}$"
str_detect(converted, pattern)
```

## Key Point: Separate with Regex
The extract() function behaves similarly to the separate() function but allows extraction of groups from regular expressions.
```{r}
## example 1: Normally formatted height
s <- c("5'10","6'1")
tab <- data.frame(x=s)

# the function seperate and extract work similarly for this format
tab %>% separate(x,c("feet","inches"),"'")
tab %>% extract(x,c("feet","inches"),regex="(\\d)\'(\\d{1,2})")
```
```{r}
## example 2: Incorrected formatted height
s <- c("5'10","6'1\"","5'8inches")
tab <- data.frame(x=s)

# The function separate not good: it leaves extra characters
# The function extract works good: it keeps only the digits bcs of regex groups
tab %>% separate(x,c("feet","inches"),sep="'",fill="right")
tab %>% extract(x,c("feet","inches"),regex="(\\d)'(\\d{1,2})")
```
### In the heigth problem
there are still some data that our pattern does not count them: some of them does not worthy to take tim but for more practice, they are great:

1- many students entered exatly 5, or 6 feet and no inches. ex: 5' but our pattern only count the ones which have one or two didgit as inches
2- some students only entered exact 5, or 6 without any symbol. ex: 5
3- some students eneted inches with decimal. ex: 5'7.4". But our pattern only look for 2 digits.
4- some data have some spaces at the end 5  '  9.
5- some entries are eneted based on meter and some of them used Eurpoean decimal: ex: 1.7
6- some (two) studnets eneted cm.
7- one students spell out the whol  number. ex: five feet seven inches

```{r}
## case2:
yes <- c("5", "6", "5")
no <- c("5'", "5''", "5'4", "5'7.4")
s <- c(yes, no)
str_replace(s,"^([4-7])$","\\1'0")
```
```{r}
## case 1,2:
str_replace(s,"^([5-6])'?$","\\1'0")
# we only permitted 5, or 6 not 4, and 7: because heights of exact 5 or 6 are very common. However, heights of exacty 4 or 7 are very rare. although the height of 84 is possible but we assume that someone who eneterd 7, eneted incorrectly.
```
```{r}
## case 3:  5'7.4" it will include the entries which allow decimal not just digits in second group 
pattern <- "^[4-7]\\s*'\\s*(\\d+\\.?\\d*)$" 
str_detect(s, pattern)
```
```{r}
## case 5: it is simliar to the approach in which x.y concerted to x'y. However, the first number only can be 0 or 1
yes <- c("1,7", "1, 8", "2, " )
no <- c("5,8", "5,3,2", "1.7")
s <- c(yes, no)
str_replace(s,"^([12])\\s*,\\s*(\\d*)$","\\1\\.\\2")
#later we will see if they are meter or inches
```

### Trimming:
spaces at the beginning or end of a string is very uninformative and also sometimes deceptive as they cannot be seen and distinguished.and because it is very common problem, function str_trim() is defines for this purpose.
```{r}
s <- "Hi "
cat(s)
identical(s,"Hi")

#function str_trim()
str_trim(" 5 ' 8 ")
```
### to upper and lower case:
we can use function str_to_lower()
```{r}
s <- c("Five feet seven inches")
str_to_lower(s)
```

### put all to gether fot problematic cases of height data:
```{r}
convert_format <- function(s){
  s %>%
    str_replace("feet|foot|ft", "'") %>% #convert feet symbols to '
    str_replace_all("inches|in|''|\"|cm|and", "") %>%  #remove inches and other symbols
    str_replace("^([4-7])\\s*[,\\.\\s+]\\s*(\\d*)$", "\\1'\\2") %>% #change x.y, x,y x y
    str_replace("^([56])'?$", "\\1'0") %>% #add 0 when to 5 or 6
    str_replace("^([12])\\s*,\\s*(\\d*)$", "\\1\\.\\2") %>% #change european decimal
    str_trim() #remove extra space
}

## writing a function which convert words to number

words_to_numbers <- function(s){
  str_to_lower(s) %>%  
    str_replace_all("zero", "0") %>%
    str_replace_all("one", "1") %>%
    str_replace_all("two", "2") %>%
    str_replace_all("three", "3") %>%
    str_replace_all("four", "4") %>%
    str_replace_all("five", "5") %>%
    str_replace_all("six", "6") %>%
    str_replace_all("seven", "7") %>%
    str_replace_all("eight", "8") %>%
    str_replace_all("nine", "9") %>%
    str_replace_all("ten", "10") %>%
    str_replace_all("eleven", "11")
}

## problems:
library(dslabs)
library(tidyverse)
#load raw data
data(reported_heights)

not_inches_or_cm <- function(x,smallest=50,tallest=84){
  inches <- suppressWarnings(as.numeric(x))
  ind <- !is.na(inches) &   #the entries that are not NA
    ((inches>=smallest & inches<=tallest) |
       (inches/2.54>=smallest & inches/2.54<=tallest))
  !ind  #everything except ind
}

## identify entries with problems
problems <- reported_heights %>%
  filter(not_inches_or_cm(height)) %>%
  .$height

length(problems)
```
Now, applying the above functions:
```{r}
## showing the remaining problems that are nor similar to our format, (x'y)
converted <- problems %>% words_to_numbers %>% convert_format 
remaining_problems <- converted[not_inches_or_cm(converted)]
pattern <- "^[4-7]\\s*'\\s*\\d+\\.?\\d*$"
index <- str_detect(remaining_problems,pattern)
remaining_problems[!index]
```

## Putting all together: (height problem)
```{r}
pattern <- "^([4-7])\\s*'\\s*(\\d+\\.?\\d*)$"
shortest <- 50
tallest <- 84
new_height <- reported_heights %>%
  mutate(original=height,
         height=words_to_numbers(height) %>% convert_format()) %>%
  extract(height,c("feet","inches"),regex=pattern,remove=FALSE) %>%
  mutate_at(c("height","feet","inches"),as.numeric) %>%
  mutate(guess=12*feet+inches) %>%
  mutate(height=case_when(
    !is.na(height) & between(height,shortest,tallest) ~ height,
    !is.na(height) & between(height/2.54,shortest,tallest) ~ height/2.54,
    !is.na(height) & between(height*100/2.54,shortest,tallest) ~ height*100/2.54,
    !is.na(guess) & inches<12 & between(guess,shortest,tallest) ~ guess,
    TRUE~as.numeric(NA))) %>%
  select(-guess)

## now, we can check all the entries which we converted:
not_inches <- function(x,smallest=50,tallest=84){
  inches <- suppressWarnings(as.numeric(x))
  ind <- is.na(inches) | inches < smallest | inches > tallest
  ind
}

new_heights %>%
  filter(not_inches(original)) %>%
  select(original,height) %>%
  arrange(height) %>%
  view()
```
```{r}
## finding the shortest students in our data:
new_heights %>% arrange(height) %>% head(n=7)

## the shortest heights such as 50,51, .. are very low and maybe students wanted to enter 5 foot but he inchered 50. However, we have to leave it because we ar enot sure about this!
```

## Key Points: string splitting
- The function str_split() splits a string into a character vector on a delimiter (such as a comma, space or underscore). By default, str_split() generates a list with one element for each original string. Use the function argument simplify=TRUE to have str_split() return a matrix instead.

- The map() function from the purrr package applies the same function to each element of a list. To extract the ith entry of each element x, use map(x, i).

- map() always returns a list. Use map_chr() to return a character vector and map_int() to return an integer.

```{r}
## suppose we do not access to csv file and instead we have a text file and we want to extract some data:
# reading the raw data (murders) line by line
filename <- system.file("extdata/murders.csv",package="dslabs")
lines <- readLines(filename)
lines %>% head()

#spliting lines with comma and removing the raw of column name
x <- str_split(lines,",")
x %>% head()
col_names <- x[[1]]
x <- x[-1]
head(x)  #this is list

# extracting first element of each list: (and then converting list to data.frame)
library(purrr)
map(x,function(y) y[1]) %>% head()
map(x,1) %>% head()
```
```{r}
## extracting column 1-5 as charactor and converting to differnt format
dat <- data.frame(parse_guess(map_chr(x,1)),  #returns a charactor vector
                  parse_guess(map_chr(x,2)),
                  parse_guess(map_chr(x,3)),
                  parse_guess(map_chr(x,4)),
                  parse_guess(map_chr(x,5))) %>%
  setNames(col_names) 
dat %>% head()
```
```{r}
##A method which is more efficient:
dat <- x %>%
  transpose() %>%
  map(~parse_guess(unlist(.))) %>%
  setNames(col_names) %>%
  as.data.frame()
```
```{r}
filename <- system.file("extdata/murders.csv",package="dslabs")
lines <- readLines(filename)

# "simplify" argument makes str_split return a matrix instead of a list
x <- str_split(lines,",",simplify=TRUE)
col_names <- x[1,]
x <- x[-1,]

x %>% as_data_frame() %>%
  setNames(col_names) %>%
  mutate_all(parse_guess)
```
```{r}
x <- str_split(lines, ",", simplify = TRUE) 
col_names <- x[1,]
x <- x[-1,]
x %>% as_data_frame() %>%
  setNames(col_names) %>%
  mutate_all(parse_guess)
```

## Extracting a table from PDF file: (Case Study)
We are going to extract table from one of dslabs dataset, scientific funding rates by gender in the Netherlands. 

```{r}
## loading data
library(dslabs)
library(tidyverse)
data("research_funding_rates")
research_funding_rates
```

this data are extracted from a paper: we are going to wrangle the data using R:
```{r}
## downloading data
library("pdftools")
temp_file <- tempfile()
url <- "https://www.pnas.org/content/suppl/2015/09/16/1510159112.DCSupplemental/pnas.201510159SI.pdf"
download.file(url,temp_file)
txt <- pdf_text(temp_file) #it does not work good here, error!!!
file.remove(temp_file)
```
```{r}
## By investigating the object text, we understand that it is a charactor vector with an entery for each page. The page 2 is the page we want to keep:
raw_data_research_funding_rates <- txt[2]  #dies not work! error!
```
Becaus ethere are some error associated with pdf_text, we use the raw data stored in the dslabs package:
```{r}
## the above steps was only for better learning but this data include in dslab:
library(dslabs)
data("raw_data_research_funding_rates")
raw_data_research_funding_rates %>% head()
# as seen, this is a string in which every line inclusing table raw are seperated by \n.

## creating a list:
tab <- str_split(raw_data_research_funding_rates,"\n")

## as we can see, we put all data in one column with difeernt raws.
tab <- tab[[1]]
tab %>% head()

## extracting the column names stored in raws 3, 4
the_names_1 <- tab[3]
the_names_2 <- tab[4]
```
```{r}
## cleaning name column: part 1
the_names_1 <- the_names_1 %>%
  str_trim() %>%
  str_replace_all(",\\s.","") %>%   #\\s. : removing anything following the comma
  str_split("\\s{2,}",simplify=TRUE)
the_names_1
```
```{r}
## cleaning name column: part 2
 the_names_2 <- the_names_2 %>%
  str_trim() %>%
  str_split("\\s+",simplify=TRUE)
the_names_2
```
```{r}
##joining the name column 1,2:
tmp_names <- str_c(rep(the_names_1,each=3),the_names_2[-1],sep="_")
tmp_names
the_names <- c(the_names_2[1],tmp_names) %>%
  str_to_lower() %>%
  str_replace_all("\\s","_")
the_names
```
```{r}
## cleaning the real data:
new_research_funding_rates <- tab[6:14] %>%  #not include total raw
  str_trim() %>%
  str_split("\\s{2,}",simplify=TRUE) %>%
  data.frame(stringsAsFactors = FALSE) %>%
  setNames(the_names) %>%
  mutate_at(-1,parse_number) #it will guess the type of data except column 1

head(new_research_funding_rate)
```
```{r}
## the new table is a s the same of dataframe stored in our package:
identical(research_funding_rates, new_research_funding_rates)
```

## Key points: recoding
- Change long factor names with the recode() function from the tidyverse. 

- Other similar functions include recode_factor() and fct_recoder() in the forcats package in the tidyverse. The same result could be obtained using the case_when() function, but recode() is more efficient to write.

```{r}
## Life expectancy timeseries for Carabbean countries:
library(dslabs)
data(gapminder)

## showing gapminder data and long names that are ineffcient
gapminder %>%
  filter(region=="Caribbean") %>%
  ggplot(aes(year,life_expectancy,color=country)) +
  geom_line()
```
```{r}
## long name countries:
gapminder %>%
    filter(region=="Caribbean") %>%
  filter(str_length(country)>=12) %>%
  distinct(country)
```
```{r}
## changing the name of countries that are long to shorter names.
gapminder %>% filter(region=="Caribbean") %>%
  mutate(country=recode(country,
                          'Antigua and Barbuda'="Barbuda",
                          'Dominican Republic' = "DR",
                          'St. Vincent and the Grenadines' = "St. Vincent",
                          'Trinidad and Tobago' = "Trinidad")) %>%
  ggplot(aes(year,life_expectancy,color=country)) +
  geom_line()
```
## Exercise: 
for the data of gapminder, for the countries that are more than 12 charctors in the middle africa, use their abbriviation instead of their name in a new column of short_name:

```{r}
library(dslabs)
data("gapminder")
#head(gapminder)

gapminder %>% filter(region=="Middle Africa",str_length(country)>12)
gapminder %>% filter(region=="Middle Africa") %>%
  mutate(short_name=recode(country, 
                                "Central African Republic" = "CAR", 
                                "Congo, Dem. Rep." = "DRC",
                                "Equatorial Guinea" = "Eq. Guinea"
             
  ))
  
```
## Exercise:
```{r}
## clean the following dataset in which decimal places in the third column with desimal symbol:

s <- c("5'10", "6'1\"", "5'8inches", "5'7.5")
tab <- data.frame(x=s)
extract(data=tab,col=x,into=c("feet","inches","decimals"),
        regex="(\\d)'(\\d{1,2})(\\.\\d)?")
```
```{r}
## clean the folloing dataset and put every name in separate column:
staff_day <- data.frame(day=c("Monday","Tuesday"),staff=c("Mandy, Chris and Laura","Steve, Ruth and Frank"),stringsAsFactors=FALSE)
staff_day
```
```{r}
## put every name in separate column
str_split(staff_day$staff,", | and ")
str_split(staff_day$staff,",\\s|\\sand\\s")
str_split(staff_day$staff,"\\s?(,|and)\\s?")
```
```{r}
## put all staff only in two columns:
x <- staff_day %>% 
  mutate(staff = str_split(staff, ", | and ")) %>% 
  unnest()
x
```
## Exercise:
Import raw Brexit referendum polling data from Wikipedia.
```{r}
library(rvest)
library(tidyverse)
library(stringr)
url <- "https://en.wikipedia.org/w/index.php?title=Opinion_polling_for_the_United_Kingdom_European_Union_membership_referendum&oldid=896735054"
tab <- read_html(url) %>% html_nodes("table")
polls <- tab[[5]] %>% html_table(fill=TRUE)
```
1- filter the raws that in their remain column, there is no data?
```{r}
## method1: (Eli) updating column name:
colNmaes <- c("dates", "remain", "leave", "undecided", "lead", "samplesize", "pollster", "poll_type", "notes")
polls_new <- polls %>%
  setNames(colNmaes) %>%
  filter(str_detect(remain,"%$"))
```
```{r}
#method2 (edx)
names(polls) <- c("dates", "remain", "leave", "undecided", "lead", "samplesize", "pollster", "poll_type", "notes")
polls <- polls[str_detect(polls$remain, "%"),-9] #-9: the name of columns
nrow(polls)
```
2- for the remain and leave columns, which have a format like "54.6%", write a command which show the remain column a number between 0,1:
```{r}
names(polls) <- c("dates", "remain", "leave", "undecided", "lead", "samplesize", "pollster", "poll_type", "notes")
polls <- polls[str_detect(polls$remain, "%"),-9] #-9: the name of columns

x1 <- as.numeric(str_replace(polls$remain,"%",""))/100
x2 <- parse_number(polls$remain)/100
#str_remove(polls$remain, "%")/100  #function str_remove works great but bcs it is not numeric, we can devide by 100
```
3- for the undecided column, some of the values are N/A (when total of remain and leave is 100%). they should be zero instead of N/A. convert N/A to 0:
```{r}
str_replace(polls$undecided, "N/A", "0")
```
4- the format of "dates" is like 25-26 April, or 29 Mar–3 Apr where the start date and end data are reported. Write command for extracting the end day and month from dates. 
note: the end date has a number with one or two digits, followed by a space, followed by the month as 1 or more letters (either capital or lowercase). In these data, all month abb or names have 3, 4 or 5 letters.
```{r}
x <- c("25-26 April","29 Mar–3 Apr")
pattern <- "\\d{1,2}\\s[a-zA-Z]+"
str_detect(x,pattern)
pattern1 <- "\\d+\\s[a-zA-Z]{3,5}"
str_detect(x,pattern1)
pattern2 <- "[0-9]+\\s[a-zA-Z]+"
str_detect(x,pattern2)
pattern3 <- "\\d+\\s[a-zA-Z]+"
str_detect(x,pattern3)
```

## key points: Data, time, and text mining:
- compute languages usually use January 1, 1970 as epoch!

- Dates are a separate data type in R.The tidyverse includes functionality for dealing with dates through the lubridate package. 

- Extract the year, month and day from a date object with the year(), month() and day() functions.

- Parsers convert strings into dates with the standard YYYY-MM-DD format (ISO 8601 format). Use the parser with the name corresponding to the string format of year, month and day (ymd(), ydm(), myd(), mdy(), dmy(), dym()).

- Get the current time with the Sys.time() function. Use the now() function instead to specify a time zone.

- You can extract values from time objects with the hour(), minute() and second() functions.

- Parsers convert strings into times (for example, hms()). Parsers can also create combined date-time objects (for example, mdy_hms()).

```{r}
## inspecting the startdate column of 2016 polls data
library(dslabs)
library(tidyverse)
data("polls_us_election_2016")
polls_us_election_2016$startdate %>% head()
class(polls_us_election_2016$startdate) #class is date
as.numeric(polls_us_election_2016$startdate) %>% head()
```
```{r}
## ggplot is aware about dates
polls_us_election_2016 %>% filter(pollster=="Ipsos" & state=="U.S.") %>%
  ggplot(aes(startdate,rawpoll_trump)) +
  geom_line()
```
```{r}
## lubridate is tidyverse date package
library(lubridate)

## selcting some random dates 
set.seed(2)
dates <- sample(polls_us_election_2016$startdate,10) %>% sort
dates
```
```{r}
## extracting day,month, and year from date strings
data.frame(date=days(dates),
           month=month(dates),
           day=day(dates),
           year=year(dates))

month(dates,label=TRUE) #extract month labels
```

```{r}
## ymd() works on different date styles
x <- c(20090101, "2009-01-02", "2009 01 03", "2009-1-4",
       "2009-1, 5", "Created on 2009 1 6", "200901 !!! 07")
ymd(x)
```
```{r}
## various parsers extract day,month, and year in different orders
x <- "09/01/02"
ymd(x) #it assumed standard YYYY-MM-DD format ymd:year,month,day
mdy(x)
dmy(x)
dym(x)
```
```{r}
## current ime
Sys.time() #current time
now()      #current time in your timezone
now("GMT") #current time in GMT such as London
now("Asia/Tehran")
OlsonNames() %>% head() #all the available timezones

## Extracting hours, minutes, and seconds from current time
now() %>% hour()
now() %>% minute()
now() %>% second()
```
```{r}
##parse time: converting strings into times
x <- c("12:34:53")
hms(x)

## parse datetime:
x <- "Nov/2/2012 12:34:56"
mdy_hms(x)
```

## Key points: Text mining (The best resource: https://www.tidytextmining.com/)
- The tidytext package helps us convert free form text into a tidy table.

- Use unnest_tokens() to extract individual words and other meaningful chunks of text.

- Sentiment analysis assigns emotions or a positive/negative score to tokens. You can extract sentiments using get_sentiments().

- Common lexicons for sentiment analysis are "bing", "afinn", "nrc" and "loughran".

- some data are numerical but some of them are text such as: sentiment analysis, counter-terrorism-analysis, cybre-crime prevention. In these kinds of data, they are composed on free form of text. We want to extract insights from these data.

Case_Study: Trump tweets: in 2016 US election, Trump used Tweeter to comminicate with voters. on August 2016, Todd Vaziri tweeted that "Every non-hyperbolic tweet is from iPhone (his staff). Every hyperbolic tweet is from Android (from him)."  Data scientist David Robinson conducted an analysis to determine if data supported this claim.

_ we can use rtweet package to extract Trump tweets from tweeter. However, his tweets are available in http://www.trumptwitterarchive.com/.

- required libararies:
```{r}
library(tidyverse)
library(ggplot2)
library(lubridate)
library(tidyr)
library(scales)
set.seed(1)
```
- Extracting Trump's tweets from the below website:
```{r}
url <- 'http://www.trumptwitterarchive.com/data/realdonaldtrump/%s.json'
trump_tweets <- map(2009:2017, ~sprintf(url, .x)) %>%
  map_df(jsonlite::fromJSON, simplifyDataFrame = TRUE) %>%
  filter(!is_retweet & !str_detect(text, '^"')) %>%
  mutate(created_at = parse_date_time(created_at, orders = "a b! d! H!:M!:S! z!* Y!", tz="EST")) 
```
- for more convenience, we use data stored in dslabs
?trump_tweets: gives us information about the variables in trump tweets data
```{r}
library(dslabs)
data("trump_tweets")
head(trump_tweets)
names(trump_tweets)  #the name of columns

#text variable: shows the text of each tweet
trump_tweets %>% select(text) %>% head  

#source variable: tells us the device is utilied to compose and upload each tweet
trump_tweets %>% count(source) %>% arrange(desc(n))  

# using extract to remove "Twitter for" part of the source and filter out retweets
trump_tweets %>% 
  extract(source, "source", "Twitter for (.*)") %>%
  count(source)
```
by the flolowing table, we are going to undestand whats going on during campaign (from the day Trump announced his campaign to election day). 
```{r}
campaign_tweets <- trump_tweets %>% 
  extract(source, "source", "Twitter for (.*)") %>%
  filter(source %in% c("Android", "iPhone") &
           created_at >= ymd("2015-06-17") & 
           created_at < ymd("2016-11-08")) %>%
  filter(!is_retweet) %>%
  arrange(created_at)
```
Now, we can use data visualization to see the possibility of tweeting two different groups of tweets from two different devices during campaign. For each tweet, we will extract the hour (in East Coast (EST)) of tweeting. and then we will calculate the proportion of tweets at each hour for each device. 

```{r}
ds_theme_set()
campaign_tweets %>%
  mutate(hour = hour(with_tz(created_at, "EST"))) %>%
  count(source, hour) %>%
  group_by(source) %>%
  mutate(percent = n / sum(n)) %>%
  ungroup %>%
  ggplot(aes(hour, percent, color = source)) +
  geom_line() +
  geom_point() +
  scale_y_continuous(labels = percent_format()) +
  labs(x = "Hour of day (EST)",
       y = "% of tweets",
       color = "")
```
It is noticiable that ther is a big pick for the android in around 6-8 AM. There is a clear different in the patterens. We will assume that two differnt entities are using these two devices for tweeting. Now, we want to undestance the difference between two tweets. The tidytext package will be introduced and used later. 

### Text as data (tidytext package):
The tidytext package helps us convert free from text into a tidy table. Data in this format enables data visualization and applying statistical techniques more effciient and easier.

```{r}
## tidytext package for tidy text data:
library(tidytext)
```

The main function for this purpose is unnest_tocken(). Tocken is the units whcih are considered to be a data point. The most common tokens are words. However, they also can be single characters, ngrams, sentences, lines, or a petteren defined by a regex. The fueach one gets a raw in the new table. For example:

```{r}
example <- data_frame(line = c(1, 2, 3, 4),
                      text = c("Roses are red,", "Violets are blue,", "Sugar is sweet,", "And so are you."))
example
```
```{r}
example %>% unnest_tokens(word, text,token="sentences") 
example %>% unnest_tokens(word, text) #example: data_frame, word:output, text:input
```
now for the tweet number 3008:
```{r}
i <- 3008
campaign_tweets$text[i]
campaign_tweets[i,] %>% 
  unnest_tokens(word, text) %>%
  select(word)
```
As seen, the function is converting tokens into words and stripts characters whcih are important to twitter such as # and @. Note a token in twitter is different than the token in regular english. Therefore, instead of using the default token, words, we define a regex that captures twitter character. The pattern appears complex but all we are defining is a patter that starts with @, # or neither and is followed by any combination of letters or digits:
```{r}
pattern <- "([^A-Za-z\\d#@']|'(?![A-Za-z\\d#@]))"
```
now using unnest_tokens() function with the regex option, we can extract the hashtags and mentions appropriately:
```{r}
campaign_tweets[i,] %>% 
  unnest_tokens(word, text, token = "regex", pattern = pattern) %>%
  select(word)
```
Another minor adjustment: removing the links to pictures
```{r}
campaign_tweets[i,] %>% 
  mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", ""))  %>%
  unnest_tokens(word, text, token = "regex", pattern = pattern) %>%
  select(word)
```
now, extracting words from all tweets:
```{r}
tweet_words <- campaign_tweets %>% 
  mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", ""))  %>%
  unnest_tokens(word, text, token = "regex", pattern = pattern) 
```
now, we can  answer questions such as "what are the most commonly used words?"
```{r}
tweet_words %>% 
  count(word) %>%
  arrange(desc(n))
```
It is not surprising that these are the top words. The top words are not informative. The tidytext package has database of these commonly used words, referred to as stop words, in text mining:
```{r}
stop_words
```
with filtering out rows representing stop words with filter(!word %in% stop_words$word), we will have:
```{r}
tweet_words <- campaign_tweets %>% 
  mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", ""))  %>%
  unnest_tokens(word, text, token = "regex", pattern = pattern) %>%
  filter(!word %in% stop_words$word ) 
```
ending up with a much more informative set of top 10 tweeted words:
```{r}
tweet_words %>% 
  count(word) %>%
  top_n(10, n) %>%
  mutate(word = reorder(word, n)) %>%
  arrange(desc(n))
```
some of theresulting words (not show here) shows a couple of unwanted characteristics in our tokens. 
First, some of our tokens are just numbers (years for example). We want to remove these and we can find them using the regex ^\d+$.
Second, some of our tokens come from a quote and they start with '. We want to remove the ' when it's at the start of a word, so we will use str_replace(). 
We add these two lines to the code above to generate our final table:

```{r}
tweet_words <- campaign_tweets %>% 
  mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", ""))  %>%
  unnest_tokens(word, text, token = "regex", pattern = pattern) %>%
  filter(!word %in% stop_words$word &
           !str_detect(word, "^\\d+$")) %>%
  mutate(word = str_replace(word, "^'", ""))
```

Now after gathering all our words in a table, and information about whcih device was utilized to compose the tweet they came from, we can start exploring which words are more common in comparison between Android to iPhone.

For each word we want to know if it is more likely to come from an Android tweet or an iPhone tweet.
previously, We introduced the odds ratio, a summary statistic useful for quantifying these differences. For each device and a given word, let's call it y, we compute the odds or the ratio between the proportion of words that are y and not y and compute the ratio of those odds.
Here we will have many proportions that are 0 so we use the 0.5 correction.
```{r}
android_iphone_or <- tweet_words %>%
  count(word, source) %>%
  spread(source, n, fill = 0) %>%
  mutate(or = (Android + 0.5) / (sum(Android) - Android + 0.5) / 
           ( (iPhone + 0.5) / (sum(iPhone) - iPhone + 0.5)))
android_iphone_or %>% arrange(desc(or))
android_iphone_or %>% arrange(or)
```
based on that several of these words are overall low frequency words we can impose a filter based on the total frequency like this:
```{r}
android_iphone_or %>% filter(Android+iPhone > 100) %>%
  arrange(desc(or))
```
```{r}
android_iphone_or %>% filter(Android+iPhone > 100) %>%
  arrange(or)
```
We already see such a pattern in the types of words are tweeted more in one device compare to the another one. However, we are interested in the tone not the specific words. 
Vaziri's assertion is that the Android tweets are more hyperbolic. So how can we check this with data? Hyperbolic is a hard sentiment to extract from words as it relies on interpreting phrases. However, words can be associated to more basic sentiment such as as anger, fear, joy and surprise. In the next section we demonstrate basic sentiment analysis.

### Setiments anaylysis:
1- first step in sentiment analysis is: assigning a sentiment to each word
(he tidytext package includes several maps or lexicons in the object sentiments)

```{r}
sentiments
```
There are different lexicons in tidytext package giving you different sentiments. 
- the "bing" sentiment devides words into positive and negative
```{r}
get_sentiments("bing")
```
- AFINN lexicon: assigns a score between -5 and 5, with -5 the most negative and 5 the most positive.
```{r}
#install.packages("textdata") #it needed to be install for using afinn
get_sentiments("afinn") #be careful about the selction. It will ask you something in the console.
```
The loughran and nrc lexicons provide several different sentiments:
```{r}
get_sentiments("loughran") %>% count(sentiment)
get_sentiments("nrc") %>% count(sentiment)
```
?sentiments to see how these are developed!

For the analysis of the tweets, we are interested in understanding the different sentiments of each tweet, so we will use the nrc.
lexicon:

```{r}
nrc <- get_sentiments("nrc") %>%
  select(word, sentiment)
```
The words and sentiments can be combined using inner_join(), which will only keep words associated with a sentiment. 
Here are 10 random words extracted from the tweets:
```{r}
tweet_words %>% inner_join(nrc, by = "word") %>% 
  select(source, word, sentiment) %>% sample_n(10)
```
Now, everythind is ready for a quantitative analysis to compare Android and iphone by comparing the sentiments of the tweets posted from each device. So, we do this wth tweet analysis, and assign a sentiment to each tweet. However, it is somewhat complaex becaus eeach twwt will have several sentiments to which attached. In fact, one for every word appearing in the lexicon. For illustrative purposes, we will perform a much simpler analysis: we will count and compare the frequecies of each sentimen appears for each device. 
```{r}
sentiment_counts <- tweet_words %>%
  left_join(nrc, by = "word") %>%
  count(source, sentiment) %>%
  spread(source, n) %>%
  mutate(sentiment = replace_na(sentiment, replace = "none"))
sentiment_counts
```
Since more words were used on the Android rather than on the phone:
```{r}
tweet_words %>% group_by(source) %>% summarize(n = n())
```
for each sentiment we can calculate the odds of being in the device: proportion of words with sentiment versus proportion of words without and then compute the odds ratio comparing the two devices:
```{r}
sentiment_counts %>%
  mutate(Android = Android / (sum(Android) - Android) , 
         iPhone = iPhone / (sum(iPhone) - iPhone), 
         or = Android/iPhone) %>%
  arrange(desc(or))
```
So some differences are observed and also there the order is interesting: the largest three sentiments are disgust, anger, and negative! But are they statistically significant? How does this compare if we are just assigning sentiments at random?

To answer that question we can compute, for each sentiment, an odds ratio and confidence interval. We will add the two values we need to form a two-by-two table and the odds ratio:
```{r}
library(broom)
log_or <- sentiment_counts %>%
  mutate( log_or = log( (Android / (sum(Android) - Android)) / (iPhone / (sum(iPhone) - iPhone))),
          se = sqrt( 1/Android + 1/(sum(Android) - Android) + 1/iPhone + 1/(sum(iPhone) - iPhone)),
          conf.low = log_or - qnorm(0.975)*se,
          conf.high = log_or + qnorm(0.975)*se) %>%
  arrange(desc(log_or))
  
log_or
```
A graphical visualization shows some sentiments that are clearly overrepresented:
```{r}
log_or %>%
  mutate(sentiment = reorder(sentiment, log_or),) %>%
  ggplot(aes(x = sentiment, ymin = conf.low, ymax = conf.high)) +
  geom_errorbar() +
  geom_point(aes(sentiment, log_or)) +
  ylab("Log odds ratio for association between Android and sentiment") +
  coord_flip() 
```

It is seen that the disgust, anger, negative sadness and fear sentiments are associated with the Android in a way that is hard to explain by chance alone. Words not associated to a sentiment were strongly associated with the iPhone source, which is in agreement with the original claim about hyperbolic tweets.

f we are interested in exploring which specific words are driving these differences, we can back to our android_iphone_or object:
```{r}
android_iphone_or %>% inner_join(nrc) %>%
  filter(sentiment == "disgust" & Android + iPhone > 10) %>%
  arrange(desc(or))
```
Making a graph:
```{r}
android_iphone_or %>% inner_join(nrc, by = "word") %>%
  mutate(sentiment = factor(sentiment, levels = log_or$sentiment)) %>%
  mutate(log_or = log(or)) %>%
  filter(Android + iPhone > 10 & abs(log_or)>1) %>%
  mutate(word = reorder(word, log_or)) %>%
  ggplot(aes(word, log_or, fill = log_or < 0)) +
  facet_wrap(~sentiment, scales = "free_x", nrow = 2) + 
  geom_bar(stat="identity", show.legend = FALSE) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) 
```

### Example1:
```{r}
library(dslabs)
library(lubridate)
library(tidyverse)
options(digits = 3)  #3 significant digits
data(brexit_polls)

# the entries with a startdate in April:
sum(month(brexit_polls$startdate)==4)

#the number of polls ended in 2016-06-12? (using round_date and unit of week)
sum(round_date(brexit_polls$enddate,unit="week")=="2016-06-12")

# in which day of week, the most polls are ended? (uisng week_day() function)
table(weekdays(brexit_polls$enddate))
```
### example 2:
In the data of movielans that is cotaning about 100,000 movie reviews. the timestamp column include the review date based on second from epoch date of 1970-01-01. 

```{r}
library(dslabs)
data(movielens)

# converting timestap column to dates:
#which year has max review?
dates <- as_datetime(movielens$timestamp) #the dates of review
review_year <- table(year(dates))
names(which.max(review_year))

#which hour has the most review of movie?
review_by_hour <- table(hour(dates))
names(which.max(review_by_hour))
```
### example 3: text mining and sentiment analysis:
Library of gutenbergr contains the digital archive of public domain books. 
```{r}
## loading the libraries:
library(tidyverse)
library(gutenbergr)
library(tidytext)
options(digits = 3)
```
```{r}
## looking at the books and the data available
gutenberg_metadata 
```
```{r}
## id of the book: Pride and Prejudice
gutenberg_metadata %>%
  filter(str_detect(title,"Pride and Prejudice"))
```
```{r}
## real id of english book of Pride and Prejudice
gutenberg_works(title=="Pride and Prejudice")$gutenberg_id
# gutenberg_works works such a filtered tavle pf Gutenberg
```
```{r}
## downloading text of the book:
P_P <- gutenberg_download(1342)
```
```{r}
## tha table of all of the words and the number of all words:
P_P_words <- P_P %>% unnest_tokens(word, text)
nrow(P_P_words)
```
```{r}
## filter the stop words and show the rest of the words:
P_P_words <- P_P_words %>% filter(!word%in%stop_words$word) 
nrow(P_P_words)

# or another method:
P_P_words <- P_P_words %>% anti_join(stop_words)
nrow(P_P_words)
```
```{r}
## removing the words which containg any digit:
P_P_words <- P_P_words %>% filter(!str_detect(word, "\\d"))
nrow(P_P_words)
```
```{r}
## the most frequent words after filtering out the stop word and digits: (what is the most common word and how many times it is repeated)
P_P_word_100 <- P_P_words %>%
  count(word) %>% 
  filter(n>100) %>%
  arrange(desc(n)) 
nrow(P_P_word_100)

# another method
P_P_words %>%
    count(word) %>%
    top_n(1, n) %>%
    pull(n)
```
Sentiment analysis:
```{r}
afinn <- get_sentiments("afinn")

## words that are both in afinn and words:
afinn_sentiment <- P_P_words %>% inner_join(afinn, by = "word")
nrow(afinn_setiment)

##proportion of words that have a positive value
mean(afinn_sentiment$value > 0)

## afinn setiments which has a value of 4:
sum(afinn_setiment$value==4)
```
## Final Project:
This projects is about the hurricane Maria in 2017. it was one of the latgest natural distaers in Puerto Rico. However, the number of death was reported so low , about 64 deaths, and very impossible record. This data was came from direct death from the hurricane. However, for undestanding the real statistics of death, staticticians anlalyzed how death rates has been changed after the hurricane and estimated the excess number of deaths likely resulted in the hurricane.  
In this project we aim to analyze the actual daily mortality data from Puerto Rico and consider if the Hurricane Maria had an immediate effect on daily mortality compared to unaffected days in September 2015-2017.

step1: reading the pdf file od data:
```{r}
## loading libraries:
library(tidyverse)
library(pdftools)
options(digits = 3)  #show three signinficant digits

#finding PDF file of daily mortality for Puerto Rico from Jan 1, 2015 to May 31, 2018:
fn <- system.file("extdata", "RD-Mortality-Report_2015-18-180531.pdf", package="dslabs")

## opening the pdf files (in windows):
system("cmd.exe", input = paste("start", fn))

## On Mac: system2("open", args = fn)
```
step2: selecting one page of pdf and extract the table and then splitting the table to a lsit
```{r}
## saving pdf as a charactor string of length 12 (=pages of pdf)
txt <- pdf_text(fn)

## splitting all the words in this page
x <- str_split(txt[9],"\n")
class(x)  
length(x)  #the number of entries
```
storing list as charactor
```{r}
s <- x[[1]]
class(s)
length(s)
```
string processing: 
reomving the white spaces
```{r}
## reomving the white spaces
s <- s %>%
  str_trim()

## example of first row
s[1]
```
extracting the numbers from string:
```{r}
## storing head (the first line that 2015 will be appeared at):
#starting header_index
header_index <- str_which(s,"2015")[1]
header_index
```
```{r}
## creating the header:
tmp <-str_split(s[header_index],"\\s+",simplify=TRUE)
month <- tmp[1]
month
header <- tmp[-1]
header
```
```{r}
## the summary row in the table which contains "Total"
tail_index <- str_which(s,"Total")
tail_index
```
```{r}
## fining the rows which has numbers in them and also the roqs with only one number which are the numbers from the graph
n <- str_count(s,"\\d+") 
sum(n==1)
```
removing entries (rows) which we dont need them:
- entries before header
- entries after row containing total
- entries with only one number on them
```{r}
## sepcifiying the row number of rows we want to remove
out <- c(1:header_index, which(n==1), tail_index:length(s))
s <- s[-out]
length(s)
```
Removing remainig text that we dont need them
Note: In regex, using the ^ inside the square brackets [] means not, like the ! means not in !=
```{r}
## removing all other text that we dont need them:
s <- str_remove_all(s,"[^\\d\\s]") #means delete all the rows that they do not contain digits
```
now spliting the strings and making a table:
```{r}
## splitting the remained string and creating a matrix from them:
#note, even there are some numbers that were related to the graph, we can easiliy get rid of them as follow:
s_new <- str_split_fixed(s, "\\s+", n = 6)[,1:5]

## converting the matrix to a data frame
tab <- s_new %>% 
    as_data_frame() %>% 
    setNames(c("day", header)) %>%
    mutate_all(as.numeric)

## the mean of death in 2015, 2016:
mean(tab$"2015")
mean(tab$"2016")

## the mean of death from 1-19 of sep and 20-30?
mean(tab$"2017"[1:19])
mean(tab$"2017"[20:30])
```
converting to a tidy format:
```{r}
#tidy the table to have day,year,death as tidy format:
tab <- tab %>% gather(year, deaths, -day) %>%
    mutate(deaths = as.numeric(deaths))
tab
```
making a plot showing day and death grouped by year:
```{r}
tab %>% 
  filter(!year=="2018") %>%
  ggplot(aes(day,deaths,color=year)) +
  geom_line() +
  geom_vline(xintercept=20) +
  geom_point()
  
```

