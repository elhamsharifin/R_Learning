---
title: "8_Machine_Learning"
author: "Elham Sharifin"
date: "5/19/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Prerequest for this course
```{r}
library(dslabs)
library(tidyverse)
data(heights)

## class of different objects
class(heights)
class(heights$sex)  #factor vector
class(heights$height)  #numeric vector
class("Male")
class(75.00000)  #numeric

## the number of rows in the dataset
nrow(heights)

## height and sex in raw 777
heights$height[777]
heights$sex[777] #method 1
heights[777,1]   #method 2

## max and the row of min height
max(heights$height)
which.min(heights$height)

## mean and median height
mean(heights$height)
median(heights$height)

##  the proportion of male in the dataset
mean(heights$sex=="Male")

## the number of individuals taller than 78"
sum(heights$height>78)

## the number of females taller than 78"
sum(heights$height>78 & heights$sex=="Female")
```
# Part I: 
## Key points: Notation
- $X_1,...,X_p$ denote the features, $Y$ denotes the outcomes, and $\hat{Y}$ denotes the predictions.

- Machine learning prediction tasks can be divided into categorical and continuous outcomes. We refer to these as classification and prediction, respectively.

## Key points: An Example
- $Y_i=$ an outcome for observation or index i.

- We use boldface for $X_i$ to distinguish the vector of predictors from the individual predictors $X_{i,1},...,X_{i,784}$.
(Images are converted into 28by28 images, so we have 784 pixels)

- When referring to an arbitrary set of features and outcomes, we drop the index i and use Y and bold X.

- Uppercase is used to refer to variables because we think of predictors as random variables.

- Lowercase is used to denote observed values. For example, $X=x$.

# Part II: Machine Learning Basics
## Key points: Caret package, training and test sets, and overall accuracy
- To mimic the ultimate evaluation process, we randomly split our data into two — a training set and a test set — and act as if we don’t know the outcome of the test set. We develop algorithms using only the training set; the test set is used only for evaluation.

- The createDataPartition() function from the caret package can be used to generate indexes for randomly splitting data. 
time: how many random samples of indexes to return.
p: proportion of the index
list: you want indexes to be returned as a list or not.

- Note: contrary to what the documentation says, this course will use the argument p as the percentage of data that goes to testing. The indexes made from createDataPartition() should be used to create the test set.

- The simplest evaluation metric for categorical outcomes is overall accuracy: the proportion of cases that were correctly predicted in the test set.

```{r}
## libarary for using ML algorithms
library(caret)
```
```{r}
## Example of heights: We want to predict sex by height data
library(dslabs)
library(tidyverse)
library(caret)
data(heights)

## defining predictors and outcome 
y <- heights$sex    #categorical outcome (Male/Female)
x <- heights$height #we only have 1 predictor

## creating training and test sets
set.seed(2007, sample.kind = "Rounding")
test_index <- createDataPartition(y, times=1, p=0.5, list=FALSE)
train_set <- heights[-test_index,]
test_set <- heights[test_index,]
```
```{r}
## Gussing the outcome : we completely ignore the predictor and simply guess sex
set.seed(2007, sample.kind = "Rounding")
y_hat <- 
  sample(c("Male","Female"),length(test_index),replace=TRUE) %>%
  factor(levels = levels(test_set$sex))

mean(y_hat==test_set$sex)

```
```{r}
## Gussing the outcome:
y_hat <- sample(c("Male", "Female"), length(test_index), replace = TRUE)
y_hat <- sample(c("Male", "Female"), length(test_index), replace = TRUE) %>% 
  factor(levels = levels(test_set$sex))
```
```{r}
## Calculate the accuracy
mean(y_hat==test_set$sex)
heights %>% group_by(sex) %>% summarize(mean(height), sd(height))
# as seen, in average, men are taller than women
y_hat <- ifelse(x > 62, "Male", "Female") %>% factor(levels = levels(test_set$sex))
mean(y == y_hat)
# 62: initial gusss (2sd less than mean of male height)
```

```{r}
## investigate the accuracy of ten cutoffs
cutoff <- seq(61,70)
accuracy <- map_dbl(cutoff,function(x){
  y_hat <- ifelse(train_set$height>x, "Male","Female") %>%
    factor(levels=levels(test_set$sex))
  mean(y_hat==train_set$sex)
})

## plot of accuracy vs. cuttoff
data.frame(cutoff,accuracy) %>%
  ggplot(aes(cutoff,accuracy)) +
  geom_line() +
  geom_point()
```
```{r}
## max accuracy and best cutoff
max(accuracy)
best_cutoff <- cutoff[which.max(accuracy)]
best_cutoff

## now applying on test_set
 y_hat <- ifelse(test_set$height>best_cutoff, "Male","Female") %>%
    factor(levels=levels(test_set$sex))

y_hat <- factor(y_hat)
mean(y_hat==test_set$sex)
# it is a little less than the train set but very better than our guessing!
```
### Example:
for the mnist dataset in the dslabs, compuate the number of available features for prediction?
```{r}
library(dslabs)
mnist <- read_mnist()
ncol(mnist$train$images)
```

## Key points: Confusion matrix
- Overall accuracy can sometimes be a deceptive measure because of unbalanced classes.

- A general improvement to using overall accuracy is to study sensitivity and specificity separately. Sensitivity, also known as the true positive rate or recall, is the proportion of actual positive outcomes correctly identified as such. Specificity, also known as the true negative rate, is the proportion of actual negative outcomes that are correctly identified as such.
$Sensivity (TPR) =\frac{TP}{TP+FN}$
$Specificity (TNR)=\frac{TN}{TN+FP}$ 
Precision: $Specificity (PPV) =\frac{TP}{TP+FP}$

- precision depends on the prevalence, since higher prevalence implies, getting higher precision event when guessing

- A confusion matrix tabulates each combination of prediction and actual value. You can create a confusion matrix in R using the table() function or the confusionMatrix() function from the caret package.
```{r}
table(predicted=y_hat,actual=test_set$sex)
```
```{r}
## Computing accuracy for each set
test_set %>%
  mutate(y_hat=y_hat) %>%
  group_by(sex) %>%
  summarize(accuracy=mean(y_hat==sex))
```
```{r}
## calculating prevalence: 
#the reason high accuracy for male and low accuracy for women
prev <-mean(y=="Male")
prev
```
```{r}
## confusio Matrix
#install.packages("e1071")
confusionMatrix(data=y_hat,reference = test_set$sex)
```
## Key points: Balanced accuracy and F1 score
- For optimization purposes, sometimes it is more useful to have a one number summary than studying both specificity and sensitivity. One preferred metric is balanced accuracy. Because specificity and sensitivity are rates, it is more appropriate to compute the harmonic average. In fact, the F1-score, a widely used one-number summary, is the harmonic average of precision and recall. 
$F_1score=\frac{1}{\frac{1}{2}(\frac{1}{recall}+\frac{1}{precision})}$
After similfying: $F_1score=2\frac{precision.recall}{precision+recall}$

- Depending on the context, some type of errors are more costly than others. The F1-score can be adapted to weigh specificity and sensitivity differently. 

- You can compute the F1-score using the F_meas() function in the caret package.

```{r}
# maximizing F-score instead of maximum accuracy
cutoff <- seq(61,70)

F_1 <- map_dbl(cutoff,function(x){
  y_hat <- ifelse(train_set$height>x,"Male","Female") %>%
    factor(levels=levels(test_set$sex))
  F_meas(data=y_hat, reference = factor(train_set$sex))
})
max(F_1)
```
```{r}
## plotting and showing cutoff equavalent with max F_1
data.frame(cutoff,F_1) %>%
  ggplot(aes(cutoff,F_1)) +
  geom_line() +
  geom_point()
```

```{r}
best_cutoff <- cutoff[which.max(F_1)]
best_cutoff
```
```{r}
## it will balance the sensitivity and specificity
y_hat <- ifelse(test_set$height > best_cutoff, "Male", "Female") %>% 
  factor(levels = levels(test_set$sex))
sensitivity(data = y_hat, reference = test_set$sex)
specificity(data = y_hat, reference = test_set$sex)
```
```{r}
## confusio matrix
confusionMatrix(data=y_hat,reference = test_set$sex)
```
## Key points: Prevalence matters in practice
-A machine learning algorithm with very high sensitivity and specificity may not be useful in practice when prevalence is close to either 0 or 1. For example, if you develop an algorithm for disease diagnosis with very high sensitivity, but the prevalence of the disease is pretty low, then the precision of your algorithm is probably very low based on Bayes' theorem.

## Key points: ROC and precision-recall curves
- A very common approach to evaluating accuracy and F1-score is to compare them graphically by plotting both. A widely used plot that does this is the receiver operating characteristic (ROC) curve. The ROC curve plots sensitivity (TPR) versus 1 - specificity or the false positive rate (FPR).

- However, ROC curves have one weakness and it is that neither of the measures plotted depend on prevalence. In cases in which prevalence matters, we may instead make a precision-recall plot, which has a similar idea with ROC curve.
```{r}
## Like previous:
## predicting sex by height data
library(dslabs)
library(tidyverse)
library(caret)
data(heights)

## defining predictors and outcome 
y <- heights$sex    #categorical outcome (Male/Female)
x <- heights$height #we only have 1 predictor

## creating training and test sets
set.seed(2007, sample.kind = "Rounding")
test_index <- createDataPartition(y, times=1, p=0.5, list=FALSE)
train_set <- heights[-test_index,]
test_set <- heights[test_index,]
```
```{r}
p <- 0.9
n <- length(test_index)
y_hat <- sample(c("Male", "Female"), n, replace = TRUE, prob=c(p, 1-p)) %>% 
  factor(levels = levels(test_set$sex))
mean(y_hat == test_set$sex)
```
```{r}
## ROC curve of gussing sex when the probability is not equal
n <- length(test_index)
probs <- seq(0, 1, length.out = 10)
guessing <- map_df(probs, function(p){
  y_hat <- 
    sample(c("Male", "Female"), n, replace = TRUE, prob=c(p, 1-p)) %>% 
    factor(levels = c("Female", "Male"))
  list(method = "Guessing",
       FPR = 1 - specificity(y_hat, test_set$sex),
       TPR = sensitivity(y_hat, test_set$sex))
})
guessing %>% qplot(FPR, TPR, data =., xlab = "1 - Specificity", ylab = "Sensitivity")
```
```{r}
## Method of cutoff
cutoffs <- c(50, seq(60, 75), 80)
height_cutoff <- map_df(cutoffs, function(x){
  y_hat <- ifelse(test_set$height > x, "Male", "Female") %>% 
    factor(levels = c("Female", "Male"))
   list(method = "Height cutoff",
        FPR = 1-specificity(y_hat, test_set$sex),
        TPR = sensitivity(y_hat, test_set$sex))
})

# plot both curves together
bind_rows(guessing, height_cutoff) %>%
  ggplot(aes(FPR, TPR, color = method)) +
  geom_line() +
  geom_point() +
  xlab("1 - Specificity") +
  ylab("Sensitivity")
```
As see, we obtain higher sensitivity with the height-based approach for all values of specificity, which imply it is, in fact, a better method.

```{r}
## adding cutoff points to the curve
library(ggrepel)
map_df(cutoffs, function(x){
  y_hat <- ifelse(test_set$height > x, "Male", "Female") %>% 
    factor(levels = c("Female", "Male"))
   list(method = "Height cutoff",
        cutoff = x, 
        FPR = 1-specificity(y_hat, test_set$sex),
        TPR = sensitivity(y_hat, test_set$sex))
}) %>%
  ggplot(aes(FPR, TPR, label = cutoff)) +
  geom_line() +
  geom_point() +
  geom_text_repel(nudge_x = 0.01, nudge_y = -0.01)
```
```{r}
# plot precision against recall
guessing <- map_df(probs, function(p){
  y_hat <- sample(c("Male", "Female"), length(test_index), 
                  replace = TRUE, prob=c(p, 1-p)) %>% 
    factor(levels = c("Female", "Male"))
  list(method = "Guess",
    recall = sensitivity(y_hat, test_set$sex),
    precision = precision(y_hat, test_set$sex))
})

height_cutoff <- map_df(cutoffs, function(x){
  y_hat <- ifelse(test_set$height > x, "Male", "Female") %>% 
    factor(levels = c("Female", "Male"))
  list(method = "Height cutoff",
       recall = sensitivity(y_hat, test_set$sex),
    precision = precision(y_hat, test_set$sex))
})

bind_rows(guessing, height_cutoff) %>%
  ggplot(aes(recall, precision, color = method)) +
  geom_line() +
  geom_point()
```
As seen, the precision of guessing is not high. This is because the prevalence is low.
```{r}
## Changing positives to mean male instead of females
guessing <- map_df(probs, function(p){
  y_hat <- sample(c("Male", "Female"), length(test_index), replace = TRUE, 
                  prob=c(p, 1-p)) %>% 
    factor(levels = c("Male", "Female"))
  list(method = "Guess",
    recall = sensitivity(y_hat, relevel(test_set$sex, "Male", "Female")),
    precision = precision(y_hat, relevel(test_set$sex, "Male", "Female")))
})

height_cutoff <- map_df(cutoffs, function(x){
  y_hat <- ifelse(test_set$height > x, "Male", "Female") %>% 
    factor(levels = c("Male", "Female"))
  list(method = "Height cutoff",
       recall = sensitivity(y_hat, relevel(test_set$sex, "Male", "Female")),
    precision = precision(y_hat, relevel(test_set$sex, "Male", "Female")))
})
bind_rows(guessing, height_cutoff) %>%
  ggplot(aes(recall, precision, color = method)) +
  geom_line() +
  geom_point()
```
As seen, if we change positives to mean male instead of females the ROC curve remains the same, but the precision recall plot changes.

### Exercise 1: prediction of sex of Online ans inclass 
On 2016-01-25 at 8:15 AM, the professor asked student to fill in the sex and height questionnaire that populated the reported_heights dataset. The online students filled out the survey during the next few days, after the lecture was posted online. We want to define a variable which we will call type, to denote the type of student, inclass or online based on the time of filling the sex information!
```{r}
library(dslabs)
library(dplyr)
library(lubridate)
data(reported_heights)

dat <- mutate(reported_heights, date_time = ymd_hms(time_stamp)) %>%
  filter(date_time >= make_date(2016, 01, 25) & date_time < make_date(2016, 02, 1)) %>%
  mutate(type = ifelse(day(date_time) == 25 & hour(date_time) == 8 & between(minute(date_time), 15, 30), "inclass","online")) %>%
  select(sex, type)

y <- factor(dat$sex, c("Female", "Male"))  #categorical outcome 
x <- dat$type   #we only have 1 predictor
```
```{r}
## the proportion of inclass and online group which are female:
dat %>% group_by(type) %>% summarize(prop_female = mean(sex == "Female"))
```
```{r}
## prediction of outcome based on the calculated probability:
y <- factor(dat$sex, c("Female", "Male"))  #categorical outcome 
x <- dat$type   #we only have 1 predictor

y_hat <- ifelse(x =="inclass", "Female", "Male") %>% factor(levels = levels(y))
mean(y == y_hat)
```
```{r}
## confusuion matrix between y and y_hat
table(y_hat,y)  #or table(y,y_hat)   #method 1

confusionMatrix(data=y_hat,reference = y)  #method 2
```
```{r}
## the sensitivity and specificity of the prediction
library(caret)
sensitivity(y_hat, y) #method 1
specificity(y_hat,y)

confusionMatrix(data=y_hat,reference = y)  #method 2
```
```{r}
## the prevalence of female in dat dataset
prev <- mean(y=="Female")
prev
```
### Exercise 2: iris dataset (several predictors) very Important example
#### step 1:creating train and test data
```{r}
# removing the setosa species and focusing on the versicolor and virginica iris species:
library(caret)
data(iris)
iris <- iris[-which(iris$Species=='setosa'),]
y <- iris$Species
```
```{r}
## partitioning dataset to train and test data:
set.seed(2,sample.kind = "Rounding")    
test_index <- createDataPartition(y,time=1,p=0.5,list=FALSE)
test <- iris[test_index,]
train <- iris[-test_index,]
```
#### step2: Which feature produces the highest accuracy?
##### Method 1: without a function
```{r}
## considering Sepal.Length
range(train$Sepal.Length)
cutoff <- seq(5,7.9,0.1)
accuracy <- map_dbl(cutoff,function(x){
  y_hat <- ifelse(train$Sepal.Length>x, "virginica","versicolor") %>%
    factor(levels=levels(test$Species))
  mean(y_hat==train$Species)
})
max(accuracy)
```
```{r}
## considering Sepal.Width
range(train$Sepal.Width)
cutoff <- seq(2,3.8,0.1)
accuracy <- map_dbl(cutoff,function(x){
  y_hat <- ifelse(train$Sepal.Width>x, "virginica","versicolor") %>%
    factor(levels=levels(test$Species))
  mean(y_hat==train$Species)
})
max(accuracy)
```
```{r}
## considering Petal.Length
range(train$Petal.Length)
cutoff <- seq(3,6.9,0.1)
accuracy <- map_dbl(cutoff,function(x){
  y_hat <- ifelse(train$Petal.Length>x, "virginica","versicolor") %>%
    factor(levels=levels(test$Species))
  mean(y_hat==train$Species)
})
max(accuracy)
```
```{r}
## considering Petal.Width
range(train$Petal.Width)
cutoff <- seq(1,2.5,0.1)
accuracy <- map_dbl(cutoff,function(x){
  y_hat <- ifelse(train$Petal.Width>x, "virginica","versicolor") %>%
    factor(levels=levels(test$Species))
  mean(y_hat==train$Species)
})
max(accuracy)
```
##### Method 2: function
```{r}
func_feature <- function(x){
	cutoff <- seq(range(x)[1],range(x)[2],by=0.1)
	sapply(cutoff,function(i){
		y_hat <- ifelse(x>i,'virginica','versicolor')
		mean(y_hat==train$Species)
	})
}
predictions <- apply(train[,-5],2,func_feature) 
#not include column5  #2:column/ and 1: raws
sapply(predictions,max)	
```
#### step 3: Overal accuracy in the test data
```{r}
## for the selected feature, calculate the overall accuracy in the test data using the smart cutoff value from the training data
#What is the overall accuracy?
predictions <- func_feature(train[,3])
cutoff <- seq(range(train[,3])[1],range(train[,3])[2],by=0.1)
cutoffs <-cutoff[which(predictions==max(predictions))]

y_hat <- ifelse(test[,3]>cutoffs[1],'virginica','versicolor')
mean(y_hat==test$Species)
```

Notice that we had an overall accuracy greater than 96% in the training data, but the overall accuracy was lower in the test data. This can happen often if we overtrain.  In fact, it could be the case that a single feature is not the best choice. For example, a combination of features might be optimal.Using a single feature and optimizing the cutoff as we did on our training data can lead to overfitting.

#### step 4: Which feature best optimizes our overall accuracy?
```{r}
foo <- function(x){
	rangedValues <- seq(range(x)[1],range(x)[2],by=0.1)
	sapply(rangedValues,function(i){
		y_hat <- ifelse(x>i,'virginica','versicolor')
		mean(y_hat==test$Species)
	})
}
predictions <- apply(test[,-5],2,foo)
sapply(predictions,max)	
```
#### step 5: exploratory data analysis on the data. 
```{r}
plot(iris,pch=21,bg=iris$Species)

```
As seen Petal.Length and Petal.Width in combination could potentially be more informative than either feature alone.

Optimize the the cutoffs for Petal.Length and Petal.Width separately in the train dataset by using the seq function with increments of 0.1. Then, report the overall accuracy when applied to the test dataset by creating a rule that predicts virginica if Petal.Length is greater than the length cutoff OR Petal.Width is greater than the width cutoff, and versicolor otherwise.
What is the overall accuracy for the test data now?
```{r}
## Elham solved
predictions_L <- func_feature(train[,3])
cutoff_L <- seq(range(train[,3])[1],range(train[,3])[2],by=0.1)
cutoffs_L <-cutoff[which(predictions_L==max(predictions_L))]
cutoffs_L

predictions_W <- func_feature(train[,4])
cutoff_W <- seq(range(train[,4])[1],range(train[,4])[2],by=0.1)
cutoffs_W <-cutoff[which(predictions_W==max(predictions_W))]
cutoffs_W

y_hat <- ifelse(test[,3]>cutoffs_L[1]|test[,4]>cutoffs_W[1],'virginica','versicolor')
mean(y_hat==test$Species)
```
```{r}
## method 2: (by edx)
library(caret)
data(iris)
iris <- iris[-which(iris$Species=='setosa'),]
y <- iris$Species

plot(iris,pch=21,bg=iris$Species)

set.seed(2)
test_index <- createDataPartition(y,times=1,p=0.5,list=FALSE)
test <- iris[test_index,]
train <- iris[-test_index,]
            
petalLengthRange <- seq(range(train$Petal.Length)[1],range(train$Petal.Length)[2],by=0.1)
petalWidthRange <- seq(range(train$Petal.Width)[1],range(train$Petal.Width)[2],by=0.1)

length_predictions <- sapply(petalLengthRange,function(i){
		y_hat <- ifelse(train$Petal.Length>i,'virginica','versicolor')
		mean(y_hat==train$Species)
	})
length_cutoff <- petalLengthRange[which.max(length_predictions)] # 4.7

width_predictions <- sapply(petalWidthRange,function(i){
		y_hat <- ifelse(train$Petal.Width>i,'virginica','versicolor')
		mean(y_hat==train$Species)
	})
width_cutoff <- petalWidthRange[which.max(width_predictions)] # 1.5

y_hat <- ifelse(test$Petal.Length>length_cutoff | test$Petal.Width>width_cutoff,'virginica','versicolor')
mean(y_hat==test$Species)
```

## Key points: Conditional probabilities
- Conditional probabilities for each class: 
$pk(x)=Pr(Y=k|X=x), for k=1,...,K$
$\hat{Y}=max_k(p_k(x))$

- So, how good will our prediction be will depend on two things:
how close $max_k(p_k(x))$ is to 1
how close our estimate $\hat{p}_k(X)$ is to ${p_k(X)}$.

- In machine learning, this is referred to as Bayes' Rule. This is a theoretical rule because in practice we don't know p(x). Having a good estimate of the p(x) will suffice for us to build optimal prediction models, since we can control the balance between specificity and sensitivity however we wish. In fact, estimating these conditional probabilities can be thought of as the main challenge of machine learning. 

## Key points
- Due to the connection between conditional probabilities and conditional expectations:
$pk(x)=Pr(Y=k|X=x), for k=1,...,K$
we often only use the expectation to denote both the conditional probability and conditional expectation.

- For continuous outcomes, we define a loss function to evaluate the model. The most commonly used one is MSE (Mean Squared Error). The reason why we care about the conditional expectation in machine learning is that the expected value minimizes the MSE:
$\hat{Y}=E(Y|X=x) minimize E\{(\hat{Y}-Y)^2|X=x\}$

Due to this property, a succinct description of the main task of machine learning is that we use data to estimate for any set of features. The main way in which competing machine learning algorithms differ is in their approach to estimating this expectation.

### example:
The test is + 85% of the time when tested on a patient who has disease (high sensitivity): P(test+|disease)=0.85

The test is - 90% of the time when tested on a healthy patient (high specificity): P(test−|heathy)=0.90

The disease is prevalent in about 2% of the community: P(disease)=0.02

the probability that if the test is positive, tou have disease?

$p(disease|+)=p(+|disease)\frac{p(disease)}{p(+)} =p(+|disease)\frac{p(disease)}{p(+|disease).p(disease)+p(+|healthy).p(healthy)}$
```{r}
(0.085*0.02)/(0.85*0.02+0.1*0.98)
```

### exercise 2:
Suppose there ia a hypothetical population of 1 million individuals with the following conditional probabilities:

- The test is positive 85% of the time when tested on a patient with the disease (high sensitivity): P(test+|disease)=0.85

- The test is negative 90% of the time when tested on a healthy patient (high specificity): P(test−|heathy)=0.90

- The disease is prevalent in about 2% of the community: 
P(disease)=0.02

```{r}
set.seed(1,sample.kind = "Rounding")

disease <- sample(c(0,1), size=1e6, replace=TRUE, prob=c(0.98,0.02))

test <- rep(NA, 1e6)
test[disease==0] <- sample(c(0,1), size=sum(disease==0), replace=TRUE, prob=c(0.90,0.10))

test[disease==1] <- sample(c(0,1), size=sum(disease==1), replace=TRUE, prob=c(0.15, 0.85))

## the probability that a test is positive:
p_positive <- mean(test)
p_negative <- 1 - p_positive

p_positive
p_negative

## the probability that an individual has disease if the test result is negative?
mean(disease[test==0]==1)

## the probability that you have disease if the test is positive?
mean(disease[test==1]==1)

## Comparing the prevalence of disease in people who test positive to the overall prevalence of disease. If a patient's test is positive, how much does that increase their risk of having the disease?
mean(disease[test==1]==1)/mean(disease==1)
```

## Exercise 3: 
Calculating conditional probabilities for being male in the heights dataset: plot the estimated conditional probability for each x: $P(x)=Pr(Male|height=x)$ for each x:
```{r}
library(dslabs)
data("heights")
heights %>%
  mutate(height=round(height)) %>%
  group_by(height) %>%
  summarize(p=mean(sex=="Male")) %>%
qplot(height, p, data =.)
```

- As seen, there is high variability for low values of height because we have few data points. 
This time use the quantile 0.1,0.2,…,0.9 and the cut() function to assure each group has the same number of points. 
hint: for any numeric value of x, you can create group based on quantile like this: cut(x, quantile(x, seq(0, 1, 0.1)), include.lowest = TRUE). 
```{r}
library(dslabs)
data("heights")
ps <- seq(0, 1, 0.1)
heights %>% 
	mutate(g=cut(height, quantile(height, ps), include.lowest = TRUE)) %>%
	group_by(g) %>%
	summarize(p = mean(sex == "Male"), height = mean(height)) %>%
	qplot(height, p, data =.)
```

### Exercise 4:
generating data from a bivariate normal distrubution using the MASS package. estimate the conditional expectations and make a plot.
```{r}
Sigma <- 9*matrix(c(1,0.5,0.5,1), 2, 2)
dat <- MASS::mvrnorm(n = 10000, c(69, 69), Sigma) %>%
	data.frame() %>% setNames(c("x", "y"))

## plot
ps <- seq(0, 1, 0.1)
dat %>% 
	mutate(g = cut(x, quantile(x, ps), include.lowest = TRUE)) %>%
group_by(g) %>%
summarize(y = mean(y), x = mean(x)) %>%
	qplot(x, y, data =.)
```

# Section 3: Linear Regression for Prediction, Smoothing, and Working with Matrices 

## Key points:Linear Regression for Prediction:
Linear regression can be considered a machine learning algorithm. Although it can be too rigid to be useful, it works rather well for some challenges. It also serves as a baseline approach: if you can’t beat it with a more complex approach, you probably want to stick to linear regression. 
```{r}
## Building a ML algorithm that predict son's height Y, using father's height X. 
library(HistData)
set.seed(1983)
galton_heights <- GaltonFamilies %>%
  filter(gender == "male") %>%
  group_by(family) %>%
  sample_n(1) %>%
  ungroup() %>%
  select(father, childHeight) %>%
  rename(son = childHeight)

y <- galton_heights$son
test_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE)
train_set <- galton_heights %>% slice(-test_index)
test_set <- galton_heights %>% slice(test_index)

m <- mean(train_set$son)
# squared loss
mean((m - test_set$son)^2)

# fit linear regression model
fit <- lm(son ~ father, data = train_set)
fit$coef
y_hat <- fit$coef[1] + fit$coef[2]*test_set$father
mean((y_hat - test_set$son)^2)
```
## Key points: predict function
The predict() function takes a fitted object from functions such as lm() or glm() and a data frame with the new predictors for which to predict. We can use predict like this:
```{r}
y_hat <- predict(fit, test_set)
```

predict() is a generic function in R that calls other functions depending on what kind of object it receives. To learn about the specifics, you can read the help files using code like this: 
```{r}
?predict.lm    # or ?predict.glm
```

```{r}
y_hat <- predict(fit, test_set)
mean((y_hat - test_set$son)^2)

# read help files
?predict.lm
?predict.glm
```

## Exercise:
Create a dataset scuh:
```{r}
set.seed(1, sample.kind="Rounding") 
n <- 100
Sigma <- 9*matrix(c(1.0, 0.5, 0.5, 1.0), 2, 2)
dat <- MASS::mvrnorm(n = 100, c(69, 69), Sigma) %>%
      data.frame() %>% setNames(c("x", "y"))
```

We will build 100 linear models using the data above and calculate the mean and standard deviation of the combined models. - First, set the seed to 1 again. 
- Then, within a replicate() loop, 
(1) partition the dataset into test and training sets with p=0.5 and using dat$y to generate your indices, 
(2) train a linear model predicting y from x, 
(3) generate predictions on the test set, and 
(4) calculate the RMSE of that model. 
Then, report the mean and standard deviation (SD) of the RMSEs from all 100 models.
```{r}
## 100 lm fit (Elham)
set.seed(1, sample.kind="Rounding") 
RMSE<-replicate(100,{
 test_index <- createDataPartition(dat$y, times=1, p=0.5, list=FALSE)
train_set <- dat[-test_index,]
test_set <- dat[test_index,]

fit <- lm(y ~ x, data = train_set)

y_hat <- predict(fit, test_set)
sqrt(mean((y_hat - test_set$y)^2)) 
})

mean(RMSE)
sd(RMSE)
```
```{r}
## Edex
set.seed(1, sample.kind="Rounding") 
rmse <- replicate(100, {
	test_index <- createDataPartition(dat$y, times = 1, p = 0.5, list = FALSE)
	train_set <- dat %>% slice(-test_index)
	test_set <- dat %>% slice(test_index)
	fit <- lm(y ~ x, data = train_set)
 	y_hat <- predict(fit, newdata = test_set)
	sqrt(mean((y_hat-test_set$y)^2))
})

mean(rmse)
sd(rmse)
```

## Exercise2:
Write a function that takes a size n, then 
(1) builds a dataset using the code provided at the top of Q1 but with n observations instead of 100 and without the set.seed(1), (2) runs the replicate() loop that you wrote to answer Q1, which builds 100 linear models and returns a vector of RMSEs, and 
(3) calculates the mean and standard deviation of the 100 RMSEs.
```{r}
set.seed(1, sample.kind="Rounding") 
func <- function(n){
Sigma <- 9*matrix(c(1.0, 0.5, 0.5, 1.0), 2, 2)
dat <- MASS::mvrnorm(n, c(69, 69), Sigma) %>%
data.frame() %>% setNames(c("x", "y"))
rmse <- replicate(100, {
	test_index <- createDataPartition(dat$y, times = 1, p = 0.5, list = FALSE)
	train_set <- dat %>% slice(-test_index)
	test_set <- dat %>% slice(test_index)
	fit <- lm(y ~ x, data = train_set)
 	y_hat <- predict(fit, newdata = test_set)
	sqrt(mean((y_hat-test_set$y)^2))
})
c(avg=mean(rmse),sd=sd(rmse))
}


n <- c(100, 500, 1000, 5000, 10000)
sapply(n,func)
```
Result: On average, the RMSE does not change much as n gets larger, but the variability of the RMSE decreases.

## Exercise 3:
Now repeat the exercise from Q1, this time making the correlation between x and y larger, as in the following code:
```{r}
set.seed(1, sample.kind = "Rounding")
n <- 100
Sigma <- 9*matrix(c(1.0, 0.95, 0.95, 1.0), 2, 2)
dat <- MASS::mvrnorm(n = 100, c(69, 69), Sigma) %>%
data.frame() %>% setNames(c("x", "y"))

## Edex
set.seed(1, sample.kind="Rounding") 
rmse <- replicate(100, {
	test_index <- createDataPartition(dat$y, times = 1, p = 0.5, list = FALSE)
	train_set <- dat %>% slice(-test_index)
	test_set <- dat %>% slice(test_index)
	fit <- lm(y ~ x, data = train_set)
 	y_hat <- predict(fit, newdata = test_set)
	sqrt(mean((y_hat-test_set$y)^2))
})

mean(rmse)
sd(rmse)
```
Result: When we increase the correlation between x and y, x has more predictive power and thus provides a better estimate of y.

## Exercise 4:
Use the below dataset
- Note that y is correlated with both x_1 and x_2 but the two predictors are independent of each other, as seen by cor(dat).

- Set the seed to 1, then use the caret package to partition into a test and training set of equal size. Compare the RMSE when using just x_1, just x_2 and both x_1 and x_2. Train a single linear model for each (not 100 like in the previous questions).

Which of the three models performs the best (has the lowest RMSE)?

```{r}
set.seed(1)
Sigma <- matrix(c(1.0, 0.75, 0.75, 0.75, 1.0, 0.25, 0.75, 0.25, 1.0), 3, 3)
dat <- MASS::mvrnorm(n = 100, c(0, 0, 0), Sigma) %>%
	data.frame() %>% setNames(c("y", "x_1", "x_2"))
cor(dat$x_1,dat$x_2)
cor(dat)
```
```{r}
# Only x_1
set.seed(1, sample.kind="Rounding") 
test_index <- createDataPartition(dat$y, times = 1, p = 0.5, list = FALSE)
train_set <- dat %>% slice(-test_index)
test_set <- dat %>% slice(test_index)
fit <- lm(y ~ x_1, data = train_set)
y_hat <- predict(fit, newdata = test_set)
rmse_1 <- sqrt(mean((y_hat-test_set$y)^2))
rmse_1
```
```{r}
# Only x_2
set.seed(1, sample.kind="Rounding") 
test_index <- createDataPartition(dat$y, times = 1, p = 0.5, list = FALSE)
train_set <- dat %>% slice(-test_index)
test_set <- dat %>% slice(test_index)
fit <- lm(y ~ x_2, data = train_set)
y_hat <- predict(fit, newdata = test_set)
rmse_2 <- sqrt(mean((y_hat-test_set$y)^2))
rmse_2
```
```{r}
# Only x_1, x_2
set.seed(1, sample.kind="Rounding") 
test_index <- createDataPartition(dat$y, times = 1, p = 0.5, list = FALSE)
train_set <- dat %>% slice(-test_index)
test_set <- dat %>% slice(test_index)
fit <- lm(y ~ x_1+x_2, data = train_set)
y_hat <- predict(fit, newdata = test_set)
rmse_2 <- sqrt(mean((y_hat-test_set$y)^2))
rmse_2
```
## Exercise 5:
Repeat the exercise from Q6 but now create an example in which x_1 and x_2 are highly correlated.
```{r}
set.seed(1)
Sigma <- matrix(c(1.0, 0.75, 0.75, 0.75, 1.0, 0.95, 0.75, 0.95, 1.0), 3, 3)
dat <- MASS::mvrnorm(n = 100, c(0, 0, 0), Sigma) %>%
	data.frame() %>% setNames(c("y", "x_1", "x_2"))
```
```{r}
# Only x_1
set.seed(1, sample.kind="Rounding") 
test_index <- createDataPartition(dat$y, times = 1, p = 0.5, list = FALSE)
train_set <- dat %>% slice(-test_index)
test_set <- dat %>% slice(test_index)
fit <- lm(y ~ x_1, data = train_set)
y_hat <- predict(fit, newdata = test_set)
rmse_1 <- sqrt(mean((y_hat-test_set$y)^2))
rmse_1
```
```{r}
# Only x_2
set.seed(1, sample.kind="Rounding") 
test_index <- createDataPartition(dat$y, times = 1, p = 0.5, list = FALSE)
train_set <- dat %>% slice(-test_index)
test_set <- dat %>% slice(test_index)
fit <- lm(y ~ x_2, data = train_set)
y_hat <- predict(fit, newdata = test_set)
rmse_2 <- sqrt(mean((y_hat-test_set$y)^2))
rmse_2
```
```{r}
# Only x_1, x_2
set.seed(1, sample.kind="Rounding") 
test_index <- createDataPartition(dat$y, times = 1, p = 0.5, list = FALSE)
train_set <- dat %>% slice(-test_index)
test_set <- dat %>% slice(test_index)
fit <- lm(y ~ x_1+x_2, data = train_set)
y_hat <- predict(fit, newdata = test_set)
rmse_2 <- sqrt(mean((y_hat-test_set$y)^2))
rmse_2
```
Result: Adding extra predictors can improve RMSE substantially, but not when the added predictors are highly correlated with other predictors.

## Key points: Regression for a Categorical Outcome
The regression approach can be extended to categorical data. For example, we can try regression to estimate the conditional probability:
$p(x)=Pr(Y=1|X=x)=\beta_0+\beta_1x$
Once we have estimates $\beta_0$  and $\beta_1$, we can obtain an actual prediction p(x). Then we can define a specific decision rule to form a prediction.

Example: what is the conditional probability of being female if you are 66 inches?
```{r}
library(dslabs)
data("heights")
y <- heights$height

set.seed(2, sample.kind = "Rounding") 

test_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE)
train_set <- heights %>% slice(-test_index)
test_set <- heights %>% slice(test_index)

train_set %>% 
  filter(round(height)==66) %>%
  summarize(y_hat = mean(sex=="Female"))
```
```{r}
heights %>% 
  mutate(x = round(height)) %>%
  group_by(x) %>%
  filter(n() >= 10) %>%
  summarize(prop = mean(sex == "Female")) %>%
  ggplot(aes(x, prop)) +
  geom_point()
```
```{r}
lm_fit <- mutate(train_set, y = as.numeric(sex == "Female")) %>% lm(y ~ height, data = .)

#predict female if p_hat>0.5
p_hat <- predict(lm_fit, test_set)
y_hat <- ifelse(p_hat > 0.5, "Female", "Male") %>% factor()
confusionMatrix(y_hat, test_set$sex)$overall["Accuracy"]
```

## Key point: Logistic Regression
- logistic regression is an extension of linear regression that assures that the estimate of conditional probability Pr(Y=1|X=x)  is between 0 and 1. 

This approach makes use of the logistic transformation: 
$g(p)=log{\frac{p}{1-p}}$ With logistic regression, we model the conditional probability directly with:
$g\{Pr(Y=1|X=x)\}=\beta_0+\beta_1x$

- Note that with this model, we can no longer use least squares. Instead we compute the maximum likelihood estimate (MLE). 

In R, we can fit the logistic regression model with the function glm() (generalized linear models). If we want to compute the conditional probabilities, we want type="response" since the default is to return the logistic transformed values.
```{r}
heights %>% 
  mutate(x = round(height)) %>%
  group_by(x) %>%
  filter(n() >= 10) %>%
  summarize(prop = mean(sex == "Female")) %>%
  ggplot(aes(x, prop)) +
  geom_point() + 
  geom_abline(intercept = lm_fit$coef[1], slope = lm_fit$coef[2])
range(p_hat)

# fit logistic regression model
glm_fit <- train_set %>% 
  mutate(y = as.numeric(sex == "Female")) %>%
  glm(y ~ height, data=., family = "binomial")

p_hat_logit <- predict(glm_fit, newdata = test_set, type = "response")

y_hat_logit <- ifelse(p_hat_logit > 0.5, "Female", "Male") %>% factor

confusionMatrix(y_hat_logit, test_set$sex)$overall[["Accuracy"]]
```
## Key points: Case Study: 2 or 7
In this case study we apply logistic regression to classify whether a digit is two or seven. We are interested in estimating a conditional probability that depends on two variables:

$g\{p(x_1,x_2)\}=g\{Pr(Y=1|X_1=x_1,X_2=x_2)\}=\beta_0+\beta_1x_1+\beta_2x_2$

Through this case, we know that logistic regression forces our estimates to be a plane and our boundary to be a line. This implies that a logistic regression approach has no chance of capturing the non-linear nature of the true  p(x1,x2) . Therefore, we need other more flexible methods that permit other shapes.

```{r}
mnist <- read_mnist()
is <- mnist_27$index_train[c(which.min(mnist_27$train$x_1), which.max(mnist_27$train$x_1))]
titles <- c("smallest","largest")
tmp <- lapply(1:2, function(i){
    expand.grid(Row=1:28, Column=1:28) %>%
        mutate(label=titles[i],
               value = mnist$train$images[is[i],])
})

tmp <- Reduce(rbind, tmp)
tmp %>% ggplot(aes(Row, Column, fill=value)) +
    geom_raster() +
    scale_y_reverse() +
    scale_fill_gradient(low="white", high="black") +
    facet_grid(.~label) +
    geom_vline(xintercept = 14.5) +
    geom_hline(yintercept = 14.5)
```
```{r}
#7: representive of upper left pannes
#2: representative of the lower right panel
data("mnist_27")
mnist_27$train %>% ggplot(aes(x_1, x_2, color = y)) + geom_point()
```
```{r}
## why the problem is challenging:
is <- mnist_27$index_train[c(which.min(mnist_27$train$x_2), which.max(mnist_27$train$x_2))]
titles <- c("smallest","largest")
tmp <- lapply(1:2, function(i){
    expand.grid(Row=1:28, Column=1:28) %>%
        mutate(label=titles[i],
               value = mnist$train$images[is[i],])
})
tmp <- Reduce(rbind, tmp)
tmp %>% ggplot(aes(Row, Column, fill=value)) +
    geom_raster() +
    scale_y_reverse() +
    scale_fill_gradient(low="white", high="black") +
    facet_grid(.~label) +
    geom_vline(xintercept = 14.5) +
    geom_hline(yintercept = 14.5)
```
```{r}
## writing the ML algorithm and accuracy with glm:
fit_glm <- glm(y ~ x_1 + x_2, data=mnist_27$train, family = "binomial")
p_hat_glm <- predict(fit_glm, mnist_27$test)
y_hat_glm <- factor(ifelse(p_hat_glm > 0.5, 7, 2))
confusionMatrix(data = y_hat_glm, reference = mnist_27$test$y)$overall["Accuracy"]
```
```{r}
## True conditional probability:
mnist_27$true_p %>% ggplot(aes(x_1, x_2, fill=p)) +
    geom_raster()
```
```{r}
## changing the color and suing curve!
mnist_27$true_p %>% ggplot(aes(x_1, x_2, z=p, fill=p)) +
    geom_raster() +
    scale_fill_gradientn(colors=c("#F8766D","white","#00BFC4")) +
    stat_contour(breaks=c(0.5), color="black")
```
```{r}
##compute the boundary that divides the values of x1 and x2 that make the estimated conditional probably lower than 0.5 and larger than 0.5, So at this boundary, the conditional probability is going to be equal to 0.5.:
p_hat <- predict(fit_glm, newdata = mnist_27$true_p)
mnist_27$true_p %>%
    mutate(p_hat = p_hat) %>%
    ggplot(aes(x_1, x_2,  z=p_hat, fill=p_hat)) +
    geom_raster() +
    scale_fill_gradientn(colors=c("#F8766D","white","#00BFC4")) +
    stat_contour(breaks=c(0.5),color="black")
```
boundary can't be anything other than a straight line, which
implies that our logistic regression approach has no chance of capturing the non-linear nature of our true conditional probability. Boundary of true conditional probability is the curve!
```{r}
p_hat <- predict(fit_glm, newdata = mnist_27$true_p)
mnist_27$true_p %>%
    mutate(p_hat = p_hat) %>%
    ggplot() +
    stat_contour(aes(x_1, x_2, z=p_hat), breaks=c(0.5), color="black") +
    geom_point(mapping = aes(x_1, x_2, color=y), data = mnist_27$test)
```

The error comes from:
Because logistic regression divides the sevens and the twos with a line, we will miss several points that can't be captured by this shape. So we need something more flexible.

## Exercise: Logistic Regression
```{r}
## Defining the following dataset:
set.seed(2, sample.kind="Rounding")  
make_data <- function(n = 1000, p = 0.5, 
				mu_0 = 0, mu_1 = 2, 
				sigma_0 = 1,  sigma_1 = 1){

y <- rbinom(n, 1, p)
f_0 <- rnorm(n, mu_0, sigma_0)
f_1 <- rnorm(n, mu_1, sigma_1)
x <- ifelse(y == 1, f_1, f_0)
  
test_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE)

list(train = data.frame(x = x, y = as.factor(y)) %>% slice(-test_index),test = data.frame(x = x, y = as.factor(y)) %>% slice(test_index))
}
dat <- make_data()
```
Note that we have defined a variable x that is predictive of a binary outcome y:
```{r}
dat$train %>% ggplot(aes(x, color = y)) + geom_density()
```
- Set the seed to 1
- then use the make_data() function defined above to generate 25 different datasets with mu_1 <- seq(0, 3, len=25). 
- Perform logistic regression on each of the 25 different datasets (predict 1 if p>0.5) and plot accuracy (res in the figures) vs mu_1 (delta in the figures).”
Which is the correct plot?
```{r}
set.seed(1, sample.kind="Rounding") 
delta <- seq(0, 3, len = 25)
res <- sapply(delta, function(d){
	dat <- make_data(mu_1 = d)
	fit_glm <- dat$train %>% glm(y ~ x, family = "binomial", data = .)
	y_hat_glm <- ifelse(predict(fit_glm, dat$test) > 0.5, 1, 0) %>% factor(levels = c(0, 1))
	mean(y_hat_glm == dat$test$y)
})
qplot(delta, res)
```

## Key Points: Introduction to Smoothing: (Low band pass fitting, curve fitting)
Smoothing is a very powerful technique used all across data analysis. It is designed to detect trends in the presence of noisy data in cases in which the shape of the trend is unknown. 
The concepts behind smoothing techniques are extremely useful in machine learning because conditional expectations/probabilities can be thought of as trends of unknown shapes that we need to estimate in the presence of uncertainty.

```{r}
## the difference between Obama and Macain election
data("polls_2008")
qplot(day, margin, data = polls_2008)
```

##Key points: Bin Smoothing and Kernel
- The general idea of smoothing is to group data points into strata in which the value of  f(x)  can be assumed to be constant. We can make this assumption because we think  f(x)  changes slowly and, as a result,  f(x)  is almost constant in small windows of time. For the example of election, we assumed that public opinion stay constant during the week. 

- This assumption implies that a good estimate for f(x) is the average of the $Y_i$  values in the window. The estimate is:
$\hat{f}(x_0)=\frac{1}{N_0}\sum_{i\in A_0}{Y_i}$

In smoothing, we call the size of the interval  $|x−x_0|$  satisfying the particular condition the window size, bandwidth or span.
```{r}
# bin smoothers
span <- 7 
fit <- with(polls_2008,ksmooth(day, margin, x.points = day, kernel="box", bandwidth =span))
polls_2008 %>% mutate(smooth = fit$y) %>%
    ggplot(aes(day, margin)) +
    geom_point(size = 3, alpha = .5, color = "grey") + 
    geom_line(aes(day, smooth), color="red")
```
```{r}
# kernel
span <- 7
fit <- with(polls_2008, ksmooth(day, margin,  x.points = day, kernel="normal", bandwidth = span))
polls_2008 %>% mutate(smooth = fit$y) %>%
  ggplot(aes(day, margin)) +
  geom_point(size = 3, alpha = .5, color = "grey") + 
  geom_line(aes(day, smooth), color="red")
```

##Key points: Local Weighted Regression (loess)
A limitation of the bin smoothing approach is that we need small windows for the approximately constant assumptions to hold which may lead to imprecise estimates of  f(x) . Local weighted regression (loess) permits us to consider larger window sizes.

One important difference between loess and bin smoother is that we assume the smooth function is locally linear in a window instead of constant.

The result of loess is a smoother fit than bin smoothing because we use larger sample sizes to estimate our local parameters.

I personally prefer degree equals 1 as it is less prone to this kind of noise.

```{r}
total_days <- diff(range(polls_2008$day))
span <- 21/total_days

fit <- loess(margin~day, degree=1, span=span, data=polls_2008)

polls_2008 %>% mutate(smooth=fit$fitted) %>% 
  ggplot(aes(day, margin)) +
  geom_point(size=3, alpha=0.5, color="grey") + 
  geom_line(aes(day,smooth),color="red")

```
```{r}
## the ggplot does have smoothing for default
polls_2008 %>% ggplot(aes(day, margin)) +
  geom_point() + 
  geom_smooth()
```
But be careful with the default table, as they are rarely optimal.

```{r}
polls_2008 %>% 
  ggplot(aes(day, margin)) +
  geom_point() + 
  geom_smooth(color="red",span=0.15,method.args=list(degree=1))
```

## Exercise: Smoothing
mortality counts for Puerto Rico for 2015-2018:
```{r}
library(tidyverse)
library(lubridate)
library(purrr)
library(pdftools)
    
fn <- system.file("extdata", "RD-Mortality-Report_2015-18-180531.pdf", package="dslabs")
dat <- map_df(str_split(pdf_text(fn), "\n"), function(s){
	s <- str_trim(s)
	header_index <- str_which(s, "2015")[1]
	tmp <- str_split(s[header_index], "\\s+", simplify = TRUE)
	month <- tmp[1]
	header <- tmp[-1]
	tail_index  <- str_which(s, "Total")
	n <- str_count(s, "\\d+")
	out <- c(1:header_index, which(n==1), which(n>=28), tail_index:length(s))
	s[-out] %>%
		str_remove_all("[^\\d\\s]") %>%
		str_trim() %>%
		str_split_fixed("\\s+", n = 6) %>%
		.[,1:5] %>%
		as_data_frame() %>% 
		setNames(c("day", header)) %>%
		mutate(month = month,
			day = as.numeric(day)) %>%
		gather(year, deaths, -c(day, month)) %>%
		mutate(deaths = as.numeric(deaths))
}) %>%
	mutate(month = recode(month, "JAN" = 1, "FEB" = 2, "MAR" = 3, "APR" = 4, "MAY" = 5, "JUN" = 6, 
                          "JUL" = 7, "AGO" = 8, "SEP" = 9, "OCT" = 10, "NOV" = 11, "DEC" = 12)) %>%
	mutate(date = make_date(year, month, day)) %>%
        filter(date <= "2018-05-01")
```

Use the loess() function to obtain a smooth estimate of the expected number of deaths as a function of date. Plot this resulting smooth function. Make the span about two months long.

```{r}
span <- 60 / as.numeric(diff(range(dat$date)))

fit <- dat %>% mutate(x = as.numeric(date)) %>% loess(deaths ~ x, data = ., span = span, degree = 1)

dat %>% mutate(smooth = predict(fit, as.numeric(date))) %>%
	ggplot() +
	geom_point(aes(date, deaths)) +
	geom_line(aes(date, smooth), lwd = 2, col = "red")
```
with different colors for each year:
```{r}
dat %>% 
    mutate(smooth = predict(fit, as.numeric(date)), day = yday(date), year = as.character(year(date))) %>%
    ggplot(aes(day, smooth, col = year)) +
    geom_line(lwd = 2)
```
Suppose we want to predict 2s and 7s in the mnist_27 dataset with just the second covariate. Can we do this? On first inspection it appears the data does not have much predictive power. In fact, if we fit a regular logistic regression the coefficient for x_2 is not significant!

This can be seen using this code:
```{r}
library(broom)
library(dslabs)
data(mnist_27)
mnist_27$train %>% glm(y ~ x_2, family = "binomial", data = .) %>% tidy()
```
Plotting a scatterplot here is not useful since y is binary:
```{r}
qplot(x_2, y, data = mnist_27$train)
```
```{r}
## Note that there is indeed predictive power, but that the conditional probability is non-linear.
#The loess line can be plotted using the following code:

mnist_27$train %>% 
	mutate(y = ifelse(y=="7", 1, 0)) %>%
	ggplot(aes(x_2, y)) + 
	geom_smooth(method = "loess")
```

## Key Points:
- The main reason for using matrices is that certain mathematical operations needed to develop efficient code can be performed using techniques from a branch of mathematics called linear algebra.

- Linear algebra and matrix notation are key elements of the language used in academic papers describing machine learning techniques. 
```{r}
library(tidyverse)
library(dslabs)
if(!exists("mnist")) mnist <- read_mnist()

class(mnist$train$images)

x <- mnist$train$images[1:1000,] # to make it a little smaller
y <- mnist$train$labels[1:1000]
```
## Key points: Matrix
In matrix algebra, we have three main types of objects: scalars, vectors, and matrices.

scalar: $\alpha=1$
Vector: $X1$
Matrix: $X=[X_1X_2]$

- In R, we can extract the dimension of a matrix with the function dim().
- We can convert a vector into a matrix using the function as.matrix().
```{r}
length(x[,1])
x_1 <- 1:5
x_2 <- 6:10
cbind(x_1, x_2)
dim(x)
dim(x_1)
dim(as.matrix(x_1))
dim(x)
```

## Key points: Converting a Vector to a Matrix
In R, we can convert a vector into a matrix with the matrix() function. The matrix is filled in by column, but we can fill by row by using the byrow argument. The function t() can be used to directly transpose a matrix. 

Note that the matrix function recycles values in the vector without warning if the product of columns and rows does not match the length of the vector. 

```{r}
my_vector <- 1:15

# fill the matrix by column
mat <- matrix(my_vector, 5, 3)
mat

# fill by row
mat_t <- matrix(my_vector, 3, 5, byrow = TRUE)
mat_t
identical(t(mat), mat_t)
matrix(my_vector, 5, 5)
grid <- matrix(x[3,], 28, 28)
image(1:28, 1:28, grid)

# flip the image back
image(1:28, 1:28, grid[, 28:1])
```
## Key points: Row and Column Summaries and Apply
- The function rowSums() computes the sum of each row.

- The function rowMeans() computes the average of each row.

- We can compute the column sums and averages using the functions colSums() and colMeans().

- The matrixStats package adds functions that performs operations on each row or column very efficiently, including the functions rowSds() and colSds().

- The apply() function lets you apply any function to a matrix. The first argument is the matrix, the second is the dimension (1 for rows, 2 for columns), and the third is the function. 

```{r}
sums <- rowSums(x)
avg <- rowMeans(x)

data_frame(labels = as.factor(y), row_averages = avg) %>%
    qplot(labels, row_averages, data = ., geom = "boxplot")

avgs <- apply(x, 1, mean) #similar to sapply but on matrix
sds <- apply(x, 2, sd)
```

## Key points: Filtering Columns Based on Summaries
- The operations used to extract columns: x[,c(351,352)].

- The operations used to extract rows: x[c(2,3),].

- We can also use logical indexes to determine which columns or rows to keep:  new_x <- x[ ,colSds(x) > 60].

- Important note: if you select only one column or only one row, the result is no longer a matrix but a vector. We can preserve the matrix class by using the argument drop=FALSE. 

```{r}
library(matrixStats)

#some pixels does not chnage very much!
sds <- colSds(x)
qplot(sds, bins = "30", color = I("black"))
image(1:28, 1:28, matrix(sds, 28, 28)[, 28:1])
```
```{r}
#extract columns and rows
x[ ,c(351,352)]
x[c(2,3),]
new_x <- x[ ,colSds(x) > 60]
dim(new_x)
class(x[,1])
dim(x[1,])
```
```{r}
#preserve the matrix class
class(x[ , 1, drop=FALSE])
dim(x[, 1, drop=FALSE])
```
## Key points: Indexing with Matrices and Binarizing the Data
We can use logical operations with matrices:
```{r}
mat <- matrix(1:15, 5, 3)
mat[mat > 6 & mat < 12] <- 0
```

We can also binarize the data using just matrix operations:
```{r}
bin_x <- x
bin_x[bin_x < 255/2] <- 0 
bin_x[bin_x > 255/2] <- 1
```
```{r}
#index with matrices
mat <- matrix(1:15, 5, 3)
as.vector(mat)
qplot(as.vector(x), bins = 30, color = I("black"))
new_x <- x
new_x[new_x < 50] <- 0

mat <- matrix(1:15, 5, 3)
mat[mat < 3] <- 0
mat

mat <- matrix(1:15, 5, 3)
mat[mat > 6 & mat < 12] <- 0
mat
```
```{r}
#binarize the data
bin_x <- x
bin_x[bin_x < 255/2] <- 0
bin_x[bin_x > 255/2] <- 1
bin_X <- (x > 255/2)*1
```

## Key points: Vectorization for Matrices and Matrix Algebra Operations
We can scale each row of a matrix using this line of code:
```{r}
(x - rowMeans(x)) / rowSds(x)
```
To scale each column of a matrix, we use this code:
```{r}
t(t(X) - colMeans(X))
```
We can also use a function called sweep() that works similarly to apply(). It takes each entry of a vector and subtracts it from the corresponding row or column:
```{r}
X_mean_0 <- sweep(x, 2, colMeans(x))
```

Matrix multiplication: t(x) %*% x
The cross product: crossprod(x)
The inverse of a function: solve(crossprod(x))
The QR decomposition: qr(x)

```{r}
#scale each row of a matrix
(x - rowMeans(x)) / rowSds(x)
```
```{r}
#scale each column
t(t(x) - colMeans(x))
```
```{r}
#take each entry of a vector and subtracts it from the corresponding row or column
x_mean_0 <- sweep(x, 2, colMeans(x))
```
```{r}
#divide by the standard deviation
x_mean_0 <- sweep(x, 2, colMeans(x))
x_standardized <- sweep(x_mean_0, 2, colSds(x), FUN = "/")
```

## Exercise:
```{r}
x <- matrix(rnorm(100*10), 100, 10)

#dimension of rows
dim(x)

# number of rows
nrow(x)
length(x[,1])
dim(x)[1]

#number of columns
ncol(x)
length(x[1,])
dim(x)[2]
```
```{r}
# adding the scalar 1 to row 1, the scalar 2 to row 2, and so on, for the matrix x?
x1 <- x + seq(nrow(x))
x2 <- sweep(x, 1, 1:nrow(x),"+")
```
```{r}
#adding the scalar 1 to column 1, the scalar 2 to column 2, and so on, for the matrix x
x <- sweep(x, 2, 1:ncol(x), FUN = "+")
```
```{r}
# average of each row of x and each column of x:
rowMeans(x)
colMeans(x)
```
## Exercise 2:
For each observation in the mnist training data, compute the proportion of pixels that are in the grey area, defined as values between 50 and 205 (but not including 50 and 205). (To visualize this, you can make a boxplot by digit class.)

What proportion of the 60000*784 pixels in the mnist training data are in the grey area overall, defined as values between 50 and 205?

```{r}
# The matrix and plot can be calculated using the following code:
mnist <- read_mnist()
y <- rowMeans(mnist$train$images>50 & mnist$train$images<205)
qplot(as.factor(mnist$train$labels), y, geom = "boxplot")

#the proportion
mean(y)
```

