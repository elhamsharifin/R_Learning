---
title: "Inference and Modelling (learned from edx (Inference_Modeling), note: the note part are exactly the same of edx course)"
author: "Elham Sharifin"
date: "5/4/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# 1: Parameters and Estimates 
## Key points: Introduction to Inference
- statistical inference, the process of deducing characteristics of a population using data from a random sample

- the statistical concepts necessary to define estimates and margins of errors

- how to forecast future results and estimate the precision of our forecast

- how to calculate and interpret confidence intervals and p-values

- Information gathered from a small random sample can be used to infer characteristics of the entire population.

- Opinion polls are useful when asking everyone in the population is impossible.

- A common use for opinion polls is determining voter preferences in political elections for the purposes of forecasting election results.

- The spread of a poll is the estimated difference between support two candidates or options.

## Key points: Sampling Model Parameters and Estimates
- The task of statistical inference is to estimate an unknown population parameter using observed data from a sample.

- In a sampling model, the collection of elements in the urn is called the population.

- A parameter is a number that summarizes data for an entire population.

- A sample is observed data from a subset of the population.

- An estimate is a summary of the observed data about a parameter that we believe is informative. It is a data-driven guess of the population parameter.

- We want to predict the proportion of the blue beads in the urn, the parameter p. The proportion of red beads in the urn is 1−p and the spread is $2p−1=p-(1-p))$.

- The sample proportion is a random variable. Sampling gives random results drawn from the population distribution.

```{r}
library (tidyverse)
library(dslabs)
ds_theme_set()
take_poll(25) #shows a plot of a random sample drawn from an urn with blue and red beads
```
```{r}
take_poll(25)  #it is random and change every time
```
## Key points: the sample average
- Many common data science tasks can be framed as estimating a parameter from a sample. for example: what is the health effect of smoking in a population? what are the differences of racial group of fatal shootings by police? what is the rate of life expectancy in the US duting the last 10 year. (in all the question: we need to estimate the parameetr p from the sample)

- We illustrate statistical inference by walking through the process to estimate p. From the estimate of p, we can easily calculate an estimate of the spread, $2p−1$.

- Consider the random variable X that is 1 if a blue bead is chosen and 0 if a red bead is chosen. The proportion of blue beads in N draws is the average of the draws $X_1,...,X_N$.

- $\bar{X}$,is the sample average. In statistics, a bar on top of a symbol denotes the average. $\bar{X}$ is a random variable because it is the average of random draws - each time we take a sample, $\bar{X}$ is different. $\bar{X}=\frac{X1+X2+...+XN}{N}$

- The number of blue beads drawn in N draws, $N\bar{X}$ is N times the proportion of values in the urn. However, we do not know the true proportion: we are trying to estimate this parameter p.

## Key points: Poling vs. predicting
- A poll taken in advance of an election estimates p for that moment, not for election day.

_ the poll in the last night of election is suppose to be more accurate because it is not very likely that people change their mind in one night!

- In order to predict election results, forecasters try to use early estimates of p to predict p on election day. 

## Key points: Properties of Our Estimate
- When interpreting values of $\bar{X}$, it is important to remember that $\bar{X}$ is a random variable with an expected value and standard error that represents the sample proportion of positive events.

- The expected value of $\bar{X}$ is the parameter of interest p. This follows from the fact that $\bar{X}$ is the sum of independent draws of a random variable times a constant $\frac{1}{N}$. $E(\bar{X})=p$

- As the number of draws N increases, the standard error of our estimate $\bar{X}$ decreases. But how large should N be to make the standard error very small! The standard error of the average of $\bar{X}$ over N draws is: $SE(\bar{X})=\sqrt {p(1−p)/N}$

- In theory, we can get more accurate estimates of p by increasing N. In practice, there are limits on the size of N due to costs, as well as other factors we discuss later.

- We can also use other random variable equations to determine the expected value of the sum of draws $E(S)$ and standard error of the sum of draws $SE(S)$.
$E(S)=Np$
$SE(\bar{X})=\sqrt{\frac{Np(1−p)}{N}}$

### Exercise 1: Plotting se vs. p
```{r}
## compute the se of a sample average that you poll 25 poeple in the population. create a vector of p from 0 (no democrae) to a(all democrate)
N <- 25
p <- seq(0,1,length=100)
se <- sqrt(p*(1-p)/N)

# plotting se vs. p:
plot(p,se)
```
### Example 2:multiple plot of se vs. p:
```{r}
## create three plot of se vs. p for three sample size of N=25, 100, 1000.
p <- seq(0,1,length=100)
for(N in c(25,100,1000)){
  se <- sqrt(p*(1-p)/N)
  plot(p,se)
}

```

### Exercise 3: 
suppose that the difference between selecting democrate from republican is: $dif=\bar{X}-(1-\bar{X)$
$E(\bar{X}-(1-\bar{X}))=2E[\bar{X}]-1$
$se(\bar{X}-(1-\bar{X}))=se[2\bar{X}-1]=se[2\bar{X}]$ #subtracting 1 does not affect se
```{r}
## suppose the actual chance of victory of democrates is 0.45. n this case, the Republicans is winning by a relatively large margin of d=−0.1, or a 10% margin of victory. what is the standar error of spread?
# spread = X¯-(1-X¯)=2X¯-1  
#se(2X¯-1)=2.se(X¯)
p <- 0.45
N <- 25
2*sqrt(p*(1-p)/N)
```
### Exercise 4:
suppose the difference between the proportion of Democrats and Republicans is about $10$ percent and the standard error of this spread is about $0.2$ for $N=25$. Does sample size if sufficient?

The sample size is not enoulgh because the standard error is larger than spread. 

# 2- The Central Limit Theorem in Practice 
## Key points: The Central Limit Theorem in Practice
- the Central Limit Theory: the distribution of sum of draws (X1+X2+...+XN) is approximately normal.

_ we also learnes: when a random variable is devided by a constant nonrandom variable, resulted random variable is strill normal:
If $X\sim N(mu,sigma)$    $\frac{X}{a}\sim N(\frac{\mu}{a},\frac{\sigma}{a})$
that shows Because $\bar{X}$ is the sum of random draws divided by a constant, the distribution of $\bar{X}$ is approximately normal.
$E(\bar{X})=p$, $SE(\bar{X})=\sqrt{\frac{p.(1-p)}{N}}$

explanation of the below formula: We want to know what is the probbaility that we are within 1 percentage point of p? It mean what is the probability $Pr(|\bar{X}-p|≤.01)= Pr(\bar{X}≤ p + 0.01)-Pr(\bar{X}≤ p - 0.01)$, now, if each side subtrct from expected value and devided by standard error:)

- We can convert $\bar{X}$ to a standard normal random variable Z: 
$Z = \frac{\bar{X}-E(\bar{X})}{SE(\bar{X})}$

- The probability that X¯ is within .01 of the actual value of p is:
$Pr(Z≤\frac{0.01}{\sqrt\frac{p(1−p)}{N}})−Pr(Z≤\frac{−0.01}{\sqrt\frac{p(1−p)}{N}})$

- The Central Limit Theorem (CLT) still works if X¯ is used in place of p. This is called a plug-in estimate. Hats over values denote estimates. Therefore:
$S\hat{E(\bar{X})} = \sqrt{\frac{\bar{X}(1-\bar{X})}{N}}$

-Using the CLT, the probability that \bar{X} is within $0.01$ of the actual value of $p$ is:
$Pr(Z≤\frac{0.01}{se[\bar{X}]})-Pr(Z≤\frac{-0.01}{se[\bar{X}]}$

```{r}
## suppose we have 12 blue and 13 green in the burn. 
# what is the probbaility that we are within 1 percentage point of p?
x_hat <- 0.48  #(12/(12+13))
se <- sqrt(x_hat*(1-x_hat)/25)
se
#Pr(Z<=0.01/se)-Pr(Z<=-0.01/se)
pnorm(0.01/se)-pnorm(-0.01/se)
```

## Key points: Margin of errors
- The margin of error is defined as 2 times the standard error of the estimate $\bar{X}$.

- There is about a $95\%$ chance that $\bar{X}$ will be within two standard errors of the actual parameter p.

```{r}
## margin of error of the previous question:
m_e <- 2*se
m_e
pnorm(2)-pnorm(-2)
```

## Key points: A Monte Carlo Simulation for the CLT
- We can run Monte Carlo simulations to compare with theoretical results assuming a value of p.

- In practice, p is unknown. We can corroborate theoretical results by running Monte Carlo simulations with one or several values of p.

- One practical choice for p when modeling is $\bar{X}$, the observed value of $\hat{X}$ in a sample.

```{r}
p <- 0.45    # unknown p to estimate
N <- 1000

## simulate one poll of size N and calculate x_hat
X <- sample(c(0,1),size=N,replace=TRUE,prob=c(1-p,p))
X_hat <- mean(X)
X_hat
```
```{r}
## Monte Carlo simulation using a specified value of p
B <- 10000
X_hat <- replicate(B,{
  X <- sample(c(0,1),size=N,replace=TRUE,prob=c(1-p,p))
  mean(X)
})

mean(X_hat)
sd(X_hat)

# the CLT say: the X_hat has a normal distribution (if E[X_hat]=p,se[X_hat]=sqrt(p*(1-p)/N))
p
sqrt(p*(1-p)/N)
```

```{r}
## Histogram and QQ-plot of Monte Carlo results
library(gridExtra)
p1 <- data.frame(X_hat=X_hat) %>% ggplot(aes(X_hat)) +
  geom_histogram(binwidth=0.005,color="black")

p2 <- data.frame(X_hat=X_hat) %>% ggplot(aes(sample=X_hat)) +
  stat_qq(dparams=list(mean=mean(X_hat),sd=sd(X_hat))) +
  geom_abline() +
  ylab("X_hat") +
  xlab("Theorotical normal")

grid.arrange(p1,p2,nrow=1)
```

## Key points: The Spread
- The spread between two outcomes with probabilities $p$ and $1−p$ is $2p−1$.

- The expected value of the spread is $2\hat{X}−1$.

- The standard error of the spread is $2SE[\hat{X}]$.

- The margin of error of the spread is 2 times the margin of error of $\hat{X}$.

## Key points: Why not run a very large poll:
- An extremely large poll would theoretically be able to predict election results almost perfectly.

- These sample sizes are not practical. In addition to cost concerns, polling doesn't reach everyone in the population (eventual voters) with equal probability, and it also may include data from outside our population (people who will not end up voting).

- These systematic errors in polling are called bias. We will learn more about bias in the future.

```{r}
N <- 100000
p <- seq(0.35,0.65,length=100)
SE <- sapply(p,function(x) 2*sqrt(x*(1-x)/N))
data.frame(p=p,SE=SE) %>% ggplot(aes(p,SE)) +
  geom_line()
```
### Exercise:
Write a function that take proportion of democrate, p, and the sample size of N, and return the sample average of democrate and republican.
```{r}
take_sample <- function(p,N){
  x <- sample(c(1,0),size=N,replace=TRUE,prob=c(p,1-p))
  mean(x)
}
# suppose p, and N are as follow: 
set.seed(1,sample.kind="Rounding")
p <- 0.45
N <- 100
take_sample(p=0.45,N=100)
```
```{r}
## repeat this function for 10,000 times and plot a histogram of errors: 
B <- 10000
p <- 0.45
N <- 100
errors <- replicate(B,{
  x <- take_sample(p,N)
  p - mean(x)
})
mean(errors)
hist(errors)
```
In th reality, we do not access to p (the proportion of democrats)
```{r}
## what is the average size of error if we define the size with the absolute value of (|p-X_bar|)
mean(abs(errors))
```

```{r}
## In reality we use the standard deviation of error instead of average of absolute values. 

## the standard deviation of errors:
sqrt(mean(errors^2)) #errors (p-x_bar)

## teh standard error using CLT:
sqrt(p*(1-p)/N)
```
```{r}
## In the reality, we do not know p. so, we use the estimated X_hat instead of p. SE^(X_bar)
p <- 0.45
N <- 100
set.seed(1,sample.kind="Rounding")
X <- sample(c(1,0),size=N,replace=TRUE,prob=c(p,1-p))
X_bar <- mean(X)

## the standard error of estimate:
sqrt(X_bar*(1-X_bar)/N)

```
```{r}
## Earlier we learned that the largest standard errors occur for p=0.5.

## ploting the largest standard error for N, ranging from 100-5000. how large does the sample size have to be to have a standard error of about 1 percent?

N <- seq(100, 5000, length = 100)
p <- 0.5
se <- sqrt(p*(1-p)/N)
plot(N,se)
```
```{r}
## plotting the qq-plot of errors to see if the errors has a normal distribution or not?
qqnorm(errors);qqline(errors)
```
```{r}
## using CLT, estimate the probability that X_bar>0.5.
p <- 0.45
N <- 100
1-pnorm(0.5,p,(sqrt(p*(1-p)/N)))
```
```{r}
## estimating the probability of a specific error size:
## assuming you do not have p, and you take a sample with size of N=100 and calculate X_bar=0.51. What is the CLT approximation for the probability that your error size is equal or larger than 0.01?
N <- 100
X_bar <- 0.51
se_X_bar <- sqrt(X_bar*(1-X_bar)/N)

## the probability that error is whithin 0.01
pnorm(0.01/se_X_bar)-pnorm(-0.01/se_X_bar)

## the probaility that error is larger than 0.01
1-(pnorm(0.01/se_X_bar)-pnorm(-0.01/se_X_bar))

```

## Key points: Confidence Intervals
- We can use statistical theory to compute the probability that a given interval contains the true parameter p. $Pr(\bar{X}-2\hat{SE}(\bar{X}) \leq p \leq \bar{X}+2\hat{SE}(\bar{X}))$ that is equal with $Pr(-2\leq \frac{\bar{X}-p}{\hat{SE}(X)}\leq2)$ that is equal to $-2\leq Z \leq 2$ (what is the probaility of a standard normal variable being between -2 and 2) which is $95\%$

- $95\%$ confidence intervals are intervals constructed to have a $95\%$ chance of including p. The margin of error is approximately a $95\%$ confidence interval.

- The start and end of these confidence intervals are random variables.

- To calculate any size confidence interval, we need to calculate the value z for which $Pr(−z≤Z≤z)$ equals the desired confidence. For example, a $99%$ confidence interval requires calculating $z$ for $Pr(−z≤Z≤z)=0.99$.

- For a confidence interval of size $q$, we solve for $z=1−\frac{1−q}{2}$.

- To determine a $95\%$ confidence interval, use $z <-  qnorm(0.975)$. This value is slightly smaller than $2$ times the standard error.

```{r}
data("nhtemp")
data.frame(year=as.numeric(time(nhtemp)),temperature=as.numeric(nhtemp)) %>% ggplot(aes(year,temperature)) +
  geom_point()+
  geom_smooth()+
  ggtitle("Average yealy temperature in New Haven")
```
```{r}
p <- 0.45
N <- 1000
## Calculating the probability that the interval includes p:
X <- sample(c(1,0),size=N,replace=TRUE,prob=c(p,1-p)) #N observations
X_hat <- mean(X)
SE_hat <- sqrt(X_hat*(1-X_hat)/N)
c(X_hat-2*SE_hat , X_hat+2*SE_hat) #building intervals of 2SE below and above mean

# it is a random and will change every time!
```
```{r}
##  Solving for z with qnorm

z <- qnorm(0.995) # calculate z to solve for 99% conf. interval
z

pnorm(qnorm(0.995))    # showing that qnorm gives the z value for a given probability

pnorm(qnorm(1-0.995))    # showing symmetry of 1-qnorm

pnorm(z) - pnorm(-z)    # showing that this z value gives correct probability for interval
```

## Key points: A Monte Carlo Simulation for Confidence Intervals
- We can run a Monte Carlo simulation to confirm that a $95\%$ confidence interval contains the true value of p $95\%$ of the time.

- A plot of confidence intervals from this simulation demonstrates that most intervals include p, but roughly $5\%$ of intervals miss the true value of p.

```{r}
B <- 10000
p <- 0.45
N <- 1000

inside <- replicate(B,{
  X <- sample(c(1,0),size=N,replace=TRUE,prob=c(p,1-p))
  X_hat <- mean(X)
  SE_hat <- sqrt(X_hat*(1-X_hat)/N)
  between(p,X_hat-2*SE_hat,X_hat+2*SE_hat) #is p included in this interval? (True of p in confidence interval)
})
mean(inside)
```

## Key points: The Correct Language
- The $95\%$ confidence intervals are random, but p is not random.

- $95\%$ refers to the probability that the random interval falls on top of p.

- It is technically incorrect to state that p has a $95\%$ chance of being in between two values because that implies p is random.

## Key points: Power
- If we are trying to predict the result of an election, then a confidence interval that includes a spread of $0$ (a tie) is not helpful.

- A confidence interval that includes a spread of $0$ does not imply a close election, it means the sample size is too small.

- Power is the probability of detecting an effect when there is a true effect to find. Power increases as sample size increases, because larger sample size means smaller standard error.

```{r}
## Confidence interval for the spread with sample size of 25
N <- 25
X_hat <- 0.48
(2*X_hat-1) + c(-2,2)*2*sqrt(X_hat*(1-X_hat)/N)

##Note:  here we used 2. However, for exact 0.95 of confidence interval, we have to use c(-qnorm(.975), qnorm(.975)) instead of 1.96.
(2*X_hat-1) + c(-qnorm(.975), qnorm(.975))*2*sqrt(X_hat*(1-X_hat)/N)
```

## Key points: p-values
- The null hypothesis is the hypothesis that there is no effect. In this case, the null hypothesis is that the spread is $0$, or 
$p=0.5$.

- $Pr(|\bar{X}-0.5|>0.02)$ with adding Expected value and deviding by standard error: We now the variable $\sqrt{N}\frac{\bar{X}-0.5}{\sqrt{0.5(1-0.5)}}$ is standard normal: 
$Pr(|\bar{X}-0.5|>0.02)=Pr(\sqrt{N}\frac{\bar{X}-0.5}{\sqrt{0.5(1-0.5)}}>\sqrt{N}\frac{0.02}{\sqrt{0.5(1-0.5)}})$

simplify: the p-value is equal to:
$Pr(\sqrt{N}\frac{|\bar{X}-0.5|}{0.5}>Z)$

- The p-value is the probability of detecting an effect of a certain size or larger when the null hypothesis is true.

- We can convert the probability of seeing an observed value under the null hypothesis into a standard normal random variable. We compute the value of $z$ that corresponds to the observed result, and then use that $z$ to compute the p-value.

- relation between p-value and confidence interval: 
If a $95\%$ confidence interval does not include our observed value, then the p-value must be smaller than $0.05$.

- It is preferable to report confidence intervals instead of p-values, as confidence intervals give information about the size of the estimate and p-values do not.

```{r}
## are there more blue beads or red beads? means: Is 2p-1>0?
N <- 100
p <- 0.5 #assumption (2p-1=0)
z <- sqrt(N)*(0.02/0.5) #according to the above formula
1-(pnorm(z)-pnorm(-z)) 
```

## another explanation of p-value:
- The p-value is the probability of observing a value as extreme or more extreme than the result given that the null hypothesis is true.

- In the context of the normal distribution, this refers to the probability of observing a Z-score whose absolute value is as high or higher than the Z-score of interest.

- Suppose we want to find the p-value of an observation 2 standard deviations larger than the mean. This means we are looking for anything with $|Z\geq2|$. 

- Reminder: $pnorm()$ gives the CDF for a normal distribution with a mean of $\mu=0$ and standard deviation of $\sigma=1$. 

- To find p-values for a given z-score $z$ in a normal distribution with mean $\mu$ and standard deviation $\sigma$, use $2*(1-pnorm(z, \mu, \sigma))$ instead.

### Exercise: Actual data of election 2016
1- suppose there are only two candidates and construct a 95% confidence interval for the election night proportion p.
```{r}
## calculating lowe and upper confidence intervals:
library(dslabs)
data("polls_us_election_2016")
poll <- polls_us_election_2016 %>%
  filter(enddate>="2016-10-31"&state=="U.S.") %>%
  filter() %>% 
  mutate(N=samplesize,X_hat=rawpoll_clinton/100) %>%
  mutate(se_hat=sqrt(X_hat*(1-X_hat)/N)) %>%
  mutate(lower_ci =X_hat-qnorm(0.975)*se_hat,upper_ci= X_hat+qnorm(0.975)*se_hat) 
  
pollster_result <- poll %>% select(pollster,enddate,X_hat,se_hat,lower_ci,upper_ci)
```
2- the real result of election was:Clinton: 48.2% and Trump 46.1%. What proportion of confidence intervals contained $p$?
```{r}
## 
p <- 0.482  #48.2%
avg_hit <- pollster_result %>% mutate(hit=lower_ci<=p&p<=upper_ci) %>% summarize(mean(hit))
avg_hit
```
3- confidence intrevals for spread:
As seen, smaller portion of polls than expected contains p. and most polls that does not include p, were undestimating. It can be say that the undecide voters historically devided evenly between the two main candidates on the elction day. 
In this case, it is more imformative to calculate spread. suppose ther are only two parties in this election. calculate the confidence intervals for the night of election?
```{r}
## calculating lowe and upper confidence intervals of spread:
polls <- polls_us_election_2016 %>% 
  filter(enddate >= "2016-10-31" & state == "U.S.") %>%
  mutate(d_hat=(rawpoll_clinton-rawpoll_trump)/100,X_hat=(d_hat+1)/2,se_hat= 2*sqrt(X_hat*(1-X_hat)/N),lower_ci=d_hat-qnorm(0.975)*se_hat,upper_ci=d_hat+qnorm(0.975)*se_hat) 

pollster_results2 <- polls %>% select(pollster, enddate, d_hat, lower_ci, upper_ci)
## Note:Because we want to calculate the spread assuming there are only 2 candidates, we can't use the raw poll values for calculation of X_hat similar to the previous example and must calculate them from d_hat=(X_hat+1)/2
```
4- real results of election: Clinton: 48.2% and Trump 46.1%.
```{r}
## calculating if real spread is in the confidence intreval or not?
spread<-0.482-0.461
hit_avg <- pollster_results2 %>% mutate(hit=lower_ci<=spread & spread<=upper_ci) %>%
  summarize(mean(hit))
hit_avg
```
5- plotting the error
```{r}
pollster_results2 %>% mutate(error=d_hat-spread) %>%
  ggplot(aes(0.021,error)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```
6- plotting error vs. pollsters that took five or more polls:
```{r}
pollster_results2 %>% mutate(errors=d_hat-0.021) %>% 
group_by(pollster) %>%
filter(n()>=5) %>%
ggplot(aes(errors,pollster)) +
geom_point() +
    theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

# Stattistical Models:
## keypoint: Poll Aggregators
- Poll aggregators combine the results of many polls to simulate polls with a large sample size and therefore generate more precise estimates than individual polls.

- Polls can be simulated with a Monte Carlo simulation and used to construct an estimate of the spread and confidence intervals.

- The actual data science exercise of forecasting elections involves more complex statistical modeling, but these underlying ideas still apply.

### Example: (Election 2012)
In the elction 2012, Barak Obama won the election by a margin of 3.9%. One of the statistians was very sure about the winning of Obama. How he was so sure?
```{r}
## we want to create the confidence intervals for the real outcome:
## Note that to calculate the exact 95% confidence interval, we should use qnorm(.975)*SE_hat instead of 2*SE_hat.

d <- 0.039  #difference or spread p-(1-p)
Ns <- c(1298, 533, 1342, 897, 774, 254, 812, 324, 1291, 1056, 2172, 516)  #12 polls taken a week before the election
p <- (d+1)/2  #prportion of Obama

confidence_interval <- sapply(Ns,function(N){
  X <- sample(c(1,0),size=N,replace=TRUE,prob=c(p,1-p))
  X_hat <- mean(X)
  se_hat <- sqrt(X_hat*(1-X_hat)/N)
  2*c(X_hat,X_hat-2*se_hat,X_hat+2*se_hat)-1
})
```
```{r}
## generating a data frame storing results
polls <- data.frame(poll=1:ncol(confidence_interval),
                    t(confidence_interval),sample_size=Ns) #t: a matrx of data frame
names(polls) <- c("poll", "estimate", "low", "high", "sample_size")
polls
```
Plotting: The eroor bar of the estimates of spread
```{r}
plot_obama <- polls %>% 
  ggplot(aes(estimate,poll)) + 
  geom_point() +
  geom_errorbar(aes(xmin=low,xmax=high),width=0.5,color="blue") +
  geom_vline(xintercept = 0) +
   geom_vline(xintercept = 0.039,linetype = "dashed")

plot_obama
```
As seen in the above figure, all of the intervals include the real result (0.039). It is obvious because they are 95% confidence intervals. Howevr, all of the intervals include zero!
we can create smallesr 95% confidence intervals and more precision prediction: we do not have access to the raw data of polls but mathematically, we can make a large data:
```{r}
### constructing a larger sample size:
sum(polls$sample_size)

## calculating estimate of d:
d_hat_avg <- polls %>%
  summarize(avg=sum(estimate*sample_size)/sum(sample_size)) %>%
  .$avg
# sum(estimate*sample_size)=total spread
# sum(sample_size)=total number of participants in the poll
d_hat_avg

## constructing an estimate for the proportion of voting to Obama, that then can be used for estimating standard error of spread
p_hat_avg <- (1+d_hat_avg)/2
moe_avg <- 2*1.96*sqrt(p_hat_avg*(1-p_hat_avg)/sum(polls$sample_size))   
moe_avg #margin of error
round(d_hat_avg*100,1)  #percentage of spread (round only one digit)
round(moe_avg*100, 1)

## we found spread of 3.7% and the margin of error of 1.8 that it shows the resulta are farther than to be included zero. 
```
```{r}
## adding the plot of new interval to the previous plot. 
plot_obama + geom_errorbar(aes(xmin=d_hat_avg-moe_avg,xmax=d_hat_avg+moe_avg,y=13),color="red") + geom_point(x=d_hat_avg,y=13)
```

## Key points: Pollsters and Multilevel Models
- Different poll aggregators generate different models of election results from the same poll data. This is because they use different statistical models.

- We will use actual polling data about the popular vote from the 2016 US presidential election to learn the principles of statistical modeling.

## Key points: Poll Data and Pollster Bias
- We analyze real 2016 US polling data organized by FiveThirtyEight. We start by using reliable national polls taken within the week before the election to generate an urn model.

- Consider p the proportion voting for Clinton and $1−p$ the proportion voting for Trump. We are interested in the spread $d=2p−1$.

- Poll results are a random normal variable with expected value of the spread $d$ and standard error $\sqrt{\frac{2p(1−p)}{N}}$.

- Our initial estimate of the spread did not include the actual spread. Part of the reason is that different pollsters have different numbers of polls in our dataset, and each pollster has a bias.

- Pollster bias reflects the fact that repeated polls by a given pollster have an expected value different from the actual spread and different from other pollsters. Each pollster has a different bias.

- The urn model does not account for pollster bias. We will develop a more flexible data-driven model that can account for effects like bias.

```{r}
## accessing to data
data("polls_us_election_2016")
names(polls_us_election_2016)

# reliable polls in the U.S. during the last week before election
polls <- polls_us_election_2016 %>%
  filter(state=="U.S." & enddate>="2016-10-31" & (grade%in%c("A","A+","A-","B+") | is.na(grade)))

# addining the proportion of spread to the polls
polls <- polls %>% mutate(spread=(rawpoll_clinton-rawpoll_trump)/100)
```
Because there are different pollsters and there are different spreads, we use an aggregate poll.
```{r}
## calculating d_hat of average:
d_hat <- polls %>%
  summarize(d_hat=sum(spread*samplesize)/sum(samplesize)) %>%
  .$d_hat
d_hat

p_hat <- (d_hat+1)/2
p_hat

moe <- 2*1.96*sqrt(p_hat*(1-p_hat)/sum(polls$samplesize))
moe

# for reporting this spread: we have a spread of 1.4% with a margin of error of 0.66%. 
```
In the elction night, it is undestood that the actual spread is 2.1%. our prediction wan sont a good one! but why?
```{r}
polls %>%
  ggplot(aes(spread)) +
  geom_histogram(color="black",binwidth = 0.01)
```
The histogram shows that the data does not have a normal distribution. what is the reason?

### Investigating poll data and pollster bias:
The polls table shows that several pollsters used several polls during the last week of voting:
```{r}
# number of polls per pollster in week before election
polls %>% group_by(pollster) %>% summarize(n())
```

```{r}
## plotting the spread vs. pollsters which has more than 6 polls:
polls %>% group_by(pollster) %>%
  filter(n()>=6) %>%
  ggplot(aes(pollster,spread)) +
  geom_point() +
  theme(axis.text.x=element_text(angle=90,hjust = 1))
```
```{r}
polls %>% group_by(pollster) %>%
    filter(n() >= 6) %>%
    summarize(se = 2 * sqrt(p_hat * (1-p_hat) / median(samplesize)))
```
```{r}
# standard errors within each pollster
polls %>% group_by(pollster) %>%
  filter(n()>=6) %>%
  summarize(se=2*sqrt(p_hat*(1-p_hat)/median(samplesize)))
```
## Key points: Data-Driven Models
- Instead of using an urn model where each poll is a random draw from the same distribution of voters, we instead define a model using an urn that contains poll results from all possible pollsters.

- We assume the expected value of this model is the actual spread 
$d=2p−1$.

- Our new standard error $\sigma$ now factors in pollster-to-pollster variability. It can no longer be calculated from $p$ or $d$ and is an unknown parameter.

- The central limit theorem still works to estimate the sample average of many polls $X_1,...,X_n$, because the average of the sum of many random variables is a normally distributed random variable with expected value $d$ and standard error $\sigma/\sqrt{N}$.

- We can estimate the unobserved $\sigma$ as the sample standard deviation, which is calculated with the sd function.

- Note that to compute the exact $95\%$ confidence interval, we would use $qnorm(.975)$ instead of $1.96$.

```{r}
## last result before the election for each pollsters
one_poll_per_pollsters <- polls %>% 
  group_by(pollster) %>%
  filter(enddate==max(enddate)) %>%
  ungroup()

## histogram of spread estimates:
one_poll_per_pollsters %>% ggplot(aes(spread)) +
  geom_histogram(binwidth=0.01)
```
In this data, each pollster reports a differnt estimate:
we assumed the expected value is $d=2p-1$. 
- in this case, because our urn rather than 0 and 1, contains numbers between -1 to 1, the standard deviation of urn is not equal to $\sqrt{\frac{p(1-p)}{N}$. 
- standard error now includes pollster to pollster variability. Thie standard deviation in unknown. So, there are two unknown parameters of standard deviation of $\sigma$ and expected value of $d$. 
```{r}
## the standard deviation of new poll
sd(one_poll_per_pollsters$spread)
```
```{r}
## calculating of expected 95% confidence intervals of new data:
results <- one_poll_per_pollsters %>%
  summarize(avg=mean(spread),se=sd(spread)/sqrt(length(spread))) %>%
  mutate(start=avg-1.96*se,end=avg+1.96*se)
round(results*100,1)
```
Our new confidence interval :
- is wider
- now it incorparates the pollster variability
- it includes the lection night result of 2.1%
- is is small enough not to include 0.

### Exercise: height revisited (Central Limit Theory)
Suppose x is the height of men in the dataset (x is the population). Using the urn analogy, we have an urn with the values of x in it. 
1- mean and standard deviation of population
```{r}
## the population average (mu) and standard deviation (sigma) of population:
library(dslabs)
data(heights)

## all males in the population
x <- heights %>% filter(sex=="Male") %>% .$height

## the population average,and standard deviation of population:
mu <- mean(x)
mu
sigma <- sd(x)
sigma
```
2- estimation of mu and sigma using sampling
```{r}
## making a sample of size 50 and estimating average and standr deviation of sampel:
head(x)
N <- 50
set.seed(1, sample.kind="Rounding")

X <- sample(x,N,replace=TRUE)
mean(X)
sd(X)
```
3- result: based on CLT, the sample average is realted to $\mu$ (population average) that it is a random variable with the expected value of $\mu$ and the standad error $\frac{\sigma}{\sqrt{N}}$.

4- confidence interval calculation for population average:
if we use $\bar{X}$ as our estimate of heights of the population from our sample size with N. 
standard error estimate of our error ($\bar{X}-\mu$) is $\frac{\sigma}{N}$.
```{r}
## constructing the 95% confidence interval for population average (mu).
head(x)
N <- 50
set.seed(1, sample.kind="Rounding")

X <- sample(x,N,replace=TRUE)
X_bar <- mean(X)

se <- sd(X)/sqrt(N)
c(X_bar-qnorm(0.975)*se,X_bar+qnorm(0.975)*se)
```

### Exercise: height revisited (Montocarlo simulation)
run a monto carlo simulation with 10,000 run and find the 95% confidence interval of these runs. what proportion of these intervals include $\mu$? (sample size of 50)
```{r}
library(dslabs)
data(heights)

## all males in the population
x <- heights %>% filter(sex=="Male") %>% .$height

## Montocarlo simulation:
B <- 10000
N <- 50
res <- replicate(B,{
  X <- sample(x,size=N,replace=TRUE)
  X_bar <- mean(X)
  se_bar <- sd(X)/sqrt(N)
  c(X_bar-qnorm(0.975)*se_bar,X_bar+qnorm(0.975)*se_bar)
  between(mean(x),X_bar-qnorm(0.975)*se_bar,X_bar+qnorm(0.975)*se_bar)
})
## the proportion of intervals that inlcude mu.
mean(res)
```

### Exercise: Visualization of Polling Bias:
we are investigating two pollsters that conducted daily polls and looking at national polls for the month before the election.
do you see any poll bias? Plot the spreads for each poll.
```{r}
## loading the data
library(dslabs)
library(dplyr)
library(ggplot2)
data("polls_us_election_2016")

polls <- polls_us_election_2016 %>%
  filter(state=="U.S." &
           enddate>="2016-10-15"& 
           pollster%in%c("Rasmussen Reports/Pulse Opinion Research","The Times-Picayune/Lucid")) %>%
  mutate(spread=rawpoll_clinton/100-rawpoll_trump/100)

## Box plot of spread for two pollsters:
polls %>% 
  ggplot(aes(pollster,spread)) +
  geom_boxplot() +
  geom_point() +
  theme(axis.text.x = element_text(angle = 0, hjust = 0.5))
```
### definining the pollster Bias: 
- Under the urn model, these two pollsters should have the same expected value: the election day difference, $d$. 
if we model the observed data $Y_{i,j}$ where:

$Y_{i,j}=d+b_{i,j}+\epsilon_{i,j}$ ($i=1,2$, for two pollsters)

in this formula: $b_i$ is the bias for pollster $i$, and $\epsilon_{i,j}$ is the poll to poll chance variability. 
Assuming that the $\epsilon_{i,j}$ are independent from each other having the expected value of 0 and standar deviation of $\sigma$ regardless of $j$.  

Important:
_ our data fit the urn model if we know: if $b_1$ is equall to $b_2$.

- in the right side of the above equation, $\epsilon_{i,j}$ is a random variable and $d$, and $b_i$ are constants. The expected value of $Y_{1,j}$ is equal to $d+b_1$

### Expected Value and Standard Error of Poll 1:
if $\bar{Y_1}$ and $\sigma_1$ be the average and standard deviation of first poll:
the expected value and standard error of $\bar{Y_1}$ are:
- $E[\bar{Y_1}]=d+b_1$, $SE[\bar{Y_1}]=\frac{\sigma_1}{\sqrt{N_1}}$

### Expected Value and Standard Error of Poll 2:
if $\bar{Y_2}$ and $\sigma_2$ be the average and standard deviation of second poll:
the expected value and standard error of $\bar{Y_2}$ are:
- $E[\bar{Y_2}]=d+b_2$, $SE[\bar{Y_2}]=\frac{\sigma_2}{\sqrt{N_2}}$

### the Expected value of differnce of avg of polls 1,poll2:
$E[\bar{Y_2}-\bar{Y_1}]=b_2-b_1$
$SE[\bar{Y_2}-\bar{Y_1}]=\sqrt{(\sigma_2^2/N_2+\sigma_1^2/N_1)}$
the standard error is equal to the sigma divided by the square root of N.

### calculating the estimates of sigma
calculate the estimates for $\sigma_1$ and $\sigma_2$:

```{r}
polls <- polls_us_election_2016 %>%
  filter(state=="U.S." &
           enddate>="2016-10-15"& 
           pollster%in%c("Rasmussen Reports/Pulse Opinion Research","The Times-Picayune/Lucid")) %>%
  mutate(spread=rawpoll_clinton/100-rawpoll_trump/100)

## calculating sigma1 and sigma2 of spread:
sigma <- polls %>% group_by(pollster) %>%
summarize(s=sd(spread))
sigma
```
### Probability Distribution of the Spread based on CLT:
Based on Central Limit Theory, if $N_1$ and $N_2$ be large enough and $\bar{Y_1}$ and $\bar{Y_2}$ and their differnce $\bar{Y_2}-\bar{Y_1}$ are approximately Normal.

### calculation of the 95% Confidence Interval of the Spreads:
```{r}
## data of polls:
polls <- polls_us_election_2016 %>%
  filter(state=="U.S." &
           enddate>="2016-10-15"& 
           pollster%in%c("Rasmussen Reports/Pulse Opinion Research","The Times-Picayune/Lucid")) %>%
  mutate(spread=rawpoll_clinton/100-rawpoll_trump/100)

## summarizing the statistical vlaues of two polls:
res <- polls %>% group_by(pollster) %>% 
summarize(avg=mean(spread),sd=sd(spread),N=n())

## calculation of avg of (Y_hat2-Y_hat1)
estimate <- (res$avg[2]-res$avg[1])
estimate

## calculation of se of (Y_hat2-Y_hat1) by above formula
se_hat <- sqrt((res$sd[2]^2/res$N[2])+(res$sd[1]^2/res$N[1]))
se_hat

## calculation of 95%confidence intervals of (Y_hat2-Y_hat1)
ci <- c(estimate-qnorm(0.975)*se_hat,estimate+qnorm(0.975)*se_hat)
ci  #it does not inlcude zero!

## calculation of p-values: (the probability that a random value is larger than the observed ratio of the estimate to the standard error)
(1-pnorm(estimate/se_hat))
```
Result: The confidence interval tells us there is a relatively strong pollster effect in adiffrennce of 5% which cannot be explained by random variability. computing p-value showes us that the chance does not explain the observed pollster effect.

### Comparing Within-Poll and Between-Poll Variability:
t_statistics define as:
$\frac{\bar{Y_2}-\bar{Y_1}}{\sqrt{\frac{s_2^2}{N_2}+\frac{s_1^2}{N_1}}}$
our goal is comparing the variability across polls to variability within polls and thats why we only selected two polls. The area of statistics in this regard is named Analysis of Variance or ANOVA. 
It is helpful for answering the question such as "is there a pollster effect?"

calculate the average and standard deviation for each pollster and examine the variability across the averages and how it compares to the variability within the pollsters, summarized by the standard deviation.

```{r}
polls <- polls_us_election_2016 %>%
  filter(state=="U.S." & enddate>="2016-10-15") %>%
  group_by(pollster) %>%
  filter(n()>=5) %>%
  mutate(spread=rawpoll_clinton/100-rawpoll_trump/100) %>%
  ungroup()
## calculating the mean and sd of spreads for each groups of polls
var <- polls %>% group_by(pollster) %>%
summarize(avg=mean(spread),s=sd(spread)) 
var
```

## Key points: Bayesian Statistics
- In the urn model, it does not make sense to talk about the probability of $p$ being greater than a certain value because $p$
is a fixed value. With Bayesian statistics, we assume that $p$
is in fact random, which allows us to calculate probabilities related to p.

- Hierarchical models describe variability at different levels and incorporate all these levels into a model for estimating $p$.

## Key points: Bayes' Theorem:
- Bayes' Theorem states that the probability of event A happening given event B is equal to the probability of both A and B divided by the probability of event B:
$Pr(A|B) <- \frac{Pr(B|A)Pr(A)}{Pr(B)}$

- Bayes' Theorem shows that a test for a very rare disease will have a high percentage of false positives even if the accuracy of the test is high.

## Equations: Cystic fibrosis test probabilities
- In these probabilities, $+$ represents a positive test, $-$ represents a negative test, $D=0$ indicates no disease, and 
$D=1$ indicates the disease is present.

- Probability of having the disease given a positive test: 
$Pr(D=1∣+)$

- $99\%$ test accuracy when disease is present: $Pr(+∣D=1)=0.99$

- $99\%$ test accuracy when disease is absent: $Pr(−∣D=0)=0.99$

- Rate of cystic fibrosis: $Pr(D=1)=0.00025$.

- Bayes' theorem can be applied like this:

$Pr(D=1|+)=\frac{Pr(+|D=1).Pr(D=1)}{Pr(+)}$
$Pr(D=1|+)=\frac{Pr(+|D=1).Pr(D=1)}{Pr(+|D=1)Pr(D=1)+Pr(+|D=0).Pr(D=0)}$

Substituting known values, we obtain:
$\frac{(0.99)(0.00025)}{(0.99)(0.00025)+(0.01)(0.99975)}=0.02$

```{r}
## Monto Carlo Simulation:
#step1: we have a sample in which some of them we can find the probaility of them are sick or healthy!
#step2: from wach group, what is the probability that the rest result works correcitly or not.

prev <- 0.00025 #the disease has 1/3900people prevelance
N <- 100000
outcome <- sample(c("Disease","Healthy"),N,replace=TRUE,prob=c(prev,1-prev))

## number of poeple with disease:
N_D <- sum(outcome=="Disease")
N_D

## Number of healthy people
N_H <- sum(outcome=="Healthy")
N_H

##
accuracy <- 0.99
test <- vector("character",N)

test[outcome=="Disease"] <- sample(c("+","-"),N_D,replace=TRUE,prob=c(accuracy,1-accuracy))

test[outcome=="Healthy"] <- sample(c("-","+"),N_H,replace=TRUE,prob=c(accuracy,1-accuracy))

table(outcome,test)

#Disease(+)/(Disease(+)+Healthy(+))=0.02
```
## Key points:Bayes in Practice
- The techniques we have used up until now are referred to as frequentist statistics as they consider only the frequency of outcomes in a dataset and do not include any outside information. Frequentist statistics allow us to compute confidence intervals and p-values.

- Frequentist statistics can have problems when sample sizes are small and when the data are extreme compared to historical results.

- Bayesian statistics allows prior knowledge to modify observed results, which alters our conclusions about event probabilities.

## Key points:The Hierarchical Model
- Hierarchical models use multiple levels of variability to model results. They are hierarchical because values in the lower levels of the model are computed using values from higher levels of the model.

- We model baseball player batting average using a hierarchical model with two levels of variability:

---- $p\sim N(\mu,τ)$ describes player-to-player variability in natural ability to hit, which has a mean $μ$ and standard deviation $τ$.
---- $Y|p \sim N(p ,\sigma)$ describes a player's observed batting average given their ability $p$, which has a mean $p$ and standard deviation $\sigma=\sqrt{p(1-p)}$. This represents variability due to luck.

---- In Bayesian hierarchical models, the first level is called the prior distribution and the second level is called the sampling distribution.

- The posterior distribution allows us to compute the probability distribution of $p$ given that we have observed data $Y$.

- By the continuous version of Bayes' rule, the expected value of the posterior distribution $p$ given $Y=y$ is a weighted average between the prior mean $\mu$ and the observed data $Y$:
$E(p|y)=B\mu+(1-B)Y$  where  $B=\frac{\sigma^2}{\sigma^2+τ^2}$
Note: (Y is avg of observed)

- The standard error of the posterior distribution $SE(p|Y)^2$ is $\frac{1}{\frac{1}{\sigma^2}+\frac{1}{τ^2}}$ . Note that you will need to take the square root of both sides to solve for the standard error.

-This Bayesian approach is also known as shrinking. When $\sigma$
is large, $B$ is close to $1$ and our prediction of $p$ shrinks towards the mean $(\mu)$.
When $\sigma$ is small, $B$ is close to $0$ and our prediction of $p$ is more weighted towards the observed data $Y$.

### Exercise: Statistics in the Courtroom
in 1999 Sharkey wa found guilty because of killing two of his kids. Both kids were found dead in 1996, and 1998. She claimed the reason for their kid's death was SIDS. Prof Meadow testifies that the chance of twi infanct dying due to SIDS is 1/73 miliion becuase: rate of SIDS=1/8500 as the result: 1/8500*1/8500=1/73 million. However his calculation is not correct because:
- Mr. Meadow assumed the second death was independent of the first son being affected, thereby ignoring possible genetic causes.

- suppose that there is a genetic component to SIDS and the 
Pr(second case of SIDS|first case of SIDS)=1/100. 

1- The probaility that both of her kids die from SIDS?
```{r}
Pr_1 <- 1/8500
Pr_2 <- 1/100 #Pr(2|1)
Pr_12 <- Pr_1*Pr_2 #Pr(2|1)=Pr(1,2)/Pr(1)
Pr_12
```
2- Based on Basian:
Pr(mother is a murderer | two kids found dead with no evidence of harm)=
Pr(2 kids dead|mother murder)*Pr(mother murder)/Pr(2 kids dead)

2- 
assume: 
- Pr(2 kids dead|mother murder)=0.5
- murder rate among mothers = 1 in 1,000,000
what is Pr(mother is a murderer∣two children found dead with no evidence of harm)?
```{r}
Pr_kid2dead_if_mothermurder <- 0.5
Pr_1 <- 1/8500 #Prob that first kid dead due to SIDS
Pr_2 <- 1/100 #Prob that the second kid die due to SIDS
Pr_kid2dead <- Pr_1*Pr_2 #Prob both kids die due to SIDS
Pr_mothermurder <- 1/1000000  
Pr_mothermurder_if_kid2dead <- Pr_kid2dead_if_mothermurder*Pr_mothermurder/(Pr_kid2dead)
Pr_mothermurder_if_kid2dead
```

### Exercise 2: Election Polls
Based on the past elections, Florida is very important state in election. create a table with the poll spread results for florida done during the last days before the election.
CLT tells that the average of spreads has normal distibution. Calculate the average of spread and calcuate the estimate of standard error?

1- Distribution of avg spread based on the results of different pollsters:
```{r}
library(dplyr)
library(dslabs)
data(polls_us_election_2016)
polls <- polls_us_election_2016 %>%
  filter(enddate>="2016-11-04",state=="Florida") %>%
  mutate(spread=rawpoll_clinton/100-rawpoll_trump/100)

results <- polls %>% summarize(avg=mean(spread),se=sd(spread/sqrt(n())))
results
# in fact the average of spreads has a normal distribution with the above mean and ad. Note that in thsi example to calculate se, we devide by sqrt(N) because we are considering all the pollsters as one and we did not grouped them by pollster to hust use sd(). If we had only 2 people(clinton and trump), we could use se=sqrt(avg(1-avg)/N)
```
2- Estimation of the Prior and Posterior Distribution:
```{r}
## from previous exercise we found the avg and se of observed 
Y <- results$avg
sigma <- results$se

## assume that for the prior distribution for florida, we have:
mu <- 0  #the result of democ and republ always very close
tau <- 0.01 

## calculating the posterir distribution:
B <- sigma^2/(sigma^2+tau^2)
Ex_value <- B*mu + (1-B)*Y
Ex_value
SE <- sqrt(1/((1/sigma^2)+(1/tau^2)))  #Be careful about sqrt
SE
## calculate the credible intervals:
c(Ex_value-qnorm(0.975)*SE,Ex_value+qnorm(0.975)*SE)

```
3- calculation of winning trump and clinton:
it means that spread become negative
spread(clinton-trump) 
```{r}
pnorm(0,Ex_value,SE)
```
4- results when we change the prior dostibution:
We put tau=0.01 supposing that these races are very close.

Change the prior variance containing values from 0.005 to 0.05 and observe how the probability of Trump winning Florida changes by making a plot.
```{r}
## observed based on pollsters
Y <- results$avg
sigma <- results$se

mu <- 0
tau <- seq(0.005,0.05,length=100)

Trump_win <- sapply(tau,function(tau){
B <- sigma^2/(sigma^2+tau^2)
Ex_value <- B*mu + (1-B)*Y
SE <- sqrt(1/((1/sigma^2)+(1/tau^2)))  
pnorm(0,Ex_value,SE)
})

plot(tau,Trump_win)
```

## Key points: Election Forecasting
In our model: 
- The spread $d\sim N(μ,τ)$ describes our best guess in the absence of polling data. We set $μ=0$ and $τ=0.035$ using historical data.
- The average of observed data $\bar{X}∣d\sim N(d,σ)$ describes randomness due to sampling and the pollster effect.

Because the posterior distribution is normal, we can report a $95\%$ credible interval that has a $95\%$ chance of overlapping the parameter using $E(p∣Y)$ and $SE(p∣Y$).

Given an estimate of $E(p∣Y)$ and $SE(p∣Y)$, we can use pnorm to compute the probability that $d>0$.

It is common to see a general bias that affects all pollsters in the same way. This bias cannot be predicted or measured before the election. We will include a term in later models to account for this variability.

```{r}
## creating poll based on the best pollsters
library(tidyverse)
library(dslabs)
polls <- polls_us_election_2016 %>%
    filter(state == "U.S." & enddate >= "2016-10-31" &
                 (grade %in% c("A+", "A", "A-", "B+") | is.na(grade))) %>%
    mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)

## grouping pollsters and filtering only last poll of pollsters
one_poll_per_pollster <- polls %>% group_by(pollster) %>%
    filter(enddate == max(enddate)) %>%
    ungroup()

results <- one_poll_per_pollster %>%
    summarize(avg = mean(spread), se = sd(spread)/sqrt(length(spread))) %>%
    mutate(start = avg - 1.96*se, end = avg + 1.96*se)

## historical data (our guess before any pollsters)
mu <- 0
tau <- 0.035

## distribution of our pollster data
Y <- results$avg
sigma <- results$se

## distribution of posterior 
B <- sigma^2/(sigma^2+tau^2)
posterior_mean <- mu*B+(1-B)*Y
posterior_se <- sqrt(1/((1/sigma^2)+(1/tau^2))) # sqrt()

posterior_mean
posterior_se

## calculating credible intervals:
posterior_mean + c(-1.96,1.96)*posterior_se

## calculating the probability that d is more than zero(Clinton winning)
1-pnorm(0,posterior_mean,posterior_se)
# Howevr, 0.99 $\%$ chance of winning by clinton is a bit too overconfident and not in agreement with FiveThirtyEight' 81.4$\%$.
```

## Key points: Mathematical Representations of Models
If we collect several polls with measured spreads $X_1,...,X_j$
with a sample size of $N$, these random variables have expected value $d$ and standard error $2\sqrt{p(1-p)/N}$.

We represent each measurement as $X_{i,j}=d+b+h_i+\epsilon _{i,j}$
- The index $i$ represents the different pollsters
- The index $j$ represents the different polls
- $X_{i,j}$ is the $j$th poll by the $i$th pollster 
- $d$ is the actual spread of the election 
- $b$ is the general bias affecting all pollsters 
- $h_i$ represents the house effect for the $i$th pollster $\epsilon_{i,j}$ represents the random error associated with the $i,j$th poll.

The sample average is now $\bar{X}=a+b+\frac{1}{N}\sum_{i=1}^{N}{X_i}$ with standard deviation $SE(\hat{X})=\sqrt{\sigma^2/N+\sigma_b^2}$ 
(Note: Eli: $\sigma^2/N=se[results]^2$)
The standard error of the general bias $\sigma_b$ does not get reduced by averaging multiple polls, which increases the variability of our final estimate.

Simulated data with $X_j=d+\epsilon_j$
```{r}
## realted to just one poster with different measurements:
j <- 6  #suppose we have j=6 poll from a pollster
N <- 2000 
d <- 0.021
p <- (d+1)/2
X <- d+ rnorm(j,0,2*sqrt(p*(1-p)/N))
```

Simulated data with $X_{i,j}=d+\epsilon_{i,j}$
```{r}
## different pollsters
I <- 5 #the number of pollsters
J <- 6 #j is the number of polls for each pollsters
d <- 0.021
p <- (d+1)/2
X <- sapply(1:I,function(i){
  d+rnorm(j,0,2*sqrt(p*(1-p)/N))
})
```
Simulated data with $X_{i,j}=d+h_i+\epsilon_{i,j}$
```{r}
I <- 5
J <- 6
N <- 2000
d <- .021
p <- (d+1)/2
h <- rnorm(I, 0, 0.025)    # assume standard error of pollster-to-pollster variability is 0.025
X <- sapply(1:I, function(i){
  d + h[i] + rnorm(J, 0, 2*sqrt(p*(1-p)/N))
})
```

Calculating probability of $d>0$ with general bias
- Note that sigma now includes an estimate of the variability due to general bias $\sigma_b=.025$.
```{r}
mu <- 0
tau <- 0.035

sigma <- sqrt(results$se^2 + .025^2)  #se=sigma/sqrt(N)
Y <- results$avg

B <- sigma^2 / (sigma^2 + tau^2)
posterior_mean <- B*mu + (1-B)*Y
posterior_se <- sqrt(1 / (1/sigma^2 + 1/tau^2))

1 - pnorm(0, posterior_mean, posterior_se)
```

## Key points: Predicting the Electoral College
- In the US election, each state has a certain number of votes that are won all-or-nothing based on the popular vote result in that state (with minor exceptions not discussed here).

- We use the left_join() function to combine the number of electoral votes with our poll results.

- For each state, we apply a Bayesian approach to generate an Election Day $d$. We keep our prior simple by assuming an expected value of $0$ and a standard deviation based on recent history of $0.02$.

- We can run a Monte Carlo simulation that for each iteration simulates poll results in each state using that state's average and standard deviation, awards electoral votes for each state to Clinton if the spread is greater than $0$, then compares the number of electoral votes won to the number of votes required to win the election (over 269).

- If we run a Monte Carlo simulation for the electoral college without accounting for general bias, we overestimate Clinton's chances of winning at over $99\%$.

- If we include a general bias term, the estimated probability of Clinton winning decreases significantly.

### step1: The firts top 5 states ranked by electoral votes
```{r}
library(tidyverse)
library(dslabs)
data("polls_us_election_2016")
head(results_us_election_2016)

results_us_election_2016 %>% arrange(desc(electoral_votes)) %>%
  top_n(5,electoral_votes)
```
### step2: calculating the average and standard deviation for each state
```{r}
results <- polls_us_election_2016 %>%
    filter(state != "U.S." &
            !grepl("CD", "state") &
            enddate >= "2016-10-31" &
            (grade %in% c("A+", "A", "A-", "B+") | is.na(grade))) %>%
    mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) %>%
    group_by(state) %>%
    summarize(avg = mean(spread), sd = sd(spread), n = n()) %>%
    mutate(state = as.character(state))

# 10 closest races = battleground states
results %>% arrange(abs(avg))
```
```{r}
# joining electoral college votes with results
results <- left_join(results, results_us_election_2016,by="state")

## states with no polls: note Rhode Island and District of Columbia = Democrat
results_us_election_2016 %>% filter(!state %in% results$state)
```
```{r}
# assigning sd to states with just one poll as median of other sd values (they did not have any sd)
results <- results %>%
    mutate(sd = ifelse(is.na(sd), median(results$sd, na.rm = TRUE), sd))
```

### step3: computing the posterior's mean and standard error:
```{r}
mu <- 0 #assuming we do not know about the pror result
tau <- 0.02

results %>% mutate(sigma = sd/sqrt(n),
                   B = sigma^2/ (sigma^2 + tau^2),
                   posterior_mean = B*mu + (1-B)*avg,
                   posterior_se = sqrt( 1 / (1/sigma^2 + 1/tau^2))) %>%
    arrange(abs(posterior_mean))
```

### step4: Monte Carlo simulation of Election Night results (no general bias)
```{r}
mu <- 0
tau <- 0.02

clinton_EV <- replicate(1000, {
    results %>% mutate(sigma = sd/sqrt(n),
                       B = sigma^2/ (sigma^2 + tau^2),
                       posterior_mean = B*mu + (1-B)*avg,
                       posterior_se = sqrt( 1 / (1/sigma^2 + 1/tau^2)),
                       simulated_result = rnorm(length(posterior_mean), posterior_mean, posterior_se),
                       clinton = ifelse(simulated_result > 0, electoral_votes, 0)) %>%    # award votes if Clinton wins state
        summarize(clinton = sum(clinton)) %>%    # total votes for Clinton
        .$clinton + 7    # 7 votes for Rhode Island and DC
})
mean(clinton_EV > 269)    # over 269 votes wins election

# histogram of outcomes
data.frame(clinton_EV) %>%
    ggplot(aes(clinton_EV)) +
    geom_histogram(binwidth = 1) +
    geom_vline(xintercept = 269)
```
### step5: Monte Carlo simulation including general bias
```{r}
mu <- 0
tau <- 0.02
bias_sd <- 0.03

clinton_EV_2 <- replicate(1000, {
    results %>% mutate(sigma = sqrt(sd^2/(n) + bias_sd^2),    # added bias_sd term
                        B = sigma^2/ (sigma^2 + tau^2),
                        posterior_mean = B*mu + (1-B)*avg,
                        posterior_se = sqrt( 1 / (1/sigma^2 + 1/tau^2)),
                        simulated_result = rnorm(length(posterior_mean), posterior_mean, posterior_se),
                        clinton = ifelse(simulated_result > 0, electoral_votes, 0)) %>%    # award votes if Clinton wins state
        summarize(clinton = sum(clinton)) %>%    # total votes for Clinton
        .$clinton + 7    # 7 votes for Rhode Island and DC
})
mean(clinton_EV_2 > 269)    # over 269 votes wins election

# histogram of outcomes
data.frame(clinton_EV_2) %>%
    ggplot(aes(clinton_EV_2)) +
    geom_histogram(binwidth = 1) +
    geom_vline(xintercept = 269)
```

## Key points: Forecasting
- In poll results, $p$ is not fixed over time. Variability within a single pollster comes from time variation.

- In order to forecast, our model must include a bias term $b_t$ to model the time effect.

- Pollsters also try to estimate $f(t)$, the trend of $p$ given time $t$ using a model like:
$Y_{i,j,t}=d+b+h_j+b_t+f(t)+\epsilon(i,j,t)$

- Once we decide on a model, we can use historical data and current data to estimate the necessary parameters to make predictions.


### variability across one pollster:
```{r}
# select all national polls by one pollster
library(tidyverse)
library(dslabs)
data("polls_us_election_2016")

one_pollster <- polls_us_election_2016 %>%
    filter(pollster == "Ipsos" & state == "U.S.") %>%
    mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)

## calculating the standard error (emprical and theoritical)
se <- one_pollster %>%
  summarize(empirical=sd(spread),
            theoritical=2*sqrt(mean(spread)*(1-mean(spread))/min(samplesize)))
se

## calculating 
one_pollster %>% ggplot(aes(spread)) +
  geom_histogram(binwidth=0.01,color="black")

# as see, the distribution is not normal as theory said. what is thid difference come from???  
```
```{r}
## extra variability comes from time variation not accounted for by the theory that assumed that p is fixed over time
one_pollster %>% ggplot(aes(enddate,spread)) + geom_point() +geom_smooth(span = 0.1)
```

### Trend across time for several pollsters:
```{r}
## the variability across pollsters becaus eof time variabilirt
polls_us_election_2016 %>%
  filter(state=="U.S." & enddate >= "2016-07-01") %>%
  group_by(pollster) %>%
  filter(n()>=10) %>%
  ungroup() %>%
  mutate(spread=rawpoll_clinton/100-rawpoll_trump/100) %>%
  ggplot(aes(enddate,spread)) +
  geom_smooth(method="loess",span=0.1) +
  geom_point(aes(color=pollster),show.legend = FALSE,alpha=0.6)
  
#the blue line is f(t) and an estimate of the error due to time variability
```

### Plotting raw percentages across time:
```{r}
polls_us_election_2016 %>%
    filter(state == "U.S." & enddate >= "2016-07-01") %>%
    select(enddate, pollster, rawpoll_clinton, rawpoll_trump) %>%
    rename(Clinton = rawpoll_clinton, Trump = rawpoll_trump) %>%
    gather(candidate, percentage, -enddate, -pollster) %>%
    mutate(candidate = factor(candidate, levels = c("Trump", "Clinton"))) %>%
    group_by(pollster) %>%
    filter(n() >= 10) %>%
    ungroup() %>%
    ggplot(aes(enddate, percentage, color = candidate)) +
    geom_point(show.legend = FALSE, alpha = 0.4) +
    geom_smooth(method = "loess", span = 0.15) +
    scale_y_continuous(limits = c(30, 50))
```

### Exercise: Election Forecasting
#### 1- Cofindence interval of polling data using CLT
```{r}
library(dplyr)
library(dslabs)
data("polls_us_election_2016")

polls <- polls_us_election_2016 %>% 
  filter(state != "U.S." & enddate >= "2016-10-31") %>% 
  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)

## Confidence Intervals of Polling Data using CLT
cis <- polls %>%
  mutate(X_hat=(spread+1)/2, se=2*sqrt(X_hat*(1-X_hat)/samplesize), lower=spread-qnorm(0.975)*se, upper=spread+qnorm(0.975)*se) %>% select(state,startdate,enddate,pollster,grade,spread,lower,upper)
```
#### 2- Compare to Actual Results:
how often the 95% confidence intreval contain the actual result for every poll:
```{r}
## selcting the actual spread from actual result
add_actual_result <- results_us_election_2016 %>%
  mutate(actual_spread=clinton/100-trump/100) %>%
  select(state,actual_spread)

## joining the actual result table to our cis table
ci_data <- cis %>%
  mutate(state = as.character(state)) %>% left_join(add_actual_result, by = "state")

##
p_hit <- ci_data  %>%
  mutate(hit=lower<=actual_spread & actual_spread<=upper) %>%
  summarize(mean(hit))
p_hit
```
#### 3- Stratify by Pollster and Grade: calculating the proportion of hits for each pollster with at least 5 polls
```{r}
results_us_election_2016 %>%
  mutate(actual_spread=clinton/100-trump/100) %>%
  select(state,actual_spread) 

ci_data <- cis %>%
  mutate(state = as.character(state)) %>% left_join(add_actual_result, by = "state")

p_hits_pollster<-ci_data %>%
  mutate(hit=lower<=actual_spread & actual_spread<=upper) %>%
  group_by(pollster) %>%
  filter(n()>=5) %>%
  summarize(proportion_hit=mean(hit),n=n(),grade=grade[1]) %>%
  arrange(desc(proportion_hit))

```
#### 4-  Stratifying by State instead of pollsters:
```{r}
results_us_election_2016 %>%
  mutate(actual_spread=clinton/100-trump/100) %>%
  select(state,actual_spread) 

ci_data <- cis %>%
  mutate(state = as.character(state)) %>% left_join(add_actual_result, by = "state")

hit_state<-ci_data %>%
  mutate(hit=lower<=actual_spread & actual_spread<=upper) %>%
  group_by(state) %>%
  filter(n()>=5) %>%
  summarize(proportion_hit=mean(hit),n=n()) %>%
  arrange(desc(proportion_hit))
```
#### 5- Plotting Prediction Results from Stratifying by State:
a bar plot of the proportion of hits for each state
```{r}
library(tidyverse)
hit_state %>%
  ggplot(aes(state,proportion_hit)) +
  geom_bar(stat = "identity") +
  coord_flip()
```
#### 6- Predicting the winner:
Add two columns to the cis table by calculating, for each poll, the difference between the predicted spread and the actual spread, and define a column hit that is true if the signs are the same.
```{r}
results_us_election_2016 %>%
  mutate(actual_spread=clinton/100-trump/100) %>%
  select(state,actual_spread) 

ci_data <- cis %>%
  mutate(state = as.character(state)) %>% left_join(add_actual_result, by = "state")

errors <- ci_data %>% 
  mutate(error=spread-actual_spread,hit=sign(spread)==sign(actual_spread))

tail(errors,n=6)
```
#### 7- Plotting Prediction Results states with more than 5 polls versus thr proportion of hit
```{r}
errors <- ci_data %>% 
  mutate(error=spread-actual_spread,hit=sign(spread)==sign(actual_spread))

p_hits <- errors %>%
  group_by(state) %>%
  filter(n()>=5) %>%
  summarize(proportion_hit=mean(hit),n())

p_hits %>%
  ggplot(aes(state,proportion_hit)) +
  geom_bar(stat = "identity") +
  coord_flip()
```
- as seen most states' polls predicted the correct winner 100% of the time. 
- Only for some states the prediction was incorrect more than 25% of the time. 
- Wisconsin got every single poll wrong.
- In Pennsylvania and Michigan, more than 90% of the polls had the signs wrong.

#### 8- Plotting the Errors:
create a hidtogram of errors:
```{r}
median(errors$error)
hist(errors$error)

```
#### 8- Plot Bias by State:
at the state level, the median error was slightly in favor of Clinton. The distribution is not centered at 0, but at 0.037. This value shows the general bias. generate a boxplot to examine if the bias was general to all states (which has grade of equal or higher than B+) or if it affected some states differently.
```{r}
errors %>%
  filter(grade%in%c("B+","A-","A","A+")| is.na(grade)) %>%
  mutate(state=reorder(state,error)) %>%  #reorder such as desc
  ggplot(aes(state,error)) +
  geom_boxplot() +
  geom_point() +
   theme(axis.text.x = element_text(angle = 90, hjust = 1))
  
```
#### 9- Plot Bias by State and filtering Error Plot for states which have only a few polls (five good polls or more)

```{r}
errors %>%
  filter(grade%in%c("B+","A-","A","A+") | is.na(grade)) %>%
  group_by(state) %>%
  filter(n()>=5) %>%
  ungroup() %>%
  mutate(state=reorder(state,error)) %>%  #reorder such as desc
  ggplot(aes(state,error)) +
  geom_boxplot() +
  geom_point() +
   theme(axis.text.x = element_text(angle = 90, hjust = 1))
```
result: as seen the West (Washington, New Mexico, California) underestimated Hillary's performance, while the Midwest (Michigan, Pennsylvania, Wisconsin, Ohio, Missouri) overestimated it. In our simulation, we did not model this behavior because we added general bias, not a regional bias. Some pollsters are now modeling correlation between similar states and estimating this correlation from historical data. To learn more about this, you can learn about random effects and mixed models. 

##Key points: the t-distribution
- In models where we must estimate two parameters, $p$ and $\sigma$, the Central Limit Theorem can result in overconfident confidence intervals for sample sizes smaller than approximately $30$.

- If the population data are known to follow a normal distribution, theory tells us how much larger to make confidence intervals to account for estimation of $\sigma$.

- Given $s$ as an estimate of $\sigma$, then $Z=\frac{\bar{X}-d}{s/\sqrt{N}}$ follows a t-distribution with N-1 degrees of freedom. 

- Degrees of freedom determine the weight of the tails of the distribution. Small values of degrees of freedom lead to increased probabilities of extreme values.

- We can determine confidence intervals using the t-distribution instead of the normal distribution by calculating the desired quantile with the function $qt()$.

```{r}
## loading the data and the data of one_poll_per_pollster
library(tidyverse)
polls <- polls_us_election_2016 %>%
    filter(state=="U.S." & enddate>="2016-10-31" & (grade%in%c("A","A+","A-","B+") | is.na(grade))) %>% mutate(spread=(rawpoll_clinton-rawpoll_trump)/100)

one_poll_per_pollsters <- polls %>% 
        group_by(pollster) %>%
        filter(enddate==max(enddate)) %>%
        ungroup()

z <- qt(0.975, nrow(one_poll_per_pollsters)-1) #t-distribution with N-1 degrees of freedom

one_poll_per_pollsters %>%
  summarize(avg=mean(spread),moe=z*sd(spread)/sqrt(length(spread))) %>%
  mutate(start=avg-moe,end=avg+moe)
# as seen, the start and end of interval are slightly bigger than them from normal_distribution. it because qt is larger than qnorm as below:

qt(0.975,14)
qnorm(0.975)

```

### Exercise:
As we know, for the normal distibution, only 5% of observations are larger than two standard deviation from the mean.

####  Using the t-Distribution:
compute the probability of seeing t-distributed random variables being more than 2 in absolute value when degree of freedom is 3. 
```{r}
## the probaility of seeing t-distributed random variables being more than 2, in absolute value with 3 degrees of freedom.
1-pt(2,3)+pt(-2,3)
```
####Plotting the t-distribution
calculate the probability of seeing t-distributed random variables being more than 2 in absolute value for degrees of freedom from 3 to 50.

```{r}
df <- 3:50
prob <- sapply(df,function(df) 1-pt(2,df)+pt(-2,df)) 

plot(df,prob)
```

### Sampling From the Normal Distribution
create a monto carlo simulation from heights for two sample sizes of 15, and 50 and compare them to each other.
```{r}
## when N=50
library(dslabs)
library(dplyr)
data(heights)

x <- heights %>% filter(sex == "Male") %>%
  .$height
set.seed(1, sample.kind="Rounding")

B<- 10000
N <- 50
mu <- mean(x)

res <- replicate(B,{
  X <- sample(x,size=N,replace=TRUE)
  X_bar <- mean(X)
  se_bar <- sd(X)/sqrt(N)
  c(X_bar-qnorm(0.975)*se_bar,X_bar+qnorm(0.975)*se_bar)
  between(mu,X_bar-qnorm(0.975)*se_bar,X_bar+qnorm(0.975)*se_bar)
})
mean(res)
```
```{r}
## when N=15
library(dslabs)
library(dplyr)
data(heights)

x <- heights %>% filter(sex == "Male") %>%
  .$height
set.seed(1, sample.kind="Rounding")

B<- 10000
N <- 15
mu <- mean(x)

res <- replicate(B,{
  X <- sample(x,size=N,replace=TRUE)
  X_bar <- mean(X)
  se_bar <- sd(X)/sqrt(N)
  interval <- c(X_bar-qnorm(0.975)*se_bar,X_bar+qnorm(0.975)*se_bar)
  between(mu,X_bar-qnorm(0.975)*se_bar,X_bar+qnorm(0.975)*se_bar)
})
mean(res)
```
#### Sampling from the t-Distribution:
when N=15 is not that much big, the distribution is Normal so the t-distribution is appliable. compute the confidence interval using t-distribution instead of Normal distribution.
```{r}
## for N=50
library(dslabs)
library(dplyr)
data(heights)

x <- heights %>% filter(sex == "Male") %>%
  .$height
set.seed(1, sample.kind="Rounding")

B<- 10000
N <- 50
mu <- mean(x)

res <- replicate(B,{
  X <- sample(x,size=N,replace=TRUE)
  X_bar <- mean(X)
  se_bar <- sd(X)/sqrt(N)
  interval <- c(X_bar-qt(0.975,N-1)*se_bar,X_bar+qt(0.975,N-1)*se_bar)
  between(mu,X_bar-qt(0.975,N-1)*se_bar,X_bar+qt(0.975,N-1)*se_bar)
})
mean(res)
# as seen, for N=50 there are not much difference between the normal and t-distributions results.
```

```{r}
## for N=15
library(dslabs)
library(dplyr)
data(heights)

x <- heights %>% filter(sex == "Male") %>%
  .$height
set.seed(1, sample.kind="Rounding")

B<- 10000
N <- 15
mu <- mean(x)

res <- replicate(B,{
  X <- sample(x,size=N,replace=TRUE)
  X_bar <- mean(X)
  se_bar <- sd(X)/sqrt(N)
  interval <- c(X_bar-qt(0.975,N-1)*se_bar,X_bar+qt(0.975,N-1)*se_bar)
  between(mu,X_bar-qt(0.975,N-1)*se_bar,X_bar+qt(0.975,N-1)*se_bar)
})
mean(res)
# The t-distribution takes the variability into account and generates larger confidence intervals.
```
# 7- Association tests
## Key points: Association Tests
- We learn how to determine the probability that an observation is due to random variability given categorical, binary or ordinal data.

- Fisher's exact test determines the p-value as the probability of observing an outcome as extreme or more extreme than the observed outcome given the null distribution.

- Data from a binary experiment are often summarized in two-by-two tables.

- The p-value can be calculated from a two-by-two table using Fisher's exact test with the function fisher.test().

### Research funding rates example:
```{r}
data("research_funding_rates")
research_funding_rates

totals <- research_funding_rates %>%
  select(-discipline) %>%
  summarize_all(funs(sum)) %>%
  summarize(yes_men=awards_men,
            no_men=applications_men-awards_men,
            yes_women=awards_women,
            no_women=applications_women-awards_women)

totals %>% summarize(percent_men=yes_men/(yes_men+no_men),percent_women=yes_women/(yes_women+no_women))
# in this section, we learn how to perform inference for this type of data
```
###  Two-by-two table and p-value for the Lady Tasting Tea problem
```{r}
tab <- matrix(c(3,1,1,3),2,2)
rownames(tab) <- c("Poured Before", "Poured After")
colnames(tab) <- c("Guessed Before", "Guessed After")
tab

# computing p-value with Fisher's Exact Test
fisher.test(tab,alternative = "greater")
```

## Key points: Chi-Squared Tests
- If the sums of the rows and the sums of the columns in the two-by-two table are fixed, then the hypergeometric distribution and  Fisher's exact test can be used. Otherwise, we must use the chi-squared test.

- The chi-squared test compares the observed two-by-two table to the two-by-two table expected by the null hypothesis and asks how likely it is that we see a deviation as large as observed or larger by chance.

- The function chisq.test() takes a two-by-two table and returns the p-value from the chi-squared test.

- The odds ratio states how many times larger the odds of an outcome are for one group relative to another group.

- A small p-value does not imply a large odds ratio. If a finding has a small p-value but also a small odds ratio, it may not be a practically significant or scientifically significant finding. but for p-value less than 0.05, it is statistically signinficant.

- Because the odds ratio is a ratio of ratios, there is no simple way to use the Central Limit Theorem to compute confidence intervals. There are advanced methods for computing confidence intervals for odds ratios that we do not discuss here. One approach is to use the theory of generalized linear models (rerference for study: McGullagh and Nelder, 1989)
### Chi-square test
```{r}
# loading data
data("research_funding_rates")
research_funding_rates

totals <- research_funding_rates %>%
  select(-discipline) %>%
  summarize_all(funs(sum)) %>%
  summarize(yes_men=awards_men,
            no_men=applications_men-awards_men,
            yes_women=awards_women,
            no_women=applications_women-awards_women)

# computing overall funding rate
funding_rate <- totals%>%
  summarize(percent_total=
              (yes_men+yes_women)/
              (yes_men+no_men+yes_women+no_women)) %>%
  .$percent_total
funding_rate

## constructing two-by-two table for observed data
two_by_two <- tibble(awarded=c("no","yes"),
                     men=c(totals$no_men,totals$yes_men),
                     women=c(totals$no_women,totals$yes_women))
two_by_two

# computing null hypothesis two-by-two table
tibble(awarded=c("no","yes"),
       men=(totals$no_men+totals$yes_men)*
         c(1-funding_rate,funding_rate),
       women=(totals$no_women+totals$yes_women)*
         c(1-funding_rate,funding_rate))

# chi-squared test
two_by_two %>%
  select(-awarded) %>%
  chisq.test()
# this means that the probability of seeing a deviation like the one we see or bigger under the null that funding is assigned at random is 0.051.
```
### Odds ratio
```{r}
# odds of getting funding for men
odds_men <- (two_by_two$men[2]/sum(two_by_two$men))/(two_by_two$men[1]/sum(two_by_two$men))
odds_men

# odds of getting funding for women
odds_women <- (two_by_two$women[2]/sum(two_by_two$women))/(two_by_two$women[1]/sum(two_by_two$women))
odds_women

# odds ratio - how many times larger odds are for men than women
odds_men/odds_women
```
### p-value and odds ratio responses to increasing sample 
```{r}
# multiplying all observations by 10 decreases p-value without changing odds ratio
two_by_two %>%
  select(-awarded) %>%
  mutate(men=men*10,women=women*10) %>%
  chisq.test()
```
### Exercise: 
In the elction data, we want to determine if polls rated A- made better predictions than polls rated C- or not?
compute the proportion of times each grade of poll predicted the correct winner of A- and C- grades.
#### 1- Comparing Proportions of Hits
```{r}
## loading poll data:
library(dplyr)
library(dslabs)
data("polls_us_election_2016")

## filtering data in the U.S. and after 2016-10-31 and calculating spread
polls <- polls_us_election_2016 %>% 
  filter(state != "U.S." & enddate >= "2016-10-31") %>% 
  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)

## Confidence Intervals of Polling Data using CLT
cis <- polls %>%
  mutate(X_hat=(spread+1)/2, se=2*sqrt(X_hat*(1-X_hat)/samplesize), lower=spread-qnorm(0.975)*se, upper=spread+qnorm(0.975)*se) %>% select(state,startdate,enddate,pollster,grade,spread,lower,upper)

## joining the actual result to the pollster data
add_actual_result <- results_us_election_2016 %>%
  mutate(actual_spread=clinton/100-trump/100) %>%
  select(state,actual_spread)

ci_data <- cis %>%
  mutate(state = as.character(state)) %>% left_join(add_actual_result, by = "state")

## calculating the errors by matching the sign of spread
errors <- ci_data %>% 
  mutate(error=spread-actual_spread,hit=sign(spread)==sign(actual_spread))
```
creating two_by_two table for grades A- and C- based on hits
```{r}
## creating a two_by_two table from grade and number of hits:
totals <- errors %>% filter(grade%in%c("A-","C-")) %>%
group_by(grade,hit) %>% summarize(n=n()) 

two_by_two <- spread(totals,grade,n)

# proportion of hits for grade A- polls
two_by_two$"A-"[2]/(two_by_two$"A-"[2]+two_by_two$"A-"[1])

# proportion of hits for grade C- polls
two_by_two$"C-"[2]/(two_by_two$"C-"[2]+two_by_two$"C-"[1])
```
#### 2- Chi-squared Test
Use a chi-squared test to show if the previous proportions are different.
```{r}
chisq_test <- two_by_two %>%
  select(-hit) %>%
  chisq.test()

chisq_test[3]
```
#### 3- Odd ratios:
compute the odds ratio to determine the magnitude of the difference in performance between these two grades of polls
```{r}
odds_C <- (two_by_two$"C-"[2]/sum(two_by_two$"C-"))/(two_by_two$"C-"[1]/sum(two_by_two$"C-"))
odds_C

odds_A <- (two_by_two$"A-"[2]/sum(two_by_two$"A-"))/(two_by_two$"A-"[1]/sum(two_by_two$"A-"))
odds_A

## ratio of odds A to C
odds_A/odds_C

```
## Final Assessment: inference and modeling skills with Brexit poll analysis
In 2016, UK performed a referendum known as Broxit to determine if the country would "Remain" in the European Union (EU) or "Leave" the EU. Even though the media and others interpreted poll results as forecasting "Remain" ( p>0.5) , the actual proportion that voted "Remain" was only 48.1%  (p=0.481). Therefore, UK  decided to leave the EU. Pollsters in the UK were criticized for overestimating support for "Remain". 

## part1:
### step1: Loading the pollster data and real result of Brexit
```{r}
# suggested libraries and options
library(tidyverse)
options(digits = 3)

# load brexit_polls object
library(dslabs)
data(brexit_polls)

p <- 0.481    # official proportion voting "Remain"
d <- 2*p-1    # official spread
```
### step2: Expected value and standard error of a poll:
The actual data for remain was p=0.48. suppose a poll with a sample size of N=1500.
```{r}
p <- 0.481
N <- 1500
## the expected and standard error of total number of voters in the sample choosing "Remain"?
p*N   #expected value of total number "Remain"
sqrt(N*p*(1-p)) # or se <- sqrt(p*(1-p)/N)*N total number

##Expected value and standard error of X^ (the proportion of "Remain" voters)
p  #the expected value of X^ is p
sqrt(p*(1-p)/N) #se

## The expected value and standard error of  d, the spread between the proportion of "Remain" voters and "Leave" voters?
2*p-1
2*sqrt(p*(1-p)/N)
```
### step3:Actual Brexit poll estimates:
add the ci=olumn name X_hat for each poll and calculate the X_hat:
```{r}
brexit_polls_new <- brexit_polls %>%
  mutate(X_hat=(spread+1)/2) 
#X_hat is not equal to p because we have undecided extran to remain and leave columns

## average and standard deviation of observed spread
mean(brexit_polls_new$spread)
sd(brexit_polls_new$spread)

## the average and standard deviation of X_hat (the estimate of parametr p)
mean(brexit_polls_new$X_hat)
sd(brexit_polls_new$X_hat)
## 
```
### step4:confidence intervals of Broxit poll:
for the first poll in brexit_polls (brexit_polls[1,]), calculate the 95% confidence interval for X^
```{r}
## the confidence interval of first poll:
X_hat1 <- brexit_polls_new$X_hat[1]
N <- (brexit_polls_new$samplesize)[1]
se <- sqrt(X_hat1*(1-X_hat1)/N)
ci <- c(X_hat1-qnorm(0.975)*se,X_hat1+qnorm(0.975)*se)
X_hat1
sd1
ci

## does the confidence interval predict a winner (does not cover  p=0.5? ) does interval contain the actual p of Refrandom?
!between(0.5,X_hat1-qnorm(0.975)*se,X_hat1+qnorm(0.975)*se)
between(0.481,X_hat1-qnorm(0.975)*se,X_hat1+qnorm(0.975)*se)
```
## part2: Brexit poll analysis
```{r}
# suggested libraries
library(tidyverse)

# load brexit_polls object and add x_hat column
library(dslabs)
data(brexit_polls)

# final proportion voting "Remain"
p <- 0.481
```
### step1: Confidence intervals for polls in June
generate the data frame of june_polls which contain only Brexit polls that end in June 2016.
```{r}
## compute confidence intervals for all polls and show how many cover the true value of d.
june_polls <- brexit_polls %>%
    mutate(X_hat = (spread + 1)/2) %>%
  filter(enddate >= "2016-06-01") %>%
  mutate(se_x_hat=sqrt(X_hat*(1-X_hat)/samplesize)) %>% 
  mutate(se_d_hat=2*se_x_hat) %>%
  mutate(lower=spread-qnorm(0.975)*se_d_hat,upper=spread+qnorm(0.975)*se_d_hat) %>%
    mutate(hit=lower<=−0.038&−0.038<=upper)#2p-1 real result
  

## the proportion of polls for which their confidence interval covering value of 0.
june_polls %>%
  mutate(hit_0=lower<=0&0<=upper) %>%
  summarize(mean(hit_0))

## The proportion of polls predict "Remain" (confidence interval entirely above 0)
june_polls %>%
  mutate(hit_above0 =sign(lower)==1&sign(upper)==1) %>%
  summarize(mean(hit_above0))

## the propoetion of interval containing the true value of p
june_polls %>%
  summarize(mean(hit))

```
### step2: hit rate by pollsters:
Group and summarize the june_polls data by pollster and calculate the proportion of hits for each pollster and the number of polls per pollster. Use arrange() to sort by hit rate.
```{r}
june_polls %>%
  group_by(pollster) %>%
  summarize(proportion_hit=mean(hit),n=n()) %>%
  arrange(desc(proportion_hit))

# Result: as seen the results are consistent with a large general bias that affects all pollsters.
```
### step3: Boxplot of Brexit polls by poll type
```{r}
## Boxplot of Brexit polls by poll type
june_polls %>% 
  ggplot(aes(poll_type,spread)) +
  geom_boxplot() +
  geom_point() +
   theme(axis.text.x = element_text(angle = 90, hjust = 1))
```
Result: based on the above boxplot:
- Telephone polls tend to show support "Remain" (spread > 0). 
- Telephone polls tend to show higher support for "Remain" than online polls (higher spread).
- Online polls have a larger interquartile range (IQR) for the spread than telephone polls, showing that they are more variable.

### step4: Combined spread across poll type
Calculate the confidence intervals of the spread combined across all polls in june_polls, grouping by poll type. 
```{r}
combined_by_type <- june_polls %>%
        group_by(poll_type) %>%
        summarize(N = sum(samplesize),
                  spread = sum(spread*samplesize)/N,
                  p_hat = (spread + 1)/2) 

#lower and upper 95% confidence interval
ci <- combined_by_type %>%
  mutate(lower=spread-qnorm(0.975)*2*sqrt(p_hat*(1-p_hat)/N),
upper=spread+qnorm(0.975)*2*sqrt(p_hat*(1-p_hat)/N))

ci
```

### step5: Interpreting combined spread estimates across poll type
- none of the online and telephone combined spread predict the outcome of the Brexit referendum (a prediction is possible if a confidence interval does not cover 0).
- Neither confidence interval covers the true value of  d=−0.038 

### Brexit poll analysis, section 3:
```{r}
# suggested libraries
library(tidyverse)

# load brexit_polls object and add x_hat column
library(dslabs)
data(brexit_polls)
brexit_polls <- brexit_polls %>%
    mutate(x_hat = (spread + 1)/2)

# final proportion voting "Remain"
p <- 0.481
2*p-1
```
### step1: Chi-squared p-value
Define brexit_hit that calculate the confidence intervals for all Brexit polls in 2016 and then calculates if the confidence interval covers the actual value of the spread  d=−0.038:
```{r}
brexit_hit <- brexit_polls %>%
  mutate(p_hat = x_hat,
         se_spread=2*sqrt(p_hat*(1-p_hat)/samplesize),
         lower_spread=spread-qnorm(0.975)*se_spread,
         upper_spread=spread+qnorm(0.975)*se_spread,
         hit=lower_spread<=-0.038&-0.038<=upper_spread) %>%
         select(poll_type, hit)

## calculating p_value (edx method)
brexit_chisq <- table(brexit_hit$poll_type, brexit_hit$hit)
chisq.test(brexit_chisq)$p.value

# online > telephone (edx method)
hit_rate <- brexit_hit %>%
    group_by(poll_type) %>%
    summarize(avg = mean(hit))
hit_rate$avg[hit_rate$poll_type == "Online"] > hit_rate$avg[hit_rate$poll_type == "Telephone"]

# statistically significant
chisq.test(brexit_chisq)$p.value < 0.05
```
```{r}
brexit_hit <- brexit_polls %>%
  mutate(p_hat = x_hat,
         se_spread=2*sqrt(p_hat*(1-p_hat)/samplesize),
         lower_spread=spread-qnorm(0.975)*se_spread,
         upper_spread=spread+qnorm(0.975)*se_spread,
         hit=lower_spread<=-0.038&-0.038<=upper_spread) %>%
         select(poll_type, hit)

## creating a two_by_two table of hit and poll type (my method)
tab <- brexit_hit %>%
  group_by(poll_type,hit) %>%
  summarize(n=n())

two_by_two_brexit <- spread(tab,poll_type,n)
two_by_two_brexit

prop_Online<-two_by_two_brexit$Online[2]/(sum(two_by_two_brexit$Online[2])+two_by_two_brexit$Online[1])

prop_Telephone<- two_by_two_brexit$Telephone[2]/(sum(two_by_two_brexit$Telephone[2])+two_by_two_brexit$Telephone[1])

prop_Online
prop_Telephone

## calculating chi-squared test to see if the difference in hit rate is significant (my method)
two_by_two_brexit %>%
  select(-hit) %>%
  chisq.test()
```
### step2: Odds ratio of online and telephone poll hit rate
Compute the odds ratio between the hit rate of online and telephone polls?
```{r}
two_by_two_brexit
odds_online <- two_by_two_brexit$Online[2]/two_by_two_brexit$Online[1]
odds_online

odds_Telephone <- two_by_two_brexit$Telephone[2]/two_by_two_brexit$Telephone[1]
odds_Telephone

odds_online/odds_Telephone
```

### step3: Plotting spread over time
```{r}
brexit_polls %>%
  ggplot(aes(enddate,spread,color=poll_type)) +
  geom_point(show.legend = FALSE, alpha = 0.4) + geom_smooth(method = "loess", span = 0.4) +
  geom_hline(yintercept = -0.038)

```

### step4: Plotting raw percentages over time
generate a graph of proportion of votes inclusing "remain", "leave", "undecided", over time colored by vote.
```{r}
brexit_long <- brexit_polls %>%
    gather(vote, proportion, "remain":"undecided") %>%
    mutate(vote = factor(vote))

## make a graph of proportion of votes over time
brexit_long %>% 
  ggplot(aes(enddate,proportion,color=vote)) +
  geom_point(show.legend = FALSE, alpha = 0.4) + geom_smooth(method = "loess", span = 0.3)
```

